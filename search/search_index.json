{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Toolbox for Robotic-assisted Discovery of Antiinfectives","text":"<p>rda-toolbox is a high-level Python package which aims to provide collection of functions to read and analyze laboratory data in the field of drug discovery.</p> <p> Please note:   If you stumbled across this package by chance, it's probably not for you.   The use case is very specifically tailored to the robotics system of the robotics-assisted discovery of new anti-infectives working group at the Leibniz-HKI. </p>"},{"location":"input/","title":"Input Specifications","text":"<p>Some functions tightly adhere to these specifications and require a certain setup to work. Most experiments require a specific input excel sheet containing information about assay parameters.</p> <p>Because reproducibility is a very important topic, especially in science:</p> <ul> <li>It is highly recommended to use uv for environments</li> <li>Create a virtual environment for each project (Screen)</li> <li>Always at least provide a requirements.txt (<code>uv pip freeze &gt; requirements.txt</code>) for each finished project</li> </ul>"},{"location":"input/#example-project-folder-structure","title":"Example Project Folder Structure","text":"<pre><code>&lt;YYYYMMDD_Assay_OrganismType&gt;\n\u251c\u2500\u2500 code/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 .venv/ # uv venv .venv\n\u251c\u2500\u2500 data/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 input/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 &lt;Assay&gt;_Input.xlsx\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 meta/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 logs/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 processed/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 raw/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 results/\n\u251c\u2500\u2500 figures/\n\u251c\u2500\u2500 methods/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 module_01/\n\u251c\u2500\u2500 readme.md\n\u2514\u2500\u2500 report/\n</code></pre> <ul> <li>MIC Input File</li> <li>Primary Input File</li> <li></li> </ul>"},{"location":"mic/","title":"Minimal Inhibitory Concentration (MIC) Assay","text":""},{"location":"mic/#read-the-inputs-initialize-the-assay-class","title":"Read the inputs, initialize the assay class","text":"<pre><code>import rda_toolbox as rda\n\nmic = rda.MIC(\n    \"../data/raw/\",  # Folderpath for rawfiles\n    \"../data/input/MIC_Input.xlsx\",  # Input excel table\n    \"../data/input/DiS_MP_AsT_2024-12-02.txt\",  # Mapping file from Motherplates to AssayTransfer plates\n    \"../data/input/AmA_AsT_AcD_20241204.txt\",  # Mapping file from AssayTransfer to ActivityDetermination plates\n    plate_type=384,\n    measurement_label=\"Raw Optical Density\",\n    negative_controls=\"Organism + Medium\",  # Label of negative controls ('Bacteria + Medium', 'Fungi + Medium', 'Organism + Medium', 'Negative Controls' etc.)\n    precip_exclude_outlier=True,  # Exclude outliers from the precipitation\n    precipitation_rawfilepath = \"../data/raw/Precipitation_measurements/\",  # Folderpath for precipitation rawfiles\n)\n</code></pre>"},{"location":"mic/#save-the-results","title":"Save the results","text":"<pre><code>mic.save_results(&lt;tables path&gt;, &lt;figures path&gt;, &lt;processed data path&gt;, figureformats=[\"svg, html\"], tableformats=[\"xlsx\", \"csv\"])\n</code></pre> <p>If everything went well you can stop now.</p> <p>If errors occured you may inspect in-between results and debug from there.</p> <p>(Check your inputs!)</p>"},{"location":"mic/#in-between-inspection","title":"In-between inspection:","text":""},{"location":"mic/#view-in-between-results-eg-in-a-notebook","title":"View in-between results (e.g. in a notebook)","text":"<p>Its possible to inspect the assay object:</p> <pre><code>mic.__dict__\n</code></pre> <p>Show how plates are related to each other (hirarchical dictionary): <pre><code>mic._mapping_dict\n</code></pre></p>"},{"location":"mic/#tables","title":"Tables","text":"<pre><code>mic.mapped_input_df\n</code></pre> <pre><code>mic.processed\n</code></pre> <pre><code>mic.results\n</code></pre>"},{"location":"mic/#visualizations","title":"Visualizations","text":"<pre><code>mic.plateheatmap\n</code></pre>"},{"location":"mic/#save-the-results-separately","title":"Save the results separately","text":"<pre><code>mic.save_tables(\"../data/results/\")\n</code></pre> <pre><code>mic.save_figures(\"../figures/\")\n</code></pre>"},{"location":"precipitation/","title":"Precipitation Test","text":"<p>Quality control method to check for precipitation of substances. A precipitating substance can be incorrectly recognized as active. This assay helps us to mark this false positive result.</p> <p>Precipitation testing is usually done additionally on an experiment such as MIC or primary screen. Therefore, the following examples are for the evaluation of a MIC assay with precipitation measurement.</p> <p>The threshold for a substance to be precipitated is the limit of quantification(LoQ) [1],[2]:</p> <p>LoQ = mean(background) + 10 * standarddeviation(background)</p> <p>There is an option to exclude outliers from the precipitation background samples. For now, outliers are detected by checking for values 2 times above the median of the background samples. <pre><code>mic = rda.MIC(\n    \"../data/raw/\",\n    \"../data/input/MIC_Input.xlsx\",\n    \"../data/input/DiS_MP_AsT_2024-12-02.txt\",\n    \"../data/input/AmA_AsT_AcD_20241204.txt\",\n    precipitation_rawfilepath = \"../data/raw/Pr\u00e4zipitationsmessung/\",\n    precip_exclude_outlier=True,\n)\n</code></pre></p>"},{"location":"precipitation/#specifying-background-samples","title":"Specifying background samples:","text":"<p>Using a list of positions (all plates): - Useful for specifying other background rows or columns for all plates - Shown example is the default, column 24 of each 384 plate is defined as background samples <pre><code>mic = rda.MIC(\n    ...\n    precipitation_rawfilepath = \"../data/raw/Pr\u00e4zipitationsmessung/\",\n    background_locations=[f\"{row}24\" for row in string.ascii_uppercase[:16]]\n)\n</code></pre></p> <p>Using a dictionary of positions with barcodes (plate specific): - Useful for specifying very specific background samples per plates <pre><code>mic = rda.MIC(\n    ...\n    precipitation_rawfilepath = \"../data/raw/Pr\u00e4zipitationsmessung/\",\n    background_locations={}\n)\n</code></pre></p>"},{"location":"precipitation/#precipitation-results","title":"Precipitation Results","text":"<p>The precipitation results are visible in the end-results tables as well as in the <code>.../figures/QualityControl</code> folder.</p> <p></p> <p>A heatmap of each precipitation test plate is written. On the top left corner the calculated \"Limit of Quantification\" (\"Nachweisgrenze\") is shown.</p>"},{"location":"precipitation/#limit-of-quantification","title":"Limit of Quantification","text":"<p>The limit of quantification (LoQ, or LOQ) is the lowest value of a signal (or concentration, activity, response...) that can be quantified with acceptable precision and accuracy [1].</p> <p>Some substances may precipitate at certain concentrations. This may interfere with our method of detecting activity.</p> <p>We calculate the LoQ like this:</p> <p>$$ LoQ = y_B + (10 * s_B) $$ - \\(y_B\\): Mean of the background - \\(s_B\\): Standarddeviation of the background</p>"},{"location":"primary/","title":"Primary Screen Assay","text":""},{"location":"primary/#read-the-inputs-initialize-the-assay-class","title":"Read the inputs, initialize the assay class","text":"<pre><code>import rda_toolbox as rda\n\nprimary = rda.PrimaryScreen(\n    \"../data/raw/\",  # Folder where the raw readerfiles are located\n    \"../data/input/PrS_Input.xlsx\",  # Assay specific Input sheet\n    \"../data/input/AmA_AsT_AcD_20241204.txt\",  # Mapping file\n    map_rowname=\"Row 96\",\n    map_colname=\"Col 96\",\n    measurement_label=\"Raw Optical Density\",\n    # Folder where the raw readerfiles for precipitation test are located\n    precipitation_rawfilepath = \"../data/raw/Precipitation_measurements/\"\n)\n</code></pre> <p>Its possible to inspect the assay object:</p> <pre><code>primary.__dict__\n</code></pre>"},{"location":"primary/#view-in-between-results-eg-in-a-notebook","title":"View in-between results (e.g. in a notebook)","text":""},{"location":"primary/#tables","title":"Tables","text":"<pre><code>primary.mapped_input_df\n</code></pre> <pre><code>primary.processed\n</code></pre> <pre><code>primary.results\n</code></pre>"},{"location":"primary/#visualizations","title":"Visualizations","text":"<pre><code>primary.plateheatmap\n</code></pre>"},{"location":"primary/#save-the-results","title":"Save the results","text":"<pre><code># Save all tables\nprimary.save_tables(\"../data/results/\")\n# Save all figures\nprimary.save_figures(\"../figures/\")\n# Save results (figures and tables)\nprimary.save_results(&lt;tables path&gt;, &lt;figures path&gt;, &lt;processed data path&gt;, figureformats=[\"svg, html\"], tableformats=[\"xlsx\", \"csv\"])\n</code></pre>"},{"location":"primary/#primary-screen-results","title":"Primary Screen Results","text":""},{"location":"setup_marimo/","title":"Setting Up An Experiment","text":"<p>In this example I will use marimo notebooks. <code>rda-toolbox</code> should work with any other notebook or virtual environment utility as well as in plain python files.</p>"},{"location":"setup_marimo/#install-uv","title":"Install UV","text":"<p>UV Installation Docs</p> <p>Start a notebook using a sandboxed environment: <pre><code>marimo edit --sandbox Analysis.py\n</code></pre></p>"},{"location":"utility_functions/","title":"Utility Functions","text":"<p>The functions described here make up the main part of this library. They are useful if you want to do only certain parts of the analysis pipeline, or, for example, if you want to write a pipeline for a new experiment.</p>"},{"location":"utility_functions/#parse-cytation10-reader-files-into-a-dataframe","title":"Parse Cytation10 reader files into a dataframe","text":"<pre><code>import rda_toolbox as rda\n\nrawdata = rda.parser.parse_readerfiles(\"&lt;rawfiles_path&gt;\")\n</code></pre> Row_384 Col_384 Raw Optical Density AcD Barcode 384 A 1 1.123 001PrS01003 B 1 2.234 001PrS01003 ... ... ... ..."},{"location":"reference/classes/","title":"(Assay) Classes","text":""},{"location":"reference/classes/#rda_toolbox.experiment_classes.Experiment","title":"<code>Experiment</code>","text":"<p>Superclass for all experiments. Reads rawdata into a DataFrame.</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.Experiment--attributes","title":"Attributes","text":"<p>rawdata : pd.DataFrame     DataFrame containing the rawdata</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.Experiment--methods","title":"Methods","text":"<p>save_plots     Save all the resulting plots to figuredir save_tables     Save all the resulting tables to tabledir save     Save all plots and tables to resultdir</p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>class Experiment:\n    \"\"\"\n    Superclass for all experiments.\n    Reads rawdata into a DataFrame.\n\n    Attributes\n    ----------\n    rawdata : pd.DataFrame\n        DataFrame containing the rawdata\n\n    Methods\n    ----------\n    save_plots\n        Save all the resulting plots to figuredir\n    save_tables\n        Save all the resulting tables to tabledir\n    save\n        Save all plots and tables to resultdir\n    \"\"\"\n\n    def __init__(self, rawfiles_folderpath: str | None, plate_type: int):\n        self._plate_type = plate_type\n        self._rows, self._columns = get_rows_cols(plate_type)\n        self._rawfiles_folderpath = rawfiles_folderpath\n        self.rawdata = parse_readerfiles(\n            rawfiles_folderpath\n        )  # Get rawdata, this will later be overwritting by adding precipitation, if available\n</code></pre>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.MIC","title":"<code>MIC</code>","text":"<p>               Bases: <code>Experiment</code></p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>class MIC(Experiment):  # Minimum Inhibitory Concentration\n    def __init__(\n        self,\n        rawfiles_folderpath,\n        inputfile_path,\n        mp_ast_mapping_filepath,\n        ast_acd_mapping_filepath,\n        plate_type=384,  # Define default plate_type for experiment\n        measurement_label: str = \"Raw Optical Density\",\n        map_rowname: str = \"Row_96\",\n        map_colname: str = \"Col_96\",\n        q_name: str = \"Quadrant\",\n        substance_id: str = \"Internal ID\",\n        negative_controls: str = \"Bacteria + Medium\",\n        blanks: str = \"Medium\",\n        norm_by_barcode: str = \"AcD Barcode 384\",\n        thresholds: list[float] = [50.0],\n        exclude_negative_zfactors: bool = False,\n        precipitation_rawfilepath: str | None = None,\n        precip_background_locations: pd.DataFrame | list[str] = [\n            f\"{row}24\" for row in string.ascii_uppercase[:16]\n        ],\n        precip_exclude_outlier: bool = False,\n        precip_conc_multiplicator: float = 2.0,\n    ):\n        super().__init__(rawfiles_folderpath, plate_type)\n        self._inputfile_path = inputfile_path\n        self._mp_ast_mapping_filepath = mp_ast_mapping_filepath\n        self._ast_acd_mapping_filepath = ast_acd_mapping_filepath\n        self._measurement_label = measurement_label\n        self.precipitation = (\n            Precipitation(\n                precipitation_rawfilepath,\n                background_locations=precip_background_locations,\n                exclude_outlier=precip_exclude_outlier,\n            )\n            # if precipitation_rawfilepath\n            # else None\n        )\n        self.precip_conc_multiplicator = precip_conc_multiplicator\n        self.rawdata = (  # Overwrite rawdata if precipitation data is available\n            # self.rawdata\n            # if self.precipitation is None\n            # else\n            add_precipitation(\n                self.rawdata, self.precipitation.results, self._mapping_dict\n            )\n        )\n        self._substances_unmapped, self._organisms, self._dilutions, self._controls = (\n            read_inputfile(inputfile_path, substance_id)\n        )\n        # self._substance_id = substance_id\n        self._negative_controls = negative_controls\n        self._blanks = blanks\n        self._norm_by_barcode = norm_by_barcode\n        self.thresholds = thresholds\n        self._processed_only_substances = (\n            self.processed[  # Negative Control is still there!\n                (self.processed[\"Dataset\"] != \"Reference\")\n                &amp; (self.processed[\"Dataset\"] != \"Positive Control\")\n                &amp; (self.processed[\"Dataset\"] != \"Blank\")\n            ]\n        )\n        self._references_results = self.processed.loc[\n            self.processed[\"Dataset\"] == \"Reference\"\n        ]\n        self.substances_precipitation = (\n            None\n            if self.precipitation.results.empty\n            else (\n                self._processed_only_substances[\n                    self._processed_only_substances[\"Dataset\"] != \"Negative Control\"\n                ]\n                .drop_duplicates(\n                    [\"Internal ID\", \"AsT Barcode 384\", \"Row_384\", \"Col_384\"]\n                )\n                .loc[\n                    :,\n                    [\n                        \"Internal ID\",\n                        \"AsT Barcode 384\",\n                        \"Row_384\",\n                        \"Col_384\",\n                        \"Concentration\",\n                        \"Precipitated\",\n                    ],\n                ]\n                .reset_index(drop=True)\n            )\n        )\n        def get_min_precip_conc_df(self):\n            if (self.precipitation.results.empty) and (not self.substances_precipitation):\n                return None\n            else:\n                precip_grps = []\n                # precip_df = self.substances_precipitation\n                for (int_id, ast_barcode), grp in self.substances_precipitation.groupby(\n                    [\"Internal ID\", \"AsT Barcode 384\"]\n                    ):\n                    grp = grp.sort_values(\"Concentration\")\n                    min_precip_conc = None\n                    if grp.Precipitated.any():\n                        min_precip_conc = grp[\"Concentration\"][grp[\"Precipitated\"].idxmax()] * self.precip_conc_multiplicator\n                    grp[\"Minimum Precipitation Concentration\"] = min_precip_conc\n                    precip_grps.append(grp)\n                precip_df = pd.concat(precip_grps)\n                precip_df = precip_df[[\"Internal ID\", \"Minimum Precipitation Concentration\"]]\n                return precip_df\n        self.substances_minimum_precipitation_conc = get_min_precip_conc_df(self)\n        self._exclude_negative_zfactor = exclude_negative_zfactors\n        self.mic_df = self.get_mic_df(\n                # self.processed.copy()\n            df = self.processed[\n                (self.processed[\"Dataset\"] != \"Negative Control\") &amp; (self.processed[\"Dataset\"] != \"Blank\")\n            ].dropna(subset=[\"Concentration\"]).copy()\n        ).reset_index(drop=True)\n\n\n\n    @property\n    def _mapping_dict(self):\n        mp_ast_mapping_dict = get_mapping_dict(\n            parse_mappingfile(\n                self._mp_ast_mapping_filepath,\n                motherplate_column=\"MP Barcode 96\",\n                childplate_column=\"AsT Barcode 384\",\n            ),\n            mother_column=\"MP Barcode 96\",\n            child_column=\"AsT Barcode 384\",\n        )\n        ast_acd_mapping_dict = get_mapping_dict(\n            parse_mappingfile(\n                self._ast_acd_mapping_filepath,\n                motherplate_column=\"AsT Barcode 384\",\n                childplate_column=\"AcD Barcode 384\",\n            ),\n            mother_column=\"AsT Barcode 384\",\n            child_column=\"AcD Barcode 384\",\n        )\n        mapping_dict = {}\n        for mp_barcode, ast_barcodes in mp_ast_mapping_dict.items():\n            tmp_dict = {}\n            for ast_barcode in ast_barcodes:\n                tmp_dict[ast_barcode] = ast_acd_mapping_dict[ast_barcode]\n            mapping_dict[mp_barcode] = tmp_dict\n        return mapping_dict\n\n    @cached_property\n    def mapped_input_df(self):\n        \"\"\"\n        Does mapping of the inputfile describing the tested substances with the\n        corresponding mappingfile(s).\n        *Basically replaces rda.process.mic_process_inputs() function so all the variables and intermediate results are available via the class*\n        \"\"\"\n\n        # Sorting of organisms via Rack is **very** important, otherwise data gets attributed to wrong organisms\n        organisms = list(self._organisms.sort_values(by=\"Rack\")[\"Organism\"])\n        formatted_organisms = list(self._organisms.sort_values(by=\"Rack\")[\"Organism formatted\"])\n\n\n        ast_platemapping, _ = read_platemapping(\n            self._mp_ast_mapping_filepath,\n            self._substances_unmapped[\"MP Barcode 96\"].unique(),\n        )\n        # Do some sanity checks:\n        necessary_columns = [\n            \"Dataset\",\n            \"Internal ID\",\n            \"MP Barcode 96\",\n            \"MP Position 96\",\n        ]\n        # Check if all necessary column are present in the input table:\n        if not all(\n            column in self._substances_unmapped.columns for column in necessary_columns\n        ):\n            raise ValueError(\n                f\"Not all necessary columns are present in the input table.\\n(Necessary columns: {necessary_columns})\"\n            )\n        # Check if all of the necessary column are complete:\n        if self._substances_unmapped[necessary_columns].isnull().values.any():\n            raise ValueError(\"Input table incomplete, contains NA (missing) values.\")\n        # Check if there are duplicates in the internal IDs (apart from references)\n        if any(\n            self._substances_unmapped[\n                self._substances_unmapped[\"Dataset\"] != \"Reference\"\n            ][\"Internal ID\"].duplicated()\n        ):\n            raise ValueError(\"Duplicate Internal IDs.\")\n\n        # Map AssayTransfer barcodes to the motherplate barcodes:\n        (\n            self._substances_unmapped[\"Row_384\"],\n            self._substances_unmapped[\"Col_384\"],\n            self._substances_unmapped[\"AsT Barcode 384\"],\n        ) = zip(\n            *self._substances_unmapped.apply(\n                lambda row: mic_assaytransfer_mapping(\n                    row[\"MP Position 96\"],\n                    row[\"MP Barcode 96\"],\n                    ast_platemapping,\n                ),\n                axis=1,\n            )\n        )\n        acd_platemapping, replicates_dict = read_platemapping(\n            self._ast_acd_mapping_filepath,\n            self._substances_unmapped[\"AsT Barcode 384\"].unique(),\n        )\n        num_replicates = list(set(replicates_dict.values()))[0]\n        single_subst_concentrations = []\n\n        for substance, subst_row in self._substances_unmapped.groupby(\"Internal ID\"):\n            # Collect the concentrations each as rows for a single substance:\n            single_subst_conc_rows = []\n            init_pos = int(subst_row[\"Col_384\"].iloc[0]) - 1\n            col_positions_384 = [list(range(1, 23, 2)), list(range(2, 23, 2))]\n            for col_i, conc in enumerate(\n                list(self._dilutions[\"Concentration\"].unique())\n            ):\n                # Add concentration:\n                subst_row[\"Concentration\"] = conc\n                # Add corresponding column:\n                subst_row[\"Col_384\"] = int(col_positions_384[init_pos][col_i])\n                single_subst_conc_rows.append(subst_row.copy())\n\n            # Concatenate all concentrations rows for a substance in a dataframe\n            single_subst_concentrations.append(pd.concat(single_subst_conc_rows))\n        # Concatenate all self._substances_unmapped dataframes to one whole\n        input_w_concentrations = pd.concat(single_subst_concentrations)\n\n        acd_dfs_list = []\n        for ast_barcode, ast_plate in input_w_concentrations.groupby(\"AsT Barcode 384\"):\n            self._controls[\"AsT Barcode 384\"] = list(\n                ast_plate[\"AsT Barcode 384\"].unique()\n            )[0]\n\n            ast_plate = pd.concat([ast_plate, self._controls.copy()])\n            for org_i, organism in enumerate(organisms):\n                for replicate in range(num_replicates):\n                    # Add the AcD barcode\n                    ast_plate[\"AcD Barcode 384\"] = acd_platemapping[ast_barcode][\n                        replicate\n                    ][org_i]\n\n                    ast_plate[\"Replicate\"] = replicate + 1\n                    # Add the scientific Organism name\n                    ast_plate[\"Organism formatted\"] = formatted_organisms[org_i]\n                    ast_plate[\"Organism\"] = organism\n                    acd_dfs_list.append(ast_plate.copy())\n                    # Add concentrations:\n        acd_single_concentrations_df = pd.concat(acd_dfs_list)\n\n        # merge rawdata with input specifications\n        df = pd.merge(self.rawdata, acd_single_concentrations_df, how=\"outer\")\n        return df\n\n    @cached_property\n    def processed(self):\n        return preprocess(\n            self.mapped_input_df,\n            substance_id=\"Internal ID\",\n            measurement=self._measurement_label.strip(\n                \"Raw \"\n            ),  # I know this is weird, its because of how background_normalize_zfactor works,\n            negative_controls=self._negative_controls,\n            blanks=self._blanks,\n            norm_by_barcode=self._norm_by_barcode,\n        )\n\n    @cached_property\n    def plateheatmap(self):\n        return plateheatmaps(\n            self.processed,\n            substance_id=\"Internal ID\",\n            barcode=self._norm_by_barcode,\n            negative_control=self._negative_controls,\n            blank=self._blanks,\n        )\n\n    # def lineplots_facet(self):\n    #    return lineplots_facet(self.processed)\n\n    @cached_property\n    def _resultfigures(self) -&gt; list[Result]:\n        result_figures = []\n        result_figures.append(\n            Result(\"QualityControl\", \"plateheatmaps\", figure=self.plateheatmap)\n        )\n        if (self.substances_precipitation is not None) and (\n            not self.substances_precipitation.empty\n        ):\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Precipitation_Heatmap\",\n                    figure=self.precipitation.plateheatmap(),\n                )\n            )\n\n        # Save plots per dataset:\n\n        processed_negative_zfactor = self._processed_only_substances[\n            self._processed_only_substances[\"Z-Factor\"] &lt; 0\n        ]\n        if (\n            not processed_negative_zfactor.empty\n            and self._exclude_negative_zfactor == True\n        ):\n            print(\n                f\"{len(processed_negative_zfactor[\"AsT Barcode 384\"].unique())} plate(s) with negative Z-Factor detected for organisms '{\", \".join(processed_negative_zfactor[\"Organism formatted\"].unique())}'.\\n\",\n                \"These plates will be excluded from the lineplots visualization!\\n (If you want to include them, use the `exclude_negative_zfactors=False` flag of the MIC class)\",\n            )\n\n        for dataset, dataset_data in self._processed_only_substances.groupby(\"Dataset\"):\n            # Look for and add the corresponding references for each dataset:\n            if \"AcD Barcode 384\" in dataset_data:\n                dataset_barcodes = list(dataset_data[\"AcD Barcode 384\"].unique())\n                corresponding_dataset_references = self._references_results.loc[\n                    (\n                        self._references_results[\"AcD Barcode 384\"].isin(\n                            dataset_barcodes\n                        )\n                    ),\n                    :,\n                ]\n            else:\n                corresponding_dataset_references = pd.DataFrame()\n\n            lineplots_input_df = pd.concat(\n                [dataset_data, corresponding_dataset_references]\n            )\n            lineplots_input_df = lineplots_input_df.dropna(\n                subset=[\"Concentration\"]\n            ).loc[\n                (lineplots_input_df[\"Dataset\"] != \"Negative Control\")\n                &amp; (lineplots_input_df[\"Dataset\"] != \"Blank\"),\n                :,\n            ]\n            if not lineplots_input_df.empty:\n                for threshold in self.thresholds:\n                    result_figures.append(\n                        Result(\n                            dataset,\n                            f\"{dataset}_lineplots_facet_thrsh{threshold}\",\n                            figure=lineplots_facet(\n                                lineplots_input_df,\n                                exclude_negative_zfactors=self._exclude_negative_zfactor,\n                                threshold=threshold,\n                            ),\n                        )\n                    )\n\n        # Save plots per threshold:\n        for threshold in self.thresholds:\n            for dataset, sub_df in self.mic_df.groupby(\"Dataset\"):\n                dummy_df = get_upsetplot_df(\n                    sub_df.dropna(subset=f\"MIC{threshold} in \u00b5M\"),\n                    counts_column=\"Internal ID\",\n                    set_column=\"Organism\",\n                )\n\n                result_figures.append(\n                    Result(\n                        dataset,\n                        f\"{dataset}_UpSetPlot\",\n                        figure=UpSetAltair(dummy_df, title=dataset),\n                    )\n                )\n                result_figures.append(\n                    Result(\n                        dataset,\n                        f\"{dataset}_PotencyDistribution\",\n                        figure=potency_distribution(sub_df, threshold, dataset),\n                    )\n                )\n        return result_figures\n\n    def get_mic_df(self, df):\n\n        pivot_df = pd.pivot_table(\n            df,\n            values=[\"Relative Optical Density\", \"Replicate\", \"Z-Factor\", \"Robust Z-Factor\"],\n            index=[\n                \"Internal ID\",\n                # \"External ID\",\n                \"Organism formatted\",\n                \"Organism\",\n                \"Concentration\",\n                \"Dataset\",\n            ],\n            aggfunc={\n                \"Relative Optical Density\": [\"mean\"],\n                \"Replicate\": [\"count\"],\n                \"Z-Factor\": [\n                    \"mean\",\n                    \"std\",\n                ],\n                \"Robust Z-Factor\": [\n                    \"mean\",\n                    \"std\"\n                ]\n                ,  # does this make sense? with std its usable.\n                # \"Z-Factor\": [\"std\"],\n            },\n            # margins=True\n            fill_value=0 # This might result in confusion, if there are no replicates (1)\n        ).reset_index()\n\n        pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n\n        mic_records = []\n        for group_names, grp in pivot_df.groupby(\n            [\"Internal ID\", \"Organism formatted\", \"Dataset\"]\n        ):\n            internal_id, organism_formatted, dataset = group_names\n            # Sort by concentration just to be sure:\n            grp = grp[\n                [\n                    \"Concentration\",\n                    \"Relative Optical Density mean\",\n                    \"Z-Factor mean\",\n                    \"Z-Factor std\",\n                    \"Robust Z-Factor mean\",\n                    \"Robust Z-Factor std\",\n                ]\n            ].sort_values(by=[\"Concentration\"])\n\n            # Get rows where the OD is below the given threshold:\n            record = {\n                \"Internal ID\": internal_id,\n                \"Organism formatted\": organism_formatted,\n                \"Dataset\": dataset,\n                \"Z-Factor mean\": list(grp[\"Z-Factor mean\"])[0],\n                \"Z-Factor std\": list(grp[\"Z-Factor std\"])[0],\n                \"Robust Z-Factor mean\": list(grp[\"Robust Z-Factor mean\"])[0],\n                \"Robust Z-Factor std\": list(grp[\"Robust Z-Factor std\"])[0],\n            }\n\n            for threshold in self.thresholds:\n                values_below_threshold = grp[\n                    grp[\"Relative Optical Density mean\"] &lt; threshold\n                ]\n                # thx to jonathan - check if the OD at maximum concentration is below threshold (instead of any concentration)\n                max_conc_below_threshold = list(\n                    grp[grp[\"Concentration\"] == max(grp[\"Concentration\"])][\n                        \"Relative Optical Density mean\"\n                    ]\n                    &lt; threshold\n                )[0]\n                if not max_conc_below_threshold:\n                    mic = None\n                else:\n                    mic = values_below_threshold.iloc[0][\"Concentration\"]\n                record[f\"MIC{threshold} in \u00b5M\"] = mic\n            mic_records.append(record)\n        # Drop entries where no MIC could be determined\n        mic_df = pd.DataFrame.from_records(mic_records)\n        # Merge inconsistent (but maybe necessary) columns again\n        mic_df = pd.merge(mic_df, df[[\"Internal ID\", \"External ID\"]], on=[\"Internal ID\"])\n        mic_df = pd.merge(mic_df, self._organisms[[\"Organism\", \"Organism formatted\"]], on=[\"Organism formatted\"])\n        mic_df = mic_df.drop_duplicates()\n        return mic_df\n\n\n    @cached_property\n    def _resulttables(self) -&gt; list[Result]:\n        \"\"\"\n        Retrieves result tables and returns them like list[Result]\n        where Resulttable is a dataclass collecting meta information about the plot.\n        \"\"\"\n        result_tables = []\n        df = self.processed.copy()\n\n\n        # mic_df = self.get_mic_df(df)\n        references_mic_results = self.get_mic_df(\n            self.processed[self.processed[\"Dataset\"] == \"Reference\"].copy()\n        ).reset_index(drop=True)\n\n        result_tables.append(\n            Result(\n                \"Reference\",\n                \"References_MIC_results_eachRefID\",\n                table=references_mic_results,\n            )\n        )\n\n        mic_df = self.mic_df\n        # If precipitation has been done, merge MPC results on long mic_df\n        if not self.precipitation.results.empty:\n            mic_df = pd.merge(self.mic_df, self.substances_minimum_precipitation_conc, on=\"Internal ID\", how=\"left\")\n\n        result_tables.append(\n            Result(\"All\", \"MIC_Results_AllDatasets_longformat\", table=self.mic_df)\n        )\n\n        for dataset, dataset_grp in mic_df.groupby(\"Dataset\"):\n            print(f\"Preparing tables for dataset: {dataset}\")\n            pivot_multiindex_df = pd.pivot_table(\n                dataset_grp,\n                values=[f\"MIC{threshold} in \u00b5M\" for threshold in self.thresholds]\n                + [\"Z-Factor mean\", \"Z-Factor std\"],\n                index=[\"Internal ID\", \"Dataset\"],\n                columns=\"Organism\",\n            ).reset_index()\n            # print(pivot_multiindex_df)\n            # self.pivot_multiindex_df = pivot_multiindex_df\n\n            for threshold in self.thresholds:\n                # print(pivot_multiindex_df.columns)\n                # print(pivot_multiindex_df)\n                if pivot_multiindex_df.empty:\n                    continue\n                organisms_thresholded_mics = pivot_multiindex_df[\n                    [\"Internal ID\", f\"MIC{threshold} in \u00b5M\"]\n                ]\n                cols = list(organisms_thresholded_mics.columns.droplevel())\n                cols[0] = \"Internal ID\"\n                organisms_thresholded_mics.columns = cols\n                organisms_thresholded_mics = organisms_thresholded_mics.sort_values(\n                    by=list(organisms_thresholded_mics.columns)[1:],\n                    na_position=\"last\",\n                )\n\n                # Fill with nan if not available\n                organisms_thresholded_mics = organisms_thresholded_mics.round(2)\n                organisms_thresholded_mics = organisms_thresholded_mics.astype(str)\n                organisms_thresholded_mics = pd.merge(organisms_thresholded_mics, self.mic_df[[\"Internal ID\", \"External ID\"]], on=[\"Internal ID\"], how=\"left\")\n                # organisms_thresholded_mics.fillna(\"NA\", inplace=True)\n\n                if not self.precipitation.results.empty:\n                    organisms_thresholded_mics = pd.merge(\n                        organisms_thresholded_mics,\n                        self.substances_minimum_precipitation_conc,\n                        how=\"left\"\n                    )\n                organisms_thresholded_mics = organisms_thresholded_mics.reset_index(drop=True)\n                organisms_thresholded_mics = organisms_thresholded_mics.drop_duplicates()\n                result_tables.append(\n                    Result(\n                        dataset,\n                        f\"{dataset}_MIC{int(round(threshold))}_results\",\n                        table=organisms_thresholded_mics.reset_index(drop=True)\n                    )\n                )\n\n        return result_tables\n\n    @cached_property\n    def results(self):\n        \"\"\"\n        Retrieves result tables (from self._resulttables)\n        and returns them in a dictionary like:\n            {\"&lt;filepath&gt;\": pd.DataFrame}\n        \"\"\"\n        return {tbl.file_basename: tbl.table for tbl in self._resulttables}\n\n    def save_figures(self, result_path, fileformats: list[str] = [\"svg\", \"html\"]):\n        _save_figures(result_path, self._resultfigures, fileformats=fileformats)\n\n    def save_tables(\n        self, result_path, processed_path, fileformats: list[str] = [\"xlsx\", \"csv\"]\n    ):\n        # Create folder if not existent:\n        pathlib.Path(processed_path).mkdir(parents=True, exist_ok=True)\n        self.processed.to_csv(os.path.join(processed_path, \"processed.csv\"))\n        _save_tables(result_path, self._resulttables, fileformats=fileformats)\n\n    def save_results(\n        self,\n        tables_path: str,\n        figures_path: str,\n        processed_path: str,\n        figureformats: list[str] = [\"svg\", \"html\"],\n        tableformats: list[str] = [\"xlsx\", \"csv\"],\n    ):\n        self.save_figures(figures_path, fileformats=figureformats)\n        self.save_tables(tables_path, processed_path, fileformats=tableformats)\n</code></pre>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.MIC.mapped_input_df","title":"<code>mapped_input_df</code>  <code>cached</code> <code>property</code>","text":"<p>Does mapping of the inputfile describing the tested substances with the corresponding mappingfile(s). Basically replaces rda.process.mic_process_inputs() function so all the variables and intermediate results are available via the class</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.MIC.results","title":"<code>results</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves result tables (from self._resulttables) and returns them in a dictionary like:</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen","title":"<code>PrimaryScreen</code>","text":"<p>               Bases: <code>Experiment</code></p> <p>Primary screen experiment. Usually done using only 1 concentration.</p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>class PrimaryScreen(Experiment):\n    \"\"\"\n    Primary screen experiment. Usually done using only 1 concentration.\n    \"\"\"\n\n    def __init__(\n        self,\n        rawfiles_folderpath: str,\n        inputfile_path: str,\n        mappingfile_path: str,\n        plate_type: int = 384,  # Define default plate_type for experiment\n        measurement_label: str = \"Raw Optical Density\",\n        map_rowname: str = \"Row_96\",\n        map_colname: str = \"Col_96\",\n        q_name: str = \"Quadrant\",\n        substance_id: str = \"Internal ID\",\n        negative_controls: str = \"Bacteria + Medium\",\n        blanks: str = \"Medium\",\n        norm_by_barcode: str = \"AcD Barcode 384\",\n        thresholds: list[float] = [50.0],\n        b_score_threshold: float = -3.0,\n        precipitation_rawfilepath: str | None = None,\n        background_locations: pd.DataFrame | list[str] = [\n            f\"{row}24\" for row in string.ascii_uppercase[:16]\n        ],\n        precip_exclude_outlier: bool = False,\n    ):\n        super().__init__(rawfiles_folderpath, plate_type)\n        self._measurement_label = measurement_label\n        self._mappingfile_path = mappingfile_path\n        self._inputfile_path = inputfile_path\n        self._substances_unmapped, self._organisms, self._dilutions, self._controls = (\n            read_inputfile(inputfile_path, substance_id)\n        )\n        self.substances = mapapply_96_to_384(\n            self._substances_unmapped,\n            rowname=map_rowname,\n            colname=map_colname,\n            q_name=q_name,\n        )\n        self._mapping_df = parse_mappingfile(\n            mappingfile_path,\n            motherplate_column=\"AsT Barcode 384\",\n            childplate_column=\"AcD Barcode 384\",\n        )\n        self._mapping_dict = get_mapping_dict(self._mapping_df)\n        # self._substance_id = substance_id\n        self._negative_controls = negative_controls\n        self._blanks = blanks\n        self._norm_by_barcode = norm_by_barcode\n        self.thresholds = thresholds\n        self.b_score_threshold = b_score_threshold\n        self.precipitation = (\n            Precipitation(\n                precipitation_rawfilepath,\n                background_locations=background_locations,\n                exclude_outlier=precip_exclude_outlier,\n            )\n            # if precipitation_rawfilepath\n            # else None\n        )\n        self.rawdata = (  # Overwrite rawdata if precipitation data is available\n            # self.rawdata\n            # if self.precipitation is None\n            # else\n            add_precipitation(\n                self.rawdata, self.precipitation.results, self._mapping_dict\n            )\n        )\n        self._processed_only_substances = self.processed[\n            (self.processed[\"Dataset\"] != \"Reference\")\n            &amp; (self.processed[\"Dataset\"] != \"Positive Control\")\n            &amp; (self.processed[\"Dataset\"] != \"Blank\")\n        ]\n        self.substances_precipitation = (\n            None\n            if self.precipitation.results.empty\n            else (\n                self._processed_only_substances[\n                    self._processed_only_substances[\"Dataset\"] != \"Negative Control\"\n                ]\n                .drop_duplicates(\n                    [\"Internal ID\", \"AsT Barcode 384\", \"Row_384\", \"Col_384\"]\n                )\n                .loc[\n                    :,\n                    [\n                        \"Internal ID\",\n                        # \"AsT Barcode 384\",\n                        # \"Row_384\",\n                        # \"Col_384\",\n                        \"Concentration\",\n                        \"Precipitated\",\n                        f\"Precipitated at {measurement_label}\",\n                    ],\n                ]\n                .reset_index(drop=True)\n            )\n        )\n\n    def check_substances(self):\n        \"\"\"\n        Do some sanity checks for the substances table.\n        - Check if all necessary columns are present.\n        - Check if substances contains missing values.\n        - Check if there are duplicate Internal IDs (references excluded)\n        \"\"\"\n        # if not all(column in self._substances_unmapped.columns for column in necessary_columns):\n        #     raise ValueError(\n        #         f\"Not all necessary columns are present in the input table.\\n(Necessary columns: {necessary_columns})\"\n        #     )\n        # # Check if all of the necessary column are complete:\n        # if substances[necessary_columns].isnull().values.any():\n        #     raise ValueError(\n        #         \"Input table incomplete, contains NA (missing) values.\"\n        #     )\n        # # Check if there are duplicates in the internal IDs (apart from references)\n        # if any(substances[substances[\"Dataset\"] != \"Reference\"][\"Internal ID\"].duplicated()):\n        #     raise ValueError(\"Duplicate Internal IDs.\")\n        pass\n\n    @cached_property\n    def mapped_input_df(self):\n        \"\"\"\n        Does mapping of the inputfile describing the tested substances with the\n        corresponding mappingfile(s).\n        *Basically replaces rda.process.primary_process_inputs() function so all the variables and intermediate results are available via the class*\n        \"\"\"\n        control_wbarcodes = []\n        # multiply controls with number of AsT plates to later merge them with substances df\n        for origin_barcode in list(self.substances[\"AsT Barcode 384\"].unique()):\n            controls_subdf = self._controls.copy()\n            controls_subdf[\"AsT Barcode 384\"] = origin_barcode\n            control_wbarcodes.append(controls_subdf)\n        controls_n_barcodes = pd.concat(control_wbarcodes)\n\n        ast_plate_df = pd.merge(\n            pd.concat([self.substances, controls_n_barcodes]),\n            self._dilutions,\n            how=\"outer\",\n        )\n\n        mapped_organisms = pd.merge(self._mapping_df, self._organisms)\n\n        result_df = pd.concat(\n            [\n                pd.merge(org_df, ast_plate_df)\n                for _, org_df in pd.merge(mapped_organisms, self.rawdata).groupby(\n                    \"Organism formatted\"\n                )\n            ]\n        )\n\n        for ast_barcode, ast_plate in result_df.groupby(\"AsT Barcode 384\"):\n            print(\n                f\"AsT Plate {ast_barcode} has size: {\n                    len(ast_plate) // len(ast_plate['AcD Barcode 384'].unique())\n                }\"\n            )\n            print(f\"{ast_barcode} -&gt; {ast_plate['AcD Barcode 384'].unique()}\")\n        # result_df = result_df.rename({self._substance_id: \"Internal ID\"}) # rename whatever substance ID was given to Internal ID\n        return result_df\n\n    @cached_property\n    def processed(self):\n        processed = preprocess(\n            self.mapped_input_df,\n            substance_id=\"Internal ID\",\n            measurement=self._measurement_label.strip(\n                \"Raw \"\n            ),  # I know this is weird, its because of how background_normalize_zfactor works,\n            negative_controls=self._negative_controls,\n            blanks=self._blanks,\n            norm_by_barcode=self._norm_by_barcode,\n        )\n\n        # Add B-Scores to plates without negative controls and blanks\n        proc_wo_controls = processed[\n            ~processed[\"Internal ID\"].isin([self._negative_controls, self._blanks])\n        ]\n        # We add b_scores here since we only want them in a primary screen and preprocess() is used generally\n        b_scores = (\n            proc_wo_controls.groupby(self._norm_by_barcode)[\n                [self._norm_by_barcode, \"Row_384\", \"Col_384\", self._measurement_label]\n            ]\n            .apply(lambda plate_grp: add_b_score(plate_grp))\n            .reset_index(drop=True)\n        )\n        processed = pd.merge(processed, b_scores, how=\"outer\")\n        return processed\n\n    @cached_property\n    def plateheatmap(self):\n        return plateheatmaps(\n            self.processed,\n            substance_id=\"Internal ID\",\n            negative_control=self._negative_controls,\n            blank=self._blanks,\n            barcode=self._norm_by_barcode,\n        )\n\n    @cached_property\n    def _resultfigures(self):\n        result_figures = []\n        # Add QualityControl overview of the plates as heatmaps:\n        result_figures.append(\n            Result(\"QualityControl\", \"plateheatmaps\", figure=self.plateheatmap)\n        )\n        # If precipitation testing was done, add it to QC result figures:\n        if not self.precipitation.results.empty:\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Heatmap_Precipitation\",\n                    figure=self.precipitation.plateheatmap(),\n                )\n            )\n\n        for threshold in self.thresholds:\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Scatter_Measurement_vs_BScore_Substances\",\n                    figure=measurement_vs_bscore_scatter(\n                        self._processed_only_substances,\n                        measurement_header=\"Relative Optical Density\",\n                        measurement_title=\"Relative Optical Density\",\n                        bscore_header=\"b_scores\",\n                        bscore_title=\"B-Score\",\n                        color_header=\"Dataset\",\n                        measurement_threshold=threshold,\n                        b_score_threshold=self.b_score_threshold,\n                    ).facet(row=\"Organism\", column=\"Dataset\"),\n                )\n            )\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Scatter_Measurement_vs_BScore_References\",\n                    figure=measurement_vs_bscore_scatter(\n                        self.processed[\n                            self.processed[\"Dataset\"] == \"Reference\"\n                        ].replace({np.nan: None}),\n                        measurement_header=\"Relative Optical Density\",\n                        measurement_title=\"Relative Optical Density\",\n                        bscore_header=\"b_scores\",\n                        bscore_title=\"B-Score\",\n                        color_header=\"Dataset\",\n                        measurement_threshold=threshold,\n                        b_score_threshold=self.b_score_threshold,\n                    ).facet(row=\"Organism\", column=\"Dataset\"),\n                )\n            )\n\n            subset = get_thresholded_subset(\n                self._processed_only_substances,\n                id_column=\"Internal ID\",\n                negative_controls=self._negative_controls,\n                blanks=self._blanks,\n                threshold=threshold,\n            )\n            for dataset, sub_df in subset.groupby(\"Dataset\"):\n                dummy_df = get_upsetplot_df(sub_df, counts_column=\"Internal ID\")\n\n                result_figures.append(\n                    Result(\n                        dataset,\n                        f\"UpSetPlot_{dataset}\",\n                        figure=UpSetAltair(dummy_df, title=dataset),\n                    )\n                )\n                # ---\n                only_actives = self.results[f\"{dataset}_all_results\"][\n                    self.results[f\"{dataset}_all_results\"]\n                    .groupby(\"Organism\")[\"Relative Optical Density mean\"]\n                    .transform(lambda x: x &lt; threshold)\n                ]\n                result_figures.append(\n                    Result(\n                        dataset,\n                        f\"Scatterplot_BScores_{dataset}\",\n                        figure=measurement_vs_bscore_scatter(\n                            only_actives, show_area=False\n                        ),\n                    )\n                )\n        return result_figures\n\n    @cached_property\n    def _resulttables(self):\n        \"\"\"\n        Retrieves result tables and returns them like list[Resulttable]\n        where Resulttable is a dataclass collecting meta information about the plot.\n        \"\"\"\n\n        # result_plots = dict() # {\"filepath\": plot}\n        result_tables = []\n        # result_tables.append(Result(\"All\", ))\n\n        df = self.processed.copy()\n        df = df[\n            (df[\"Dataset\"] != \"Reference\")\n            &amp; (df[\"Dataset\"] != \"Positive Control\")\n            &amp; (df[\"Dataset\"] != \"Blank\")\n        ].dropna(subset=[\"Concentration\"])\n\n        pivot_df = pd.pivot_table(\n            df,\n            values=[\n                \"Relative Optical Density\",\n                \"Replicate\",\n                \"Z-Factor\",\n                \"Robust Z-Factor\",\n                \"b_scores\",\n            ],\n            index=[\n                \"Internal ID\",\n                \"Organism formatted\",\n                \"Organism\",\n                \"Concentration\",\n                \"Dataset\",\n            ],\n            aggfunc={\n                \"Relative Optical Density\": [\"mean\"],\n                \"Replicate\": [\"count\"],\n                \"b_scores\": [\"mean\"],\n            },\n        ).reset_index()\n\n        pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n\n        for threshold in self.thresholds:\n            # Apply Threshold to % Growth:\n            # pivot_df[f\"Relative Growth &lt; {threshold}\"] = pivot_df.groupby(\n            #     [\"Internal ID\", \"Organism\", \"Dataset\"]\n            # )[\"Relative Optical Density mean\"].transform(lambda x: x &lt; threshold)\n            # Apply B-Score Treshold:\n            # B-Scores &lt;= -3: https://doi.org/10.1128/mbio.00205-25\n            # pivot_df[f\"B Score &lt;= {self.b_score_threshold}\"] = pivot_df.groupby(\n            #     [\"Internal ID\", \"Organism\", \"Dataset\"]\n            # )[\"b_scores mean\"].transform(lambda x: x &lt;= self.b_score_threshold)\n\n            for dataset, dataset_grp in pivot_df.groupby(\"Dataset\"):\n                # dataset = dataset[0]\n                # resultpath = os.path.join(filepath, dataset)\n                # result_tables[f\"{dataset}_all_results\"] = dataset_grp\n                if not self.precipitation.results.empty:\n                    dataset_grp = pd.merge(dataset_grp, self.substances_precipitation)\n                result_tables.append(\n                    Result(dataset, f\"{dataset}_all_results\", table=dataset_grp)\n                )\n\n                # Apply threshold conditions:\n                thresholded_dataset_grp = dataset_grp.groupby(\"Internal ID\").filter(\n                    lambda x: check_activity_conditions(\n                        x[\"Relative Optical Density mean\"],\n                        x[\"b_scores mean\"],\n                        threshold,\n                        self.b_score_threshold,\n                    )\n                )\n\n                # Pivot the long table for excel viewability:\n                pivot_multiindex_df = pd.pivot_table(\n                    thresholded_dataset_grp,\n                    values=[\"Relative Optical Density mean\", \"b_scores mean\"],\n                    index=[\"Internal ID\", \"Dataset\", \"Concentration\", \"Organism\"],\n                    columns=\"Organism formatted\",\n                ).reset_index()\n\n                # pivot_multiindex_df = pd.pivot_table(\n                #     dataset_grp,\n                #     values=[\"Relative Optical Density mean\"],\n                #     index=[\"Internal ID\", \"Dataset\", \"Concentration\"],\n                #     columns=\"Organism\",\n                # ).reset_index()\n                # cols = list(pivot_multiindex_df.columns.droplevel())\n                # cols[:3] = list(map(lambda x: x[0], pivot_multiindex_df.columns[:3]))\n                # pivot_multiindex_df.columns = cols\n\n                # # Apply threshold (active in any organism)\n                # thresholded_pivot = pivot_multiindex_df.iloc[\n                #     list(\n                #         pivot_multiindex_df.iloc[:, 3:].apply(\n                #             lambda x: any(list(map(lambda i: i &lt; threshold, x))), axis=1\n                #         )\n                #     )\n                # ]\n\n                # Sort by columns each organism after the other\n                # return pivot_multiindex_df.sort_values(by=cols[3:])\n\n                # Sort rows by mean between the organisms (lowest mean activity first)\n                # results_sorted_by_mean_activity = pivot_multiindex_df.iloc[\n                #     pivot_multiindex_df.iloc[:, 3:].mean(axis=1).argsort()\n                # ]\n\n                # Sort rows by mean between the organisms (lowest mean measurement first)\n                results_sorted_by_mean_activity = pivot_multiindex_df.loc[\n                    pivot_multiindex_df.loc[\n                        :,\n                        list(\n                            filter(\n                                lambda x: x[0].startswith(\"Relative Optical Density\"),\n                                pivot_multiindex_df.columns,\n                            )\n                        ),\n                    ]\n                    .mean(axis=1)\n                    .argsort()\n                ]\n\n                if not self.precipitation.results.empty:\n                    results_sorted_by_mean_activity = pd.merge(\n                        results_sorted_by_mean_activity, self.substances_precipitation\n                    )\n\n                # Correct \"mean\" header if its only one replicate (remove 'mean')\n                if sum(thresholded_dataset_grp[\"Replicate count\"].unique()) == 1:\n                    results_sorted_by_mean_activity = results_sorted_by_mean_activity.rename(\n                        columns={\n                            \"Relative Optical Density mean\": \"Relative Optical Density\",\n                            \"b_scores mean\": \"B-Score\",\n                        }\n                    )\n\n                results_sorted_by_mean_activity = (\n                    results_sorted_by_mean_activity.rename(\n                        columns={\"b_scores mean\": \"B-Score mean\"}\n                    )\n                )\n\n                result_tables.append(\n                    Result(\n                        dataset,\n                        f\"{dataset}_threshold{round(threshold)}_results\",\n                        table=results_sorted_by_mean_activity,\n                    )\n                )\n        return result_tables\n\n    @cached_property\n    def results(self):\n        \"\"\"\n        Retrieves result tables (from self._resulttables)\n        and returns them in a dictionary like:\n            {\"&lt;filepath&gt;\": pd.DataFrame}\n        \"\"\"\n        return {tbl.file_basename: tbl.table for tbl in self._resulttables}\n\n    def save_figures(self, resultpath, fileformats: list[str] = [\"svg\", \"html\"]):\n        _save_figures(resultpath, self._resultfigures, fileformats=fileformats)\n\n    def save_tables(\n        self, result_path, processed_path, fileformats: list[str] = [\"xlsx\", \"csv\"]\n    ):\n        pathlib.Path(processed_path).mkdir(parents=True, exist_ok=True)\n        self.processed.to_csv(os.path.join(processed_path, \"processed.csv\"))\n        _save_tables(result_path, self._resulttables, fileformats=fileformats)\n\n    def save_results(\n        self,\n        tables_path: str,\n        figures_path: str,\n        processed_path: str,\n        figureformats: list[str] = [\"svg\", \"html\"],\n        tableformats: list[str] = [\"xlsx\", \"csv\"],\n    ):\n        self.save_figures(figures_path, fileformats=figureformats)\n        self.save_tables(tables_path, processed_path, fileformats=tableformats)\n</code></pre>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen.mapped_input_df","title":"<code>mapped_input_df</code>  <code>cached</code> <code>property</code>","text":"<p>Does mapping of the inputfile describing the tested substances with the corresponding mappingfile(s). Basically replaces rda.process.primary_process_inputs() function so all the variables and intermediate results are available via the class</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen.results","title":"<code>results</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves result tables (from self._resulttables) and returns them in a dictionary like:</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen.check_substances","title":"<code>check_substances()</code>","text":"<p>Do some sanity checks for the substances table. - Check if all necessary columns are present. - Check if substances contains missing values. - Check if there are duplicate Internal IDs (references excluded)</p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>def check_substances(self):\n    \"\"\"\n    Do some sanity checks for the substances table.\n    - Check if all necessary columns are present.\n    - Check if substances contains missing values.\n    - Check if there are duplicate Internal IDs (references excluded)\n    \"\"\"\n    # if not all(column in self._substances_unmapped.columns for column in necessary_columns):\n    #     raise ValueError(\n    #         f\"Not all necessary columns are present in the input table.\\n(Necessary columns: {necessary_columns})\"\n    #     )\n    # # Check if all of the necessary column are complete:\n    # if substances[necessary_columns].isnull().values.any():\n    #     raise ValueError(\n    #         \"Input table incomplete, contains NA (missing) values.\"\n    #     )\n    # # Check if there are duplicates in the internal IDs (apart from references)\n    # if any(substances[substances[\"Dataset\"] != \"Reference\"][\"Internal ID\"].duplicated()):\n    #     raise ValueError(\"Duplicate Internal IDs.\")\n    pass\n</code></pre>"},{"location":"reference/parser/","title":"Parsing Functions","text":""},{"location":"reference/parser/#rda_toolbox.parser.collect_metadata","title":"<code>collect_metadata(filedicts)</code>","text":"<p>Helperfunction to collect the metadata from all reader files into a dataframe.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def collect_metadata(filedicts: list[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Helperfunction to collect the metadata from all reader files into a dataframe.\n    \"\"\"\n    allmetadata_df = pd.DataFrame()\n    for filedict in filedicts:\n        meta_df = pd.DataFrame(filedict[\"metadata\"], index=[0])\n        meta_df[\"Barcode\"] = filedict[\"Barcode\"]\n        allmetadata_df = pd.concat([allmetadata_df, meta_df], ignore_index=True)\n    return allmetadata_df\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.collect_results","title":"<code>collect_results(filedicts)</code>","text":"<p>Collect and merge results from the readerfiles.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def collect_results(filedicts: list[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Collect and merge results from the readerfiles.\n    \"\"\"\n    allresults_df = pd.DataFrame(\n        {\"Row\": [], \"Column\": [], \"Raw Optical Density\": []}\n    )  # , \"Layout\": [], \"Concentration\": []})\n    platetype_s = list(set(fd[\"plate_type\"] for fd in filedicts))\n    if len(platetype_s) == 1:\n        platetype = platetype_s[0]\n    else:\n        raise Exception(f\"Different plate types used {platetype_s}\")\n\n    for filedict in filedicts:\n        # long_layout_df = get_long_df(\"Layout\")\n        # long_concentrations_df = get_long_df(\"Concentration\")\n        # long_rawdata_df = get_long_df(\"Raw Optical Density\")\n\n        long_rawdata_df = pd.melt(\n            filedict[\"Raw Optical Density\"].reset_index(names=\"Row\"),\n            id_vars=[\"Row\"],\n            var_name=\"Column\",\n            value_name=\"Raw Optical Density\",\n        )\n\n        long_rawdata_df[\"Barcode\"] = filedict[\"Barcode\"]\n        # df_merged = reduce(\n        #     lambda  left,right: pd.merge(left,right,on=['Row', 'Column'], how='outer'),\n        #     [long_rawdata_df, long_layout_df, long_concentrations_df]\n        # )\n        allresults_df = pd.concat([allresults_df, long_rawdata_df], axis=0)\n        platetype = filedict[\"plate_type\"]\n\n    allresults_df.rename(\n        columns={\"Row\": f\"Row_{platetype}\", \"Column\": f\"Col_{platetype}\"}, inplace=True\n    )\n    return allresults_df.reset_index(drop=True)\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.filepaths_to_filedicts","title":"<code>filepaths_to_filedicts(filepaths)</code>","text":"<p>Wrapper function to obtain a list of dictionaries which contain the raw files information like</p> <ul> <li>different entries of metadata<ul> <li>Plate Type</li> <li>Barcode</li> <li>Date</li> <li>Time</li> <li>etc.</li> </ul> </li> <li>Raw Optical Density (DataFrame)</li> <li>Concentration (DataFrame)</li> <li>Layout (DataFrame)</li> </ul> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def filepaths_to_filedicts(filepaths: list[str]) -&gt; list[dict]:\n    \"\"\"\n    Wrapper function to obtain a list of dictionaries which contain the raw files information like\n\n    - different entries of metadata\n        - Plate Type\n        - Barcode\n        - Date\n        - Time\n        - etc.\n    - Raw Optical Density (DataFrame)\n    - Concentration (DataFrame)\n    - Layout (DataFrame)\n    \"\"\"\n    filedicts = []\n    for path in filepaths:\n        file = open(path)\n        contents = StringIO(file.read())\n        filedicts.append(readerfile_parser(basename(path), contents))\n        file.close()\n    return filedicts\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.parse_mappingfile","title":"<code>parse_mappingfile(filepath, motherplate_column='Origin Plate', childplate_column='AcD Barcode 384')</code>","text":"<p>Simple mappingfile parser function. Expects to start with a \"Motherplate\" line followed by corresponding \"Childplates\" in a single line.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def parse_mappingfile(\n    filepath: str,\n    motherplate_column: str = \"Origin Plate\",\n    childplate_column: str = \"AcD Barcode 384\",\n):\n    \"\"\"\n    Simple mappingfile parser function.\n    Expects to start with a \"Motherplate\" line followed by corresponding \"Childplates\" in a single line.\n    \"\"\"\n    filedict = dict()\n    with open(filepath) as file:\n        filecontents = file.read().splitlines()\n        key = None\n        for i, line in enumerate(filecontents):\n            line = line.split(\";\")\n            if i % 2 == 0:  # if i is even (expect MPs on even lines, alternating with childplates)\n            # if len(line) == 1:\n                key = line[0]\n            else:\n                if not key:\n                    raise ValueError(\n                        \"Motherplate barcode expected on first line.\"\n                    )\n                if key in filedict:\n                    filedict[key].append(line)\n                else:\n                    filedict[key] = [line]\n    mapping_df = pd.DataFrame(\n        [\n            (motherplate, childplate, rep_num, rack_nr)\n            for motherplate, replicates in filedict.items()\n            for rep_num, childplates in enumerate(replicates, start=1)\n            for rack_nr, childplate in enumerate(childplates, start=1)\n        ],\n        columns=[motherplate_column, childplate_column, \"Replicate\", \"Rack\"],\n    )\n    return mapping_df\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.parse_readerfiles","title":"<code>parse_readerfiles(path)</code>","text":"<p>Reads CytationC10 readerfiles (plain text files) and merges the results into a DataFrame which is returned. Wrapper for readerfiles_rawdf to keep backwards compatibility. Improves readerfiles_rawdf, provide a single path for convenience.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def parse_readerfiles(path: str | None) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Reads CytationC10 readerfiles (plain text files) and merges the results into a DataFrame which is returned.\n    Wrapper for readerfiles_rawdf to keep backwards compatibility.\n    Improves readerfiles_rawdf, provide a single path for convenience.\n    \"\"\"\n    if not path:\n        return None\n    paths = [\n            os.path.join(path, f)\n            for f in os.listdir(path)\n            if os.path.isfile(os.path.join(path, f))\n    ]\n    df = readerfiles_rawdf(paths)\n    df[\"Col_384\"] = df[\"Col_384\"].astype(int)\n    return df\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.process_inputfile","title":"<code>process_inputfile(file_object)</code>","text":"Read Input excel file which should have the following columns <ul> <li>Barcode</li> <li>Organism</li> <li>Row_384</li> <li>Col_384</li> <li>ID</li> </ul> <p>Optional columns:     - Concentration in mg/mL (or other units)     - Cutoff</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def process_inputfile(file_object):\n    \"\"\"\n    Read Input excel file which should have the following columns:\n        - Barcode\n        - Organism\n        - Row_384\n        - Col_384\n        - ID\n    Optional columns:\n        - Concentration in mg/mL (or other units)\n        - Cutoff\n    \"\"\"\n    if not file_object:\n        return None\n    excel_file = pd.ExcelFile(file_object)\n    substance_df = pd.read_excel(excel_file, \"substances\")\n    layout_df = pd.read_excel(excel_file, \"layout\")\n    df = pd.merge(layout_df, substance_df, how=\"cross\")\n    # df.rename(columns={\n    #     \"barcode\": \"Barcode\",\n    #     \"replicate\": \"Replicate\",\n    #     \"organism\": \"Organism\",\n    #     \"plate_row\": \"Row_384\",\n    #     \"plate_column\": \"Col_384\",\n    #     \"id\": \"ID\",\n    #     \"concentration\": \"Concentration in mg/mL\",\n    # }, inplace=True)\n    df[\"ID\"] = df[\"ID\"].astype(str)\n    return df\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.read_platemapping","title":"<code>read_platemapping(filepath, orig_barcodes)</code>","text":"<p>Reads a mappingfile generated by the barcode reader.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def read_platemapping(filepath: str, orig_barcodes: list[str]):\n    \"\"\"\n    Reads a mappingfile generated by the barcode reader.\n\n    \"\"\"\n    filedict = dict()\n    orig_barcodes = list(map(str, orig_barcodes))\n    with open(filepath) as file:\n        filecontents = file.read().splitlines()\n        origin_barcode = \"\"\n        origin_replicates = []\n        for line in filecontents:\n            line = line.split(\";\")\n            if len(line) == 1 and line[0] in orig_barcodes:\n                origin_barcode = line[0]\n                origin_replicates.append(origin_barcode)\n                # print(\"Origin barcode: \", origin_barcode)\n                if origin_barcode not in filedict:\n                    filedict[origin_barcode] = []\n            else:\n                filedict[origin_barcode].append(line)\n        replicates_dict = {i:origin_replicates.count(i) for i in origin_replicates}\n        if sorted(list(filedict.keys())) != sorted(orig_barcodes):\n            raise ValueError(\n                f\"The origin barcodes from the mappingfile '{os.path.basename(filepath)}' and MP barcodes in MIC_input.xlsx do not coincide.\"\n            )\n        return filedict, replicates_dict\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.readerfile_parser","title":"<code>readerfile_parser(filename, file_object, resulttable_header='Results')</code>","text":"<p>Parser for files created by the BioTek Cytation C10 Confocal Imaging Reader.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def readerfile_parser(\n    filename: str, file_object: StringIO, resulttable_header: str = \"Results\"\n) -&gt; dict:\n    \"\"\"\n    Parser for files created by the BioTek Cytation C10 Confocal Imaging Reader.\n    \"\"\"\n    lines = file_object.readlines()\n    lines = list(filter(None, map(lambda x: x.strip(\"\\n\").strip(\"\\r\"), lines)))\n    if len(lines) == 0:\n        raise ValueError(f\"Empty raw file {filename}.\")\n\n    # search the file for plate type definition and use it to derive number of rows and columns\n    found_plate_type = re.findall(r\"Plate Type;[A-z ]*([0-9]*)\", \"\".join(lines))\n    plate_type = 96  # define default plate type and let it be 96-well plate as this is what we started with\n    if found_plate_type:\n        plate_type = int(found_plate_type[0])\n\n    num_rows, num_columns = get_rows_cols(plate_type)\n\n    filedict = dict()\n    metadata = dict()\n    filedict[\"Reader Filename\"] = filename\n    filedict[\"plate_type\"] = plate_type\n    # TODO: get barcode via regex\n    barcode_found = re.findall(\n        r\"\\d{3}[A-Z][a-z]?[a-zA-Z]\\d{2}\\d{3}\", filedict[\"Reader Filename\"]\n    )\n    if not barcode_found:\n        filedict[\"Barcode\"] = filedict[\"Reader Filename\"]\n    else:\n        filedict[\"Barcode\"] = barcode_found[0]\n    # filedict[\"Barcode\"] = Path(filedict[\"Reader Filename\"]).stem.split(\"_\")[-1]\n\n    results = np.empty([num_rows, num_columns], dtype=float)\n    # using dtype=str results in unicode strings of length 1 ('U1'), therefore we use 'U25'\n    layout = np.empty([num_rows, num_columns], dtype=\"U25\")\n    concentrations = np.empty([num_rows, num_columns], dtype=float)\n\n    metadata_regex = r\";?([a-zA-Z0-9 \\/]*)[;:]+([a-zA-Z0-9 \\/\\\\:_.-]*),?\"\n    line_num = 0\n    while line_num &lt; len(lines):\n        if lines[line_num] == resulttable_header:\n            line_num += 1\n            header = map(\n                int, lines[line_num].strip(\"\\n\").split(\";\")[1:]\n            )  # get the header\n            index = [\"\"] * num_rows\n            for _row_num in range(num_rows):  # for the next num_rows, read result data\n                line_num += 1\n                res_line = lines[line_num].split(\";\")\n                # Split at ; and slice off rowlabel and excitation/emission value:\n                index[_row_num] = res_line[0]\n                results[_row_num] = res_line[1:-1]\n            # Initialize DataFrame from results and add it to filedict\n            filedict[\"Raw Optical Density\"] = pd.DataFrame(\n                data=results, index=index, columns=header\n            )\n            line_num += 1\n        elif lines[line_num] == \"Layout\":  # For the next num_rows, read layout data\n            line_num += 1\n            header = list(\n                map(int, lines[line_num].strip(\"\\n\").split(\";\")[1:])\n            )  # Because we use header twice here, we collect it via list()\n            index = [\"\"] * num_rows\n            for _row_num in range(num_rows):\n                line_num += 1\n                layout_line = lines[line_num].split(\";\")\n                index[_row_num] = layout_line[0]\n                layout[_row_num] = layout_line[1:-1]\n                # Each second line yields a concentration layout line\n                line_num += 1\n                conc_line = lines[line_num].split(\";\")\n                concentrations[_row_num] = [\n                    None if not x else float(x) for x in conc_line[1:-1]\n                ]\n            # Add layouts to filedict\n            filedict[\"Layout\"] = pd.DataFrame(data=layout, index=index, columns=header)\n            filedict[\"Concentration\"] = pd.DataFrame(\n                data=concentrations, index=index, columns=header\n            )\n            line_num += 1\n        else:\n            metadata_pairs = re.findall(metadata_regex, lines[line_num])\n            line_num += 1\n            if not metadata_pairs:\n                continue\n            else:\n                for key, value in metadata_pairs:\n                    if not all(\n                        [key, value]\n                    ):  # if any of the keys or values are empty, skip\n                        continue\n                    else:\n                        metadata[key.strip(\" :\")] = value.strip(\" \")\n    filedict[\"metadata\"] = metadata\n    return filedict\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.readerfiles_metadf","title":"<code>readerfiles_metadf(paths)</code>","text":"<p>Parses metadata from files declared by filepaths and merges the results into a DataFrame.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def readerfiles_metadf(paths: list[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Parses metadata from files declared by filepaths and merges the results into a DataFrame.\n    \"\"\"\n    filedicts = filepaths_to_filedicts(paths)\n    return collect_metadata(filedicts)\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.readerfiles_rawdf","title":"<code>readerfiles_rawdf(paths)</code>","text":"<p>Parses data from files declared by filepaths and merges the results into a DataFrame :param paths: A list of filepaths corresponding to the raw reader files generated by Cytation10 :type paths: list[str] :return: A DataFrame in tidy and long format with the raw readerfile contents :rtype: pd.DataFrame</p> <p>:Example:</p> <pre><code>```Python\nimport glob\n\nrawdata_df = readerfiles_rawdf(glob.glob(\"path/to/raw/files/*\"))\n```\n</code></pre> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def readerfiles_rawdf(paths: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Parses data from files declared by filepaths and merges the results into a DataFrame\n    :param paths: A list of filepaths corresponding to the raw reader files generated by Cytation10\n    :type paths: list[str]\n    :return: A DataFrame in tidy and long format with the raw readerfile contents\n    :rtype: pd.DataFrame\n\n    :Example:\n\n        ```Python\n        import glob\n\n        rawdata_df = readerfiles_rawdf(glob.glob(\"path/to/raw/files/*\"))\n        ```\n    \"\"\"\n    filedicts = filepaths_to_filedicts(paths)\n    rawdata = collect_results(filedicts)\n    rawdata[\"Col_384\"] = rawdata[\"Col_384\"].astype(str)\n    rawdata.rename(columns={\"Barcode\": \"AcD Barcode 384\"}, inplace=True)\n    return rawdata\n</code></pre>"},{"location":"reference/plot/","title":"Plotting Functions","text":""},{"location":"reference/plot/#rda_toolbox.plot.UpSetAltair","title":"<code>UpSetAltair(data=None, title='', subtitle='', sets=None, abbre=None, sort_by='frequency', sort_by_order='ascending', inter_degree_frequency='ascending', width=1200, height=700, height_ratio=0.6, horizontal_bar_chart_width=300, set_colors_dict=dict(), highlight_color='#777777', glyph_size=200, set_label_bg_size=1000, line_connection_size=2, horizontal_bar_size=20, vertical_bar_label_size=16, vertical_bar_padding=20, set_labelstyle='normal')</code>","text":"<p>This function generates Altair-based interactive UpSet plots.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Tabular data containing the membership of each element (row) in exclusive intersecting sets (column).</p> <code>None</code> <code>sets</code> <code>list</code> <p>List of set names of interest to show in the UpSet plots. This list reflects the order of sets to be shown in the plots as well.</p> <code>None</code> <code>abbre</code> <code>dict</code> <p>Dictionary mapping set names to abbreviated set names.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>\"frequency\" or \"degree\"</p> <code>'frequency'</code> <code>sort_by_order</code> <code>str</code> <p>\"ascending\" or \"descending\"</p> <code>'ascending'</code> <code>inter_degree_frequency</code> <code>str</code> <p>\"ascending\" or \"descending\", only makes sense if sort_by=\"degree\"</p> <code>'ascending'</code> <code>width</code> <code>int</code> <p>Vertical size of the UpSet plot.</p> <code>1200</code> <code>height</code> <code>int</code> <p>Horizontal size of the UpSet plot.</p> <code>700</code> <code>height_ratio</code> <code>float</code> <p>Ratio of height between upper and under views, ranges from 0 to 1.</p> <code>0.6</code> <code>horizontal_bar_chart_width</code> <code>int</code> <p>Width of horizontal bar chart on the bottom-right.</p> <code>300</code> <code>set_colors_dict</code> <code>dict</code> <p>Dictionary containing the sets as keys with corresponding colors as values</p> <code>dict()</code> <code>highlight_color</code> <code>str</code> <p>Color to encode intersecting sets upon mouse hover.</p> <code>'#777777'</code> <code>glyph_size</code> <code>int</code> <p>Size of UpSet glyph (\u2b24).</p> <code>200</code> <code>set_label_bg_size</code> <code>int</code> <p>Size of label background in the horizontal bar chart.</p> <code>1000</code> <code>line_connection_size</code> <code>int</code> <p>width of lines in matrix view.</p> <code>2</code> <code>horizontal_bar_size</code> <code>int</code> <p>Height of bars in the horizontal bar chart.</p> <code>20</code> <code>vertical_bar_label_size</code> <code>int</code> <p>Font size of texts in the vertical bar chart on the top.</p> <code>16</code> <code>vertical_bar_padding</code> <code>int</code> <p>Gap between a pair of bars in the vertical bar charts.</p> <code>20</code> <code>set_labelstyle</code> <code>str</code> <p>\"normal\" (default) or \"italic\"</p> <code>'normal'</code> <p>Run rda.utility.get_upsetplot_df() on the df before trying this function.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def UpSetAltair(\n    data=None,\n    title=\"\",\n    subtitle=\"\",\n    sets=None,\n    abbre=None,\n    sort_by=\"frequency\",\n    sort_by_order=\"ascending\",\n    inter_degree_frequency=\"ascending\",\n    width=1200,\n    height=700,\n    height_ratio=0.6,\n    horizontal_bar_chart_width=300,\n    set_colors_dict=dict(),\n    highlight_color=\"#777777\",\n    glyph_size=200,\n    set_label_bg_size=1000,\n    line_connection_size=2,\n    horizontal_bar_size=20,\n    vertical_bar_label_size=16,\n    vertical_bar_padding=20,\n    set_labelstyle=\"normal\",\n):\n    \"\"\"This function generates Altair-based interactive UpSet plots.\n\n    Parameters:\n          data (pandas.DataFrame): Tabular data containing the membership of each element (row) in exclusive intersecting sets (column).\n          sets (list): List of set names of interest to show in the UpSet plots. This list reflects the order of sets to be shown in the plots as well.\n          abbre (dict): Dictionary mapping set names to abbreviated set names.\n          sort_by (str): \"frequency\" or \"degree\"\n          sort_by_order (str): \"ascending\" or \"descending\"\n          inter_degree_frequency (str): \"ascending\" or \"descending\", only makes sense if sort_by=\"degree\"\n          width (int): Vertical size of the UpSet plot.\n          height (int): Horizontal size of the UpSet plot.\n          height_ratio (float): Ratio of height between upper and under views, ranges from 0 to 1.\n          horizontal_bar_chart_width (int): Width of horizontal bar chart on the bottom-right.\n          set_colors_dict (dict): Dictionary containing the sets as keys with corresponding colors as values\n          highlight_color (str): Color to encode intersecting sets upon mouse hover.\n          glyph_size (int): Size of UpSet glyph (\u2b24).\n          set_label_bg_size (int): Size of label background in the horizontal bar chart.\n          line_connection_size (int): width of lines in matrix view.\n          horizontal_bar_size (int): Height of bars in the horizontal bar chart.\n          vertical_bar_label_size (int): Font size of texts in the vertical bar chart on the top.\n          vertical_bar_padding (int): Gap between a pair of bars in the vertical bar charts.\n          set_labelstyle (str): \"normal\" (default) or \"italic\"\n\n    Run rda.utility.get_upsetplot_df() on the df before trying this function.\n    \"\"\"\n\n    if data is None:\n        print(\"No data and/or a list of sets are provided\")\n        return\n    if sets is None:\n        sets = list(data.columns[1:])\n\n    if (height_ratio &lt; 0) or (1 &lt; height_ratio):\n        print(\"height_ratio set to 0.5\")\n        height_ratio = 0.5\n    if not abbre:\n        abbre = {set: set for set in sets}\n    if len(sets) != len(abbre):\n        abbre = sets\n        print(\n            \"Dropping the `abbre` list because the lengths of `sets` and `abbre` are not identical.\"\n        )\n    if not set_colors_dict:  # build default colors dict\n        colors = [  # observable10\n            \"#4269d0\",\n            \"#efb118\",\n            \"#ff725c\",\n            \"#6cc5b0\",\n            \"#3ca951\",\n            \"#ff8ab7\",\n            \"#a463f2\",\n            \"#97bbf5\",\n            \"#9c6b4e\",\n            \"#9498a0\",\n        ]\n        if len(sets) &gt; len(colors):\n            colors = colors * len(sets)\n        set_colors_dict = {key: value for key, value in zip(sets, colors[: len(sets)])}\n    else:\n        if sorted(list(set_colors_dict.keys())) != sorted(sets):\n            raise ValueError(\n                f\"Wrong set names, correct names are:\\n{dict((set, '') for set in sets)}\"\n            )\n    # filter set_colors_dict with the sets which are actually in the data df (sets)\n    # this might be needed if set_colors_dict if more comprehensive than the data\n    set_colors_dict = {\n        key: value for key, value in set_colors_dict.items() if key in sets\n    }\n    \"\"\"\n    Data Preprocessing\n    \"\"\"\n    data = data.copy()\n    data[\"count\"] = 0\n    data = data[sets + [\"count\"]]\n    data = data.groupby(sets).count().reset_index()\n\n    data[\"intersection_id\"] = data.index\n    data[\"degree\"] = data[sets].sum(axis=1)\n    data = data.sort_values(\n        by=[\"count\"],\n        ascending=True if inter_degree_frequency == \"ascending\" else False,\n    )\n\n    data = pd.melt(data, id_vars=[\"intersection_id\", \"count\", \"degree\"])\n    data = data.rename(columns={\"variable\": \"set\", \"value\": \"is_intersect\"})\n\n    set_to_abbre = pd.DataFrame(abbre.items(), columns=[\"set\", \"set_abbre\"])\n\n    set_to_order = (\n        data[data[\"is_intersect\"] == 1]\n        .groupby(\"set\")\n        .sum()\n        .reset_index()\n        .sort_values(by=\"count\", ascending=False)\n        .filter([\"set\"])\n    )\n    set_to_order[\"set_order\"] = list(range(len(sets)))\n\n    degree_calculation = \"\"\n    for s in sets:\n        degree_calculation += f\"(isDefined(datum['{s}']) ? datum['{s}'] : 0)\"\n        if sets[-1] != s:\n            degree_calculation += \"+\"\n    \"\"\"\n    Selections\n    \"\"\"\n    legend_selection = alt.selection_point(fields=[\"set\"], bind=\"legend\")\n    color_selection = alt.selection_point(\n        fields=[\"intersection_id\"], on=\"pointerover\", empty=False\n    )\n    opacity_selection = alt.selection_point(fields=[\"intersection_id\"])\n\n    \"\"\"\n    Styles\n    \"\"\"\n    vertical_bar_chart_height = height * height_ratio\n    matrix_height = height - vertical_bar_chart_height\n    matrix_width = width - horizontal_bar_chart_width\n\n    vertical_bar_size = min(\n        30,\n        width / len(data[\"intersection_id\"].unique().tolist()) - vertical_bar_padding,\n    )\n\n    main_color = \"#3A3A3A\"\n    brush_opacity = alt.condition(~opacity_selection, alt.value(1), alt.value(0.6))\n    brush_color = alt.condition(\n        color_selection, alt.value(highlight_color), alt.value(main_color)\n    )\n    is_show_horizontal_bar_label_bg = len(list(abbre.values())[0]) &lt;= 2\n    horizontal_bar_label_bg_color = (\n        \"white\" if is_show_horizontal_bar_label_bg else \"black\"\n    )\n\n    x_sort = alt.Sort(\n        field=\"count\" if sort_by == \"frequency\" else \"degree\",\n        order=sort_by_order,\n    )\n\n    tooltip = [\n        alt.Tooltip(\"max(count):Q\", title=\"Cardinality\"),\n        alt.Tooltip(\"degree:Q\", title=\"Degree\"),\n    ]\n    \"\"\"\n    Plots\n    \"\"\"\n    # To use native interactivity in Altair, we are using the data transformation functions\n    # supported in Altair.\n    base = (\n        alt.Chart(data)\n        .transform_pivot(\n            \"set\",\n            op=\"max\",\n            groupby=[\"intersection_id\", \"count\"],\n            value=\"is_intersect\",\n        )\n        .transform_aggregate(\n            # count, set1, set2, ...\n            count=\"sum(count)\",\n            groupby=sets,\n        )\n        .transform_calculate(\n            # count, set1, set2, ...\n            degree=degree_calculation\n        )\n        .transform_filter(\n            # count, set1, set2, ..., degree\n            alt.datum[\"degree\"]\n            != 0\n        )\n        .transform_window(\n            # count, set1, set2, ..., degree\n            intersection_id=\"row_number()\",\n            frame=[None, None],\n        )\n        .transform_fold(\n            # count, set1, set2, ..., degree, intersection_id\n            sets,\n            as_=[\"set\", \"is_intersect\"],\n        )\n        .transform_lookup(\n            # count, set, is_intersect, degree, intersection_id\n            lookup=\"set\",\n            from_=alt.LookupData(set_to_abbre, \"set\", [\"set_abbre\"]),\n        )\n        .transform_lookup(\n            # count, set, is_intersect, degree, intersection_id, set_abbre\n            lookup=\"set\",\n            from_=alt.LookupData(set_to_order, \"set\", [\"set_order\"]),\n        )\n        .transform_filter(\n            # Make sure to remove the filtered sets.\n            legend_selection\n        )\n        .transform_window(\n            # count, set, is_intersect, degree, intersection_id, set_abbre\n            set_order=\"distinct(set)\",\n            frame=[None, 0],\n            sort=[{\"field\": \"set_order\"}],\n        )\n        .transform_lookup(\n            lookup=\"set\",\n            from_=alt.LookupData(set_to_order, \"set\", [\"set_order\"]),\n        )\n    )\n\n    vertical_bar = (\n        base.mark_bar(color=main_color)  # , size=vertical_bar_size)\n        .encode(\n            x=alt.X(\n                \"intersection_id:N\",\n                axis=alt.Axis(grid=False, labels=False, ticks=False, domain=True),\n                sort=x_sort,\n                title=None,\n            ),\n            y=alt.Y(\n                \"max(count):Q\",\n                axis=alt.Axis(grid=False, tickCount=3, orient=\"right\"),\n                title=\"Intersection Size\",\n            ),\n            color=brush_color,\n            tooltip=tooltip,\n        )\n        .properties(width=matrix_width, height=vertical_bar_chart_height)\n    )\n    vertical_bar_text = vertical_bar.mark_text(\n        color=main_color, dy=-10, size=vertical_bar_label_size, fontSize=20\n    ).encode(text=alt.Text(\"count:Q\", format=\".0f\"))\n    vertical_bar_chart = (vertical_bar + vertical_bar_text).add_params(\n        color_selection,\n    )\n\n    circle_bg = (\n        vertical_bar.mark_circle(size=glyph_size, opacity=1)\n        .encode(\n            x=alt.X(\n                \"intersection_id:N\",\n                axis=alt.Axis(grid=False, labels=False, ticks=False, domain=False),\n                sort=x_sort,\n                title=None,\n            ),\n            y=alt.Y(\n                \"set_order:N\",\n                axis=alt.Axis(grid=False, labels=False, ticks=False, domain=False),\n                title=None,\n            ),\n            color=alt.value(\"#E6E6E6\"),\n        )\n        .properties(height=matrix_height)\n    )\n    rect_bg = (\n        circle_bg.mark_rect()\n        .transform_filter(alt.datum[\"set_order\"] % 2 == 1)\n        .encode(color=alt.value(\"#F7F7F7\"))\n    )\n    circle = circle_bg.transform_filter(alt.datum[\"is_intersect\"] == 1).encode(\n        color=brush_color\n    )\n    line_connection = (\n        circle_bg.mark_bar(size=line_connection_size, color=main_color)\n        .transform_filter(alt.datum[\"is_intersect\"] == 1)\n        .encode(\n            y=alt.Y(\"min(set_order):N\"),\n            y2=alt.Y2(\"max(set_order):N\"),\n            color=brush_color,\n        )\n    )\n    matrix_view = alt.layer(\n        circle + rect_bg + circle_bg + line_connection + circle\n    ).add_params(\n        # Duplicate `circle` is to properly show tooltips.\n        color_selection,\n    )\n\n    # Cardinality by sets (horizontal bar chart)\n    horizontal_bar_label_bg = base.mark_circle(size=set_label_bg_size).encode(\n        y=alt.Y(\n            \"set_order:N\",\n            axis=alt.Axis(grid=False, labels=False, ticks=False, domain=False),\n            title=None,\n        ),\n        color=alt.Color(\n            \"set:N\",\n            scale=alt.Scale(\n                domain=list(set_colors_dict.keys()),\n                range=list(set_colors_dict.values()),\n            ),\n            title=None,\n        ),\n        opacity=alt.value(1),\n    )\n    horizontal_bar_label = horizontal_bar_label_bg.mark_text(\n        align=(\"center\" if is_show_horizontal_bar_label_bg else \"center\"),\n        fontSize=20,\n        fontStyle=set_labelstyle,\n    ).encode(\n        text=alt.Text(\"set_abbre:N\"),\n        color=alt.value(horizontal_bar_label_bg_color),\n    )\n    horizontal_bar_axis = (\n        (horizontal_bar_label_bg + horizontal_bar_label)\n        if is_show_horizontal_bar_label_bg\n        else horizontal_bar_label\n    )\n\n    horizontal_bar = (\n        horizontal_bar_label_bg.mark_bar(size=horizontal_bar_size)\n        .transform_filter(alt.datum[\"is_intersect\"] == 1)\n        .encode(\n            x=alt.X(\n                \"sum(count):Q\",\n                axis=alt.Axis(grid=False, tickCount=3),\n                title=\"Set Size\",\n                # scale=alt.Scale(range=color_range)\n            ),\n            # color=alt.Color(None,legend=None), # remove interactivity, color and legend\n        )\n        .properties(width=horizontal_bar_chart_width)\n    )\n    horizontal_bar_text = horizontal_bar.mark_text(\n        align=\"left\", dx=2, fontSize=20\n    ).encode(text=\"sum(count):Q\")\n    horizontal_bar_chart = alt.layer(horizontal_bar, horizontal_bar_text)\n    # Concat Plots\n    upsetaltair = alt.vconcat(\n        vertical_bar_chart,\n        alt.hconcat(\n            matrix_view,\n            horizontal_bar_axis,\n            horizontal_bar_chart,\n            spacing=5,\n        ).resolve_scale(y=\"shared\"),\n        spacing=20,\n    ).add_params(\n        legend_selection,\n    )\n\n    # Apply top-level configuration\n    upsetaltair = upsetaltair_top_level_configuration(\n        upsetaltair,\n        legend_orient=\"top\",\n        legend_symbol_size=set_label_bg_size / 2.0,\n    ).properties(\n        title={\n            \"text\": title,\n            \"subtitle\": subtitle,\n            \"fontSize\": 20,\n            \"fontWeight\": 500,\n            \"subtitleColor\": main_color,\n            \"subtitleFontSize\": 14,\n        }\n    )\n    return upsetaltair\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.UpSet_per_dataset","title":"<code>UpSet_per_dataset(df, save_formats=['pdf', 'svg'], id_column='Internal ID')</code>","text":"<p>UpsetPlot wrapper function which applies threshold to processed data (without controls, references etc.). For each dataset present in the given df, create a dummy_df for rda.UpSetAltair() and save the UpSetPlot.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def UpSet_per_dataset(\n    df: pd.DataFrame,  # processed\n    save_formats=[\"pdf\", \"svg\"],\n    id_column=\"Internal ID\",\n):\n    \"\"\"\n    UpsetPlot wrapper function which applies threshold to processed data (without controls, references etc.).\n    For each dataset present in the given df, create a dummy_df for rda.UpSetAltair() and save the UpSetPlot.\n    \"\"\"\n    subset = get_thresholded_subset(\n        df,\n        id_column=\"Internal ID\",\n        negative_controls=\"Bacteria + Medium\",\n        blanks=\"Medium\",\n        threshold=50,\n    )\n\n    for dataset, sub_df in subset.groupby(\"Dataset\"):\n        dummy_df = get_upsetplot_df(sub_df, counts_column=id_column)\n        # Create dataset folder if non-existent\n        pathlib.Path(f\"../figures/{dataset}\").mkdir(parents=True, exist_ok=True)\n        for save_format in save_formats:\n            filename = f\"../figures/{dataset}/UpSetPlot_{dataset}.{save_format}\"\n            print(\"Saving\", filename)\n            dataset_upsetplot = UpSetAltair(dummy_df, title=dataset).save(filename)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.lineplots_facet","title":"<code>lineplots_facet(df, hline_y=50, by_id='Internal ID', whisker_width=10, exclude_negative_zfactors=True, threshold=50.0)</code>","text":"<p>Assay: MIC Input: processed_df Output: Altair Chart with faceted lineplots. Negative controls and blanks are dropped inside the function.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def lineplots_facet(\n    df: pd.DataFrame,\n    hline_y: int=50,\n    by_id: str=\"Internal ID\",\n    whisker_width: int=10,\n    exclude_negative_zfactors: bool=True,\n    threshold: float=50.0,\n) -&gt; alt.vegalite.v5.api.HConcatChart:\n    \"\"\"\n    Assay: MIC\n    Input: processed_df\n    Output: Altair Chart with faceted lineplots.\n    Negative controls and blanks are dropped inside the function.\n    \"\"\"\n    df = prepare_visualization(\n        df, by_id=by_id, exclude_negative_zfactors=exclude_negative_zfactors, threshold=threshold\n    )\n    hline_y = 50\n    organism_columns = []\n\n    color = alt.condition(\n        # alt.datum.Concentration\n        alt.datum.max_conc_below_threshold,\n        alt.Color(f\"{by_id}:N\"),\n        alt.value(\"lightgray\"),\n    )\n    for organism, org_data in df.groupby([\"Organism\"]):\n        base = alt.Chart(org_data).encode(color=color)  # , title=organism)\n        lineplot = base.mark_line(point=True, size=0.8).encode(\n            x=alt.X(\n                \"Concentration:O\",\n                title=\"Concentration in \u00b5M\",\n                axis=alt.Axis(labelAngle=-45, format=\".2e\", formatType=\"number\"),\n            ),\n            y=alt.Y(\n                \"Mean Relative Optical Density:Q\",\n                title=\"Relative Optical Density\",\n                scale=alt.Scale(domain=[-20, 160], clamp=True),\n            ),\n            # color=\"Internal ID:N\",\n            shape=alt.Shape(\"External ID:N\", legend=None),\n            # color=color,\n            tooltip=[\n                \"Internal ID\",\n                \"External ID\",\n                \"Organism\",\n                \"Dataset\",\n                \"Concentration\",\n                \"Used Replicates\",\n                \"Raw Optical Density\",\n                \"Mean Relative Optical Density\",\n                r\"Std\\. Relative Optical Density\",\n                \"Z-Factor\",\n            ],\n        )\n\n        error_bars = base.mark_rule().encode(\n            x=\"Concentration:O\",\n            y=\"uerror:Q\",\n            y2=\"lerror:Q\",\n        )\n        uerror_whiskers = base.mark_tick(size=whisker_width).encode(\n            x=\"Concentration:O\",\n            y=\"uerror:Q\",\n        )\n        lerror_whiskers = base.mark_tick(size=whisker_width).encode(\n            x=\"Concentration:O\",\n            y=\"lerror:Q\",\n        )\n\n        hline = base.mark_rule(strokeDash=[3, 2]).encode(\n            y=alt.datum(hline_y),\n            # x=[alt.value(0), alt.value(50)],\n            color=alt.value(\"black\"),\n        )\n\n        org_column = (\n            alt.layer(lineplot, error_bars, uerror_whiskers, lerror_whiskers, hline)\n            .facet(\n                row=\"AsT Barcode 384\",\n                column=\"AsT Plate Subgroup\",\n                title=alt.Title(organism, anchor=\"middle\"),\n            )\n            .resolve_axis(x=\"independent\")\n            .resolve_scale(color=\"independent\", shape=\"independent\")\n            # .add_params(selection)\n        )\n\n        organism_columns.append(org_column)\n    return alt.hconcat(*organism_columns).configure_point(size=60)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.measurement_vs_bscore_scatter","title":"<code>measurement_vs_bscore_scatter(df, measurement_header='Relative Optical Density mean', measurement_title='Relative Optical Density', bscore_header='b_scores mean', bscore_title='B-Score', color_header='Organism', show_area=True, measurement_threshold=50, b_score_threshold=-3)</code>","text":"<p>Creates a scatter plot for Primary Screens plotting the raw measurement values against B-Scores. Dont forget to exclude controls from the given DF.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def measurement_vs_bscore_scatter(\n    df: pd.DataFrame,\n    measurement_header: str = \"Relative Optical Density mean\",\n    measurement_title: str = \"Relative Optical Density\",\n    bscore_header: str = \"b_scores mean\",\n    bscore_title: str = \"B-Score\",\n    color_header: str = \"Organism\",\n    show_area: bool = True,\n    measurement_threshold: float = 50,\n    b_score_threshold: float = -3,\n):\n    \"\"\"\n    Creates a scatter plot for Primary Screens plotting the raw measurement values against B-Scores.\n    Dont forget to exclude controls from the given DF.\n    \"\"\"\n    chart_df = df.copy()\n    # Add values for thresholds\n    chart_df[\"Growth Threshold\"] = measurement_threshold\n    chart_df[\"B-Score Threshold\"] = b_score_threshold\n    base = alt.Chart(chart_df, width=600)\n    chart = base.mark_circle().encode(\n        x=alt.X(f\"{bscore_header}:Q\", title=bscore_title),\n        y=alt.Y(\n            f\"{measurement_header}:Q\",\n            scale=alt.Scale(reverse=True),\n            title=measurement_title,\n        ),\n        color=f\"{color_header}:N\",\n    )\n    growth_threshold_rule = base.mark_rule(color=\"blue\", strokeDash=[4.4]).encode(\n        y=\"Growth Threshold:Q\"\n    )\n    bscore_threshold_rule = base.mark_rule(color=\"red\", strokeDash=[4.4]).encode(\n        x=\"B-Score Threshold:Q\"\n    )\n\n    rect = base.mark_rect(color=\"blue\").encode(\n        y=f\"min({measurement_header}):Q\",\n        y2=\"Growth Threshold:Q\",\n        x=\"B-Score Threshold:Q\",\n        x2=f\"min({bscore_header}):Q\",\n        opacity=alt.value(0.2),\n    )\n\n    if show_area:\n        return alt.layer(chart, growth_threshold_rule, bscore_threshold_rule, rect)\n    else:\n        return alt.layer(chart, growth_threshold_rule, bscore_threshold_rule)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.mic_hitstogram","title":"<code>mic_hitstogram(data, mic_col, title='Count Distribution of Hits over Concentration')</code>","text":"<p>It's a Hi(t)stogram... Plots distribution of hits over determined MICs. Example: mic_distribution_overview(mic_results_long, 'MIC50 in \u00b5M')</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def mic_hitstogram(\n    data, mic_col, title=\"Count Distribution of Hits over Concentration\"\n):\n    \"\"\"\n    It's a Hi(t)stogram...\n    Plots distribution of hits over determined MICs.\n    Example: mic_distribution_overview(mic_results_long, 'MIC50 in \u00b5M')\n    \"\"\"\n    data = data.dropna(subset=[mic_col])\n    bars = (\n        alt.Chart(data, title=alt.Title(title))\n        .mark_bar()\n        .encode(\n            x=alt.X(f\"{mic_col}:O\"),\n            y=alt.Y(\"count(Internal ID):Q\"),\n            xOffset=\"Organism:N\",\n            color=\"Organism:N\",\n        )\n    )\n    text = (\n        alt.Chart(data)\n        .mark_text(dx=0, dy=-5)\n        .encode(\n            x=alt.X(f\"{mic_col}:O\"),\n            y=alt.Y(\"count(Internal ID):Q\"),\n            text=alt.Text(\"count(Internal ID):Q\"),\n            xOffset=\"Organism:N\",\n            color=\"Organism:N\",\n        )\n    )\n\n    return alt.layer(bars, text)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.plateheatmaps","title":"<code>plateheatmaps(df, substance_id='ID', measurement='Raw Optical Density', barcode='Barcode', negative_control='Negative Control', blank='Medium')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with relevant data</p> required <code>substance_id</code> <code>str</code> <p>column name in df containing the unique substance id</p> <code>'ID'</code> <code>measurement</code> <code>str</code> <p>column name in df with the measurements to colorize via heatmaps</p> <code>'Raw Optical Density'</code> <code>negative_control</code> <code>str</code> <p>controls with organism + medium</p> <code>'Negative Control'</code> <code>blank</code> <code>str</code> <p>controls with only medium (no organism and therefore no growth)</p> <code>'Medium'</code> <p>Plots heatmaps of the plates from df in a gridlike manner. Exclude unwanted plates, for example Blanks from the df outside this function, like so <code>df[df[\"Organism\"] != \"Blank\"]</code> before plotting, otherwise it will appear as an extra plate.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def plateheatmaps(\n    df,\n    substance_id=\"ID\",\n    measurement=\"Raw Optical Density\",\n    barcode=\"Barcode\",\n    negative_control=\"Negative Control\",\n    blank=\"Medium\",\n) -&gt; alt.vegalite.v5.api.HConcatChart:\n    \"\"\"\n    Parameters:\n        df (pandas.DataFrame): Dataframe with relevant data\n        substance_id (str): column name in df containing the unique substance id\n        measurement (str): column name in df with the measurements to colorize via heatmaps\n        negative_control (str): controls with organism + medium\n        blank (str): controls with only medium (no organism and therefore no growth)\n\n    Plots heatmaps of the plates from df in a gridlike manner.\n    Exclude unwanted plates, for example Blanks from the df outside this function, like so\n    `df[df[\"Organism\"] != \"Blank\"]`\n    before plotting, otherwise it will appear as an extra plate.\n    \"\"\"\n    df[\"Col_384\"] = df[\"Col_384\"].astype(int)\n    # df[substance_id] = df[substance_id].astype(str)\n    plots = []\n    for _, _organism_df in df.groupby(\"Organism\"):\n        plots.append(\n            get_heatmap(\n                _organism_df,\n                substance_id,\n                measurement,\n                negative_control,\n                blank,\n            )\n            .facet(\n                row=alt.Row(f\"{barcode}:N\"),\n                column=alt.Column(\"Replicate:N\"),\n                title=alt.Title(\n                    _organism_df[\"Organism\"].unique()[0],\n                    orient=\"top\",\n                    anchor=\"middle\",\n                    dx=-20,\n                ),\n            )\n            .resolve_scale(color=\"shared\")\n            .resolve_axis(x=\"independent\", y=\"independent\")\n        )\n\n    plate_heatmaps = (\n        alt.hconcat(*plots).resolve_scale(color=\"independent\").resolve_axis(y=\"shared\")\n    )\n    return plate_heatmaps\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.potency_distribution","title":"<code>potency_distribution(dataset_grp, threshold, dataset, intervals=[0.05, 0.1, 0.78, 6.25, 50], title='Potency Distribution', ylabel='Number of Compounds', xlabel='MIC Interval', legendlabelorient='bottom')</code>","text":"<p>Input: MIC.results[\"MIC_Results_AllDatasets_longformat\"]</p> <p>Returns a potency distribution (histogram if MIC intervals) plot.</p> <p>Example: Obtain a list of potency distribution plots. One plot per dataset and threshold. <pre><code>plots_per_dataset = []\nthresholds = [50.0]\nfor threshold in thresholds:\n    for dataset, dataset_grp in mic_df.groupby(\"Dataset\"):\n        plots_per_dataset.append(potency_distribution(dataset_grp, threshold, dataset))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dataset_grp</code> <code>DataFrame</code> <p>Group DataFrame from grouping via Datasets.</p> required <code>threshold</code> <code>float</code> <p>single threshold value (usually from a list of thresholds).</p> required <code>dataset</code> <code>str</code> <p>The name of the dataset.</p> required <code>intervals</code> <code>list[float]</code> <p>the upper limits for the interval bins. Interval example: (x, y] -&gt; open below x, &lt;= y</p> <code>[0.05, 0.1, 0.78, 6.25, 50]</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Potency Distribution'</code> <code>ylabel</code> <code>str</code> <p>Y-Axis label.</p> <code>'Number of Compounds'</code> <code>xlabel</code> <code>str</code> <p>X-Axis label.</p> <code>'MIC Interval'</code> <code>legendlabelorient</code> <code>str</code> <p>Position of the legend (options: \"left\", \"right\", \"top\", \"bottom\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\", \"none\" (Default))</p> <code>'bottom'</code> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def potency_distribution(\n    dataset_grp: pd.DataFrame,\n    threshold: float,\n    dataset: str,\n    intervals: list[float] = [0.05, 0.1, 0.78, 6.25, 50],\n    title: str = \"Potency Distribution\",\n    ylabel: str = \"Number of Compounds\",\n    xlabel: str = \"MIC Interval\",\n    legendlabelorient: str = \"bottom\",  # right, bottom, top-left, etc.\n):\n    \"\"\"\n    Input: MIC.results[\"MIC_Results_AllDatasets_longformat\"]\n\n    Returns a potency distribution (histogram if MIC intervals) plot.\n\n    Example: Obtain a list of potency distribution plots. One plot per dataset and threshold.\n    ```\n    plots_per_dataset = []\n    thresholds = [50.0]\n    for threshold in thresholds:\n        for dataset, dataset_grp in mic_df.groupby(\"Dataset\"):\n            plots_per_dataset.append(potency_distribution(dataset_grp, threshold, dataset))\n    ```\n\n    Parameters:\n        dataset_grp (pd.DataFrame): Group DataFrame from grouping via Datasets.\n        threshold (float): single threshold value (usually from a list of thresholds).\n        dataset (str): The name of the dataset.\n        intervals (list[float]): the upper limits for the interval bins. Interval example: (x, y] -&gt; open below x, &lt;= y\n        title (str): Plot title.\n        ylabel (str): Y-Axis label.\n        xlabel (str): X-Axis label.\n        legendlabelorient (str): Position of the legend (options: \"left\", \"right\", \"top\", \"bottom\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\", \"none\" (Default))\n    \"\"\"\n    no_mic = (\n        dataset_grp[dataset_grp[f\"MIC{threshold} in \u00b5M\"].isna()][\"Organism\"]\n        .value_counts()\n        .reset_index(name=ylabel)\n    )\n    no_mic[xlabel] = f\"&gt;{max(intervals)}\"\n    sub_df = (\n        dataset_grp.groupby(\"Organism\")[f\"MIC{threshold} in \u00b5M\"]\n        .value_counts(bins=intervals, dropna=False)\n        .rename_axis([\"Organism\", xlabel])\n        .reset_index(name=ylabel)\n    )\n    sub_df[xlabel] = sub_df[xlabel].astype(str)\n    sub_df = pd.concat([no_mic, sub_df])\n    legendcolumns = None\n    if legendlabelorient == \"bottom\":\n        legendcolumns = 3\n    base = alt.Chart(sub_df, title=alt.Title(title, subtitle=[f\"Dataset: {dataset}\"]))\n    bar = base.mark_bar(stroke=\"white\").encode(\n        alt.X(f\"{xlabel}:N\").axis(labelAngle=0),\n        y=alt.Y(f\"{ylabel}:Q\").scale(domain=[0, max(sub_df[ylabel])+2]),\n        color=alt.Color(\"Organism:N\").legend(\n            orient=legendlabelorient,\n            labelLimit=200,\n            fillColor=\"white\",\n            columns=legendcolumns,\n        ),\n        xOffset=\"Organism:N\",\n    )\n    text = base.mark_text(dy=-5).encode(\n        alt.X(f\"{xlabel}:N\"),\n        y=f\"{ylabel}:Q\",\n        xOffset=\"Organism:N\",\n        text=f\"{ylabel}:Q\",\n    )\n    return alt.layer(bar, text)\n</code></pre>"},{"location":"reference/process/","title":"Processing Functions","text":""},{"location":"reference/process/#rda_toolbox.process.add_b_score","title":"<code>add_b_score(plate_df, measurement_header='Raw Optical Density', row_header='Row_384', col_header='Col_384')</code>","text":"<p>Expects a Dataframe comprising a whole plate (without controls!).</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def add_b_score(\n    plate_df: pd.DataFrame,\n    measurement_header: str = \"Raw Optical Density\",\n    row_header: str = \"Row_384\",\n    col_header: str = \"Col_384\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Expects a Dataframe comprising a **whole** plate (without controls!).\n    \"\"\"\n    # We could also collect iterations of the median polish function and plot the results to show progress of normalization\n    plate_df = median_polish_df(plate_df)\n    mad_value = median_absolute_deviation(plate_df[measurement_header], scale=1.4826)\n    plate_df[\"b_scores\"] = plate_df[measurement_header] / mad_value\n    return plate_df.drop(\n        columns=[\"row_effect\", \"col_effect\", \"row_median\", \"col_median\", measurement_header]\n        ).round({\"b_scores\": 2})\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.background_normalize_zfactor","title":"<code>background_normalize_zfactor(grp, substance_id, measurement, negative_controls, blanks, norm_by_barcode)</code>","text":"<p>This function is supposed to be applied to a grouped DataFrame. It does the following operations: - Background subtraction by subtracting the mean of the blanks per plate - Normalization by applying max-normalization using the 'Negative Controls' - Z-Factor calculation using negative controls and blanks</p> <p><code>negative_controls</code> are controls with organism (e.g. bacteria) and medium and are labeled in the input DataFrame as 'Negative Controls'. <code>blanks</code> are controls with only medium and are labeled in the input DataFrame as 'Medium'.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def background_normalize_zfactor(\n    grp: pd.DataFrame,\n    substance_id,\n    measurement,\n    negative_controls,\n    blanks,\n    norm_by_barcode,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    This function is supposed to be applied to a grouped DataFrame.\n    It does the following operations:\n    - Background subtraction by subtracting the mean of the blanks per plate\n    - Normalization by applying max-normalization using the 'Negative Controls'\n    - Z-Factor calculation using negative controls and blanks\n\n    *`negative_controls` are controls with organism (e.g. bacteria) and medium*\n    *and are labeled in the input DataFrame as 'Negative Controls'.*\n    *`blanks` are controls with only medium and are labeled*\n    *in the input DataFrame as 'Medium'.*\n    \"\"\"\n\n    plate_blanks_mean = grp[grp[substance_id] == blanks][f\"Raw {measurement}\"].mean()\n    # Subtract background noise:\n    grp[f\"Denoised {measurement}\"] = grp[f\"Raw {measurement}\"] - plate_blanks_mean\n    plate_denoised_negative_mean = grp[grp[substance_id] == negative_controls][\n        f\"Denoised {measurement}\"\n    ].mean()\n    plate_denoised_blank_mean = grp[grp[substance_id] == blanks][\n        f\"Denoised {measurement}\"\n    ].mean()\n    # Normalize:\n    grp[f\"Relative {measurement}\"] = grp[f\"Denoised {measurement}\"].apply(\n        lambda x: max_normalization(x, plate_denoised_negative_mean)\n    )\n    # Z-Factor:\n    plate_neg_controls = grp[grp[substance_id] == negative_controls][\n        f\"Raw {measurement}\"\n    ]\n    plate_blank_controls = grp[grp[substance_id] == blanks][f\"Raw {measurement}\"]\n\n    # Check inputs :)\n    if (len(plate_neg_controls) == 0):\n        raise KeyError(\"Please check if keyword 'negative_controls' is matching with input table.\")\n    elif (len(plate_blank_controls) == 0):\n        raise KeyError(\"Please check if keyword 'blanks' is matching with input table.\")\n\n    grp[\"Z-Factor\"] = zfactor(plate_neg_controls, plate_blank_controls)\n\n    # Robust Z-Factor using median instead of mean:\n    grp[\"Robust Z-Factor\"] = zfactor_median(plate_neg_controls, plate_blank_controls)\n\n    return grp\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.get_thresholded_subset","title":"<code>get_thresholded_subset(df, id_column='ID', negative_controls='Negative Control', blanks='Medium', blankplate_organism='Blank', threshold=None)</code>","text":"<p>Expects a DataFrame with a mic_cutoff column</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def get_thresholded_subset(\n    df: pd.DataFrame,\n    id_column=\"ID\",\n    negative_controls: str = \"Negative Control\",\n    blanks: str = \"Medium\",\n    blankplate_organism: str = \"Blank\",\n    threshold=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Expects a DataFrame with a mic_cutoff column\n    \"\"\"\n    # TODO: hardcode less columns\n\n    # Use only substance entries, no controls, no blanks etc.:\n    substance_df = df.loc[\n        (df[id_column] != blanks)\n        &amp; (df[id_column] != negative_controls)\n        &amp; (df[\"Organism\"] != blankplate_organism),\n        :,\n    ].copy()\n    # Apply threshold:\n    if threshold:\n        substance_df[\"Cutoff\"] = threshold\n    else:\n        if \"mic_cutoff\" not in substance_df:\n            raise KeyError(\"No 'mic_cutoff' column in Input.xlsx\")\n    selection = substance_df[\n        substance_df[\"Relative Optical Density\"] &lt; substance_df[\"Cutoff\"]\n    ]\n    # Apply mean and std in case of replicates:\n    result = selection.groupby([id_column, \"Organism\", \"Dataset\"], as_index=False).agg(\n        {\n            \"Relative Optical Density\": [\"mean\", \"std\"],\n            id_column: [\"first\", \"count\"],\n            \"Organism\": \"first\",\n            \"Cutoff\": \"first\",\n            \"Dataset\": \"first\",\n        }\n    )\n    result.columns = [\n        \"Relative Optical Density mean\",\n        \"Relative Optical Density std\",\n        id_column,\n        \"Replicates\",\n        \"Organism\",\n        \"Cutoff\",\n        \"Dataset\",\n    ]\n    return result\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.mic_results","title":"<code>mic_results(df, filepath, thresholds=[20, 50])</code>","text":"<p>Expects the results from rda.preprocess() function. Means measurements between replicates and obtains the MIC values per substance and organism. Saves excel files per dataset and sheets per organism with Minimum Inhibitory Concentrations (MICs) at the given thresholds.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def mic_results(df, filepath, thresholds=[20, 50]):\n    \"\"\"\n    Expects the results from rda.preprocess() function.\n    Means measurements between replicates and obtains the MIC values per substance and organism.\n    Saves excel files per dataset and sheets per organism with Minimum Inhibitory Concentrations (MICs)\n    at the given thresholds.\n    \"\"\"\n\n    df = df[(df[\"Dataset\"] != \"Negative Control\") &amp; (df[\"Dataset\"] != \"Blank\")].dropna(\n        subset=[\"Concentration\"]\n    )\n    # the above should remove entries where Concentration == NAN\n\n    # Pivot table to get the aggregated values:\n    pivot_df = pd.pivot_table(\n        df,\n        values=[\"Relative Optical Density\", \"Replicate\", \"Z-Factor\"],\n        index=[\n            \"Internal ID\",\n            \"External ID\",\n            \"Organism\",\n            \"Concentration\",\n            \"Dataset\",\n        ],\n        aggfunc={\n            \"Relative Optical Density\": [\"mean\"],\n            \"Replicate\": [\"count\"],\n            \"Z-Factor\": [\"mean\", \"std\"],  # does this make sense? with std its usable.\n            # \"Z-Factor\": [\"std\"],\n        },\n    ).reset_index()\n\n    # merge pandas hirarchical column index (wtf is this pandas!?)\n    pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n\n    mic_records = []\n    for group_names, grp in pivot_df.groupby(\n        [\"Internal ID\", \"External ID\", \"Organism\", \"Dataset\"]\n    ):\n        internal_id, external_id, organism, dataset = group_names\n        # Sort by concentration just to be sure:\n        grp = grp[\n            [\n                \"Concentration\",\n                \"Relative Optical Density mean\",\n                \"Z-Factor mean\",\n                \"Z-Factor std\",\n            ]\n        ].sort_values(by=[\"Concentration\"])\n        # print(grp)\n        # Get rows where the OD is below the given threshold:\n        record = {\n            \"Internal ID\": internal_id,\n            \"External ID\": external_id,\n            \"Organism\": organism,\n            \"Dataset\": dataset,\n            \"Z-Factor mean\": list(grp[\"Z-Factor mean\"])[0],\n            \"Z-Factor std\": list(grp[\"Z-Factor std\"])[0],\n        }\n\n        for threshold in thresholds:\n            values_below_threshold = grp[\n                grp[\"Relative Optical Density mean\"] &lt; threshold\n            ]\n            # thx to jonathan - check if the OD at maximum concentration is below threshold (instead of any concentration)\n            max_conc_below_threshold = list(\n                grp[grp[\"Concentration\"] == max(grp[\"Concentration\"])][\n                    \"Relative Optical Density mean\"\n                ]\n                &lt; threshold\n            )[0]\n            if not max_conc_below_threshold:\n                mic = None\n            else:\n                mic = values_below_threshold.iloc[0][\"Concentration\"]\n            record[f\"MIC{threshold} in \u00b5M\"] = mic\n        mic_records.append(record)\n    # Drop entries where no MIC could be determined\n    mic_df = pd.DataFrame.from_records(mic_records)\n    # mic_df.dropna(\n    #     subset=[f\"MIC{threshold} in \u00b5M\" for threshold in thresholds],\n    #     how=\"all\",\n    #     inplace=True,\n    # )\n    mic_df.round(2).to_excel(\n        os.path.join(filepath, \"MIC_Results_AllDatasets_longformat.xlsx\"), index=False\n    )\n    for dataset, dataset_grp in mic_df.groupby([\"Dataset\"]):\n        pivot_multiindex_df = pd.pivot_table(\n            dataset_grp,\n            values=[f\"MIC{threshold} in \u00b5M\" for threshold in thresholds]\n            + [\"Z-Factor mean\", \"Z-Factor std\"],\n            index=[\"Internal ID\", \"External ID\", \"Dataset\"],\n            columns=\"Organism\",\n        ).reset_index()\n\n        resultpath = os.path.join(filepath, dataset[0])\n\n        # References special case\n        if dataset[0] == \"Reference\":\n            references_mic_results(df, resultpath, thresholds=thresholds)\n            continue  # skip for references\n\n        pathlib.Path(resultpath).mkdir(parents=True, exist_ok=True)\n        for threshold in thresholds:\n            organisms_thresholded_mics = pivot_multiindex_df[\n                [\"Internal ID\", \"External ID\", f\"MIC{threshold} in \u00b5M\"]\n            ]\n            cols = list(organisms_thresholded_mics.columns.droplevel())\n            cols[0] = \"Internal ID\"\n            cols[1] = \"External ID\"\n            organisms_thresholded_mics.columns = cols\n            organisms_thresholded_mics = organisms_thresholded_mics.sort_values(\n                by=list(organisms_thresholded_mics.columns)[2:],\n                na_position=\"last\",\n            )\n            # organisms_thresholded_mics.dropna(\n            #     subset=list(organisms_thresholded_mics.columns)[2:],\n            #     how=\"all\",\n            #     inplace=True,\n            # )\n            organisms_thresholded_mics.fillna(\"NA\", inplace=True)\n            organisms_thresholded_mics.to_excel(\n                os.path.join(resultpath, f\"{dataset[0]}_MIC{threshold}_results.xlsx\"),\n                index=False,\n            )\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.preprocess","title":"<code>preprocess(df, substance_id='ID', measurement='Optical Density', negative_controls='Negative Control', blanks='Blank', norm_by_barcode='Barcode')</code>","text":"<ul> <li>raw_df: raw reader data obtained with <code>rda.readerfiles_rawdf()</code></li> <li>input_df: input specifications table with required columns:<ul> <li>Dataset (with specified references as their own dataset 'Reference')</li> <li>ID (substance_id) (with specified blanks and negative_controls)</li> <li>Assay Transfer Barcode</li> <li>Row_384 (or Row_96)</li> <li>Col_384 (or Col_96)</li> <li>Concentration</li> <li>Replicate (specifying replicate number)</li> <li>Organism (scientific organism name i.e. with strain)</li> </ul> </li> </ul> <p>Processing function which merges raw reader data (raw_df) with input specifications table (input_df) and then normalizes, calculates Z-Factor per plate (norm_by_barcode) and rounds to sensible decimal places.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def preprocess(\n    df: pd.DataFrame,  # mapped inputs\n    substance_id: str = \"ID\",\n    measurement: str = \"Optical Density\",\n    negative_controls: str = \"Negative Control\",\n    blanks: str = \"Blank\",\n    norm_by_barcode=\"Barcode\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    - raw_df: raw reader data obtained with `rda.readerfiles_rawdf()`\n    - input_df: input specifications table with required columns:\n        - Dataset (with specified references as their own dataset 'Reference')\n        - ID (substance_id) (with specified blanks and negative_controls)\n        - Assay Transfer Barcode\n        - Row_384 (or Row_96)\n        - Col_384 (or Col_96)\n        - Concentration\n        - Replicate (specifying replicate number)\n        - Organism (scientific organism name i.e. with strain)\n    ---\n    Processing function which merges raw reader data (raw_df)\n    with input specifications table (input_df) and then\n    normalizes, calculates Z-Factor per plate (norm_by_barcode)\n    and rounds to sensible decimal places.\n    \"\"\"\n    # merging reader data and input specifications table\n    # df = pd.merge(raw_df, input_df, how=\"outer\")\n    # df = df.groupby(norm_by_barcode)[df.columns].apply(\n    #     lambda plate_grp: add_b_score(\n    #         plate_grp[plate_grp[\"\"]],\n    #         # measurement_header=\"Raw Optical Density\"\n    #     )\n    # )\n    df[substance_id] = df[substance_id].astype(str)\n    df = (\n        df.groupby(norm_by_barcode)[df.columns]\n        .apply(\n            lambda grp: background_normalize_zfactor(\n                grp,\n                substance_id,\n                measurement,\n                negative_controls,\n                blanks,\n                norm_by_barcode,\n            )\n        )\n        .reset_index(drop=True)\n    )\n\n    # df[substance_id] = df[substance_id].astype(str)\n\n    # detect and report NA values (defined in input, not in raw data)\n    orgs_w_missing_data = df[df[f\"Raw {measurement}\"].isna()][\"Organism formatted\"].unique()\n    if orgs_w_missing_data.size &gt; 0:\n        print(\n            f\"\"\"Processed data:\n      Organisms with missing data, excluded from processed data: {orgs_w_missing_data}.\n      If this is not intended, please check the Input.xlsx or if raw data files are complete.\n              \"\"\"\n        )\n        df = df.dropna(subset=[f\"Raw {measurement}\"])\n    # Report missing\n    # Remove missing from \"processed\" dataframe\n    return df.round(\n        {\n            \"Denoised Optical Density\": 2,\n            \"Relative Optical Density\": 2,\n            \"Z-Factor\": 2,\n            \"Robust Z-Factor\": 2,\n            # \"Concentration\": 2,\n        }\n    )\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.primary_results","title":"<code>primary_results(df, substance_id, filepath='../data/results/', thresholds=[50])</code>","text":"<p>Expects the results from rda.preprocess() function.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def primary_results(\n    df: pd.DataFrame,\n    substance_id,\n    filepath=\"../data/results/\",\n    thresholds: list[float] = [50],\n):\n    \"\"\"\n    Expects the results from rda.preprocess() function.\n    \"\"\"\n    df = df[\n        (df[\"Dataset\"] != \"Reference\")\n        &amp; (df[\"Dataset\"] != \"Positive Control\")\n        &amp; (df[\"Dataset\"] != \"Blank\")\n    ].dropna(subset=[\"Concentration\"])\n\n    pivot_df = pd.pivot_table(\n        df,\n        values=[\"Relative Optical Density\", \"Replicate\", \"Z-Factor\"],\n        index=[\n            substance_id,\n            \"Organism\",\n            \"Concentration\",\n            \"Dataset\",\n        ],\n        aggfunc={\n            \"Relative Optical Density\": [\"mean\"],\n            \"Replicate\": [\"count\"],\n        },\n    ).reset_index()\n    pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n\n    for threshold in thresholds:\n        pivot_df[f\"Relative Growth &lt; {threshold}\"] = pivot_df.groupby(\n            [substance_id, \"Organism\", \"Dataset\"]\n        )[\"Relative Optical Density mean\"].transform(lambda x: x &lt; threshold)\n\n        for dataset, dataset_grp in pivot_df.groupby([\"Dataset\"]):\n            dataset = dataset[0]\n            resultpath = os.path.join(filepath, dataset)\n            pathlib.Path(resultpath).mkdir(parents=True, exist_ok=True)\n\n            print(\n                \"Saving\",\n                os.path.join(resultpath, f\"{dataset}_all_results.xlsx\"),\n            )\n            dataset_grp.to_excel(\n                os.path.join(resultpath, f\"{dataset}_all_results.xlsx\"),\n                index=False,\n            )\n            print(\n                \"Saving\",\n                os.path.join(resultpath, f\"{dataset}_all_results.csv\"),\n            )\n            dataset_grp.to_csv(\n                os.path.join(resultpath, f\"{dataset}_all_results.csv\"),\n                index=False,\n            )\n\n            pivot_multiindex_df = pd.pivot_table(\n                dataset_grp,\n                values=[f\"Relative Optical Density mean\"],\n                index=[substance_id, \"Dataset\", \"Concentration\"],\n                columns=\"Organism\",\n            ).reset_index()\n            cols = list(pivot_multiindex_df.columns.droplevel())\n            cols[:3] = list(map(lambda x: x[0], pivot_multiindex_df.columns[:3]))\n            pivot_multiindex_df.columns = cols\n\n            # Apply threshold (active in any organism)\n            thresholded_pivot = pivot_multiindex_df.iloc[\n                list(\n                    pivot_multiindex_df.iloc[:, 3:].apply(\n                        lambda x: any(list(map(lambda i: i &lt; threshold, x))), axis=1\n                    )\n                )\n            ]\n\n            # Sort by columns each organism after the other\n            # return pivot_multiindex_df.sort_values(by=cols[3:])\n\n            # Sort rows by mean between the organisms (lowest mean activity first)\n            results_sorted_by_mean_activity = thresholded_pivot.iloc[\n                thresholded_pivot.iloc[:, 3:].mean(axis=1).argsort()\n            ]\n            print(\n                \"Saving\",\n                os.path.join(\n                    resultpath, f\"{dataset}_threshold{threshold}_results.xlsx\"\n                ),\n            )\n            results_sorted_by_mean_activity.to_excel(\n                os.path.join(\n                    resultpath, f\"{dataset}_threshold{threshold}_results.xlsx\"\n                ),\n                index=False,\n            )\n            print(\n                \"Saving\",\n                os.path.join(resultpath, f\"{dataset}_threshold{threshold}_results.csv\"),\n            )\n            results_sorted_by_mean_activity.to_csv(\n                os.path.join(resultpath, f\"{dataset}_threshold{threshold}_results.csv\"),\n                index=False,\n            )\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.references_mic_results","title":"<code>references_mic_results(preprocessed_data, resultpath, thresholds=[20, 50])</code>","text":"<p>This function saves an excel file for the reference substances. Since reference substances have duplicate Internal IDs (since they are used multiple times), they would be meaned between duplicates. To circumvent this, this function exists which gets the MIC for each reference per (AcD) plate instead of per Internal ID.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def references_mic_results(\n    preprocessed_data,\n    resultpath,\n    thresholds=[20, 50],\n):\n    \"\"\"\n    This function saves an excel file for the reference substances.\n    Since reference substances have duplicate Internal IDs\n    (since they are used multiple times), they would be meaned between duplicates.\n    To circumvent this, this function exists which gets the MIC\n    for each reference **per (AcD) plate** instead of per Internal ID.\n    \"\"\"\n    only_references = preprocessed_data[preprocessed_data[\"Dataset\"] == \"Reference\"]\n    mic_records = []\n    for group_names, grp in only_references.groupby(\n        [\n            \"Internal ID\",\n            \"External ID\",\n            \"Organism\",\n            \"Dataset\",\n            \"AcD Barcode 384\",\n        ]\n    ):\n        internal_id, external_id, organism, dataset, acd_barcode = group_names\n        grp = grp.copy().sort_values(by=[\"Concentration\"])\n        record = {\n            \"Internal ID\": internal_id,\n            \"External ID\": external_id,\n            \"Organism\": organism,\n            \"Dataset\": dataset,\n            \"AcD Barcode 384\": acd_barcode,\n            \"Z-Factor\": list(grp[\"Z-Factor\"])[0],\n        }\n        for threshold in thresholds:\n            values_below_threshold = grp[grp[\"Relative Optical Density\"] &lt; threshold]\n            # thx to jonathan - check if the OD at maximum concentration is below threshold (instead of any concentration)\n            max_conc_below_threshold = list(\n                grp[grp[\"Concentration\"] == max(grp[\"Concentration\"])][\n                    \"Relative Optical Density\"\n                ]\n                &lt; threshold\n            )[0]\n            if not max_conc_below_threshold:\n                mic = None\n            else:\n                mic = values_below_threshold.iloc[0][\"Concentration\"]\n            record[f\"MIC{threshold} in \u00b5M\"] = mic\n        mic_records.append(record)\n    mic_df = pd.DataFrame.from_records(mic_records)\n    mic_df.sort_values(by=[\"External ID\", \"Organism\"]).to_excel(\n        os.path.join(resultpath, \"References_MIC_results_eachRefID.xlsx\"), index=False\n    )\n</code></pre>"},{"location":"reference/utility/","title":"Utility Functions","text":""},{"location":"reference/utility/#rda_toolbox.utility.chunks","title":"<code>chunks(l, n)</code>","text":"<p>Useful function if you want to put a certain amount of observations into one plot. Yield n number of striped chunks from l.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def chunks(l, n):\n    \"\"\"\n    Useful function if you want to put a certain amount\n    of observations into one plot.\n    Yield n number of striped chunks from l.\n    \"\"\"\n    for i in range(0, n):\n        yield l[i::n]\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.format_organism_name","title":"<code>format_organism_name(raw_organism_name)</code>","text":"<p>Create internal, formatted orgnames but keep external for plots...</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def format_organism_name(raw_organism_name: str) -&gt; str:\n    \"\"\"\n    Create internal, formatted orgnames but keep external for plots...\n\n    \"\"\"\n    # I dont know how many spaces people introduce...\n    normalized_spaces = re.sub(r'\\s+', ' ', raw_organism_name).strip()\n    # I dont know which organism names people think of... (MRSA ST033793 vs. Acinetobacter Baumannii ATCC 17978)\n    # Or which other evil things people come up with\n    return normalized_spaces.lower()\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.generate_inputtable","title":"<code>generate_inputtable(readout_df=None, platetype=384)</code>","text":"<p>Generates an input table for the corresponding readout dataframe. If not readout df is provided, create a minimal input df.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def generate_inputtable(readout_df=None, platetype: int = 384):\n    \"\"\"\n    Generates an input table for the corresponding readout dataframe.\n    If not readout df is provided, create a minimal input df.\n    \"\"\"\n    if readout_df is None:\n        barcodes = [\"001PrS01001\"]\n    else:\n        barcodes = readout_df[\"Barcode\"].unique()\n\n    substance_df = pd.DataFrame(\n        {\n            \"ID\": [f\"Substance {i}\" for i in range(1, platetype + 1)],\n            f\"Row_{platetype}\": [*list(string.ascii_uppercase[:16]) * 24],\n            f\"Col_{platetype}\": sum([[i] * 16 for i in range(1, 24 + 1)], []),\n            \"Concentration in mg/mL\": 1,\n        }\n    )\n    layout_df = pd.DataFrame(\n        {\n            \"Barcode\": barcodes,\n            \"Replicate\": [1] * len(barcodes),\n            \"Organism\": [\n                f\"Placeholder Organism {letter}\"\n                for letter in string.ascii_uppercase[: len(barcodes)]\n            ],\n        }\n    )\n    df = pd.merge(layout_df, substance_df, how=\"cross\")\n    return df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.get_rows_cols","title":"<code>get_rows_cols(platetype)</code>","text":"<p>Obtain number of rows and columns as tuple for corresponding plate type.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def get_rows_cols(platetype: int) -&gt; tuple[int, int]:\n    \"\"\"\n    Obtain number of rows and columns as tuple for corresponding plate type.\n    \"\"\"\n    match platetype:\n        case 96:\n            return 8, 12\n        case 384:\n            return 16, 24\n        case _:\n            raise ValueError(\"Not a valid plate type\")\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.get_selection","title":"<code>get_selection(df, threshold_value, x_column='Relative Optical Density')</code>","text":"<p>Apply this ahead of get_upsetplot_df (to obtain dummies df). After all the above, apply UpSetAltair.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def get_selection(df, threshold_value, x_column=\"Relative Optical Density\"):\n    \"\"\"\n    Apply this ahead of get_upsetplot_df (to obtain dummies df).\n    After all the above, apply UpSetAltair.\n    \"\"\"\n    selection_results = df[df[x_column] &lt; threshold_value].copy()\n    selection_results[\"threshold\"] = f\"&lt;{threshold_value}\"\n\n    return selection_results\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.get_upsetplot_df","title":"<code>get_upsetplot_df(df, set_column='Organism', counts_column='ID')</code>","text":"<p>Function to obtain a correctly formatted DataFrame. According to UpSetR-shiny this table is supposed to be encoded in binary and set up so that each column represents a set, and each row represents an element. If an element is in the set it is represented as a 1 in that position. If an element is not in the set it is represented as a 0.</p> <p>Thanks to: https://stackoverflow.com/questions/37381862/get-dummies-for-pandas-column-containing-list</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def get_upsetplot_df(df, set_column=\"Organism\", counts_column=\"ID\"):\n    \"\"\"\n    Function to obtain a correctly formatted DataFrame.\n    According to [UpSetR-shiny](https://github.com/hms-dbmi/UpSetR-shiny)\n    this table is supposed to be encoded in binary and set up so that each column represents a set, and each row represents an element.\n    If an element is in the set it is represented as a 1 in that position. If an element is not in the set it is represented as a 0.\n\n    *Thanks to: https://stackoverflow.com/questions/37381862/get-dummies-for-pandas-column-containing-list*\n    \"\"\"\n    tmp_df = (\n        df.groupby(counts_column)[set_column].apply(lambda x: x.unique()).reset_index()\n    )\n    dummies_df = (\n        pd.get_dummies(\n            tmp_df.join(\n                pd.Series(\n                    tmp_df[set_column]\n                    .apply(pd.Series)\n                    .stack()\n                    .reset_index(1, drop=True),\n                    name=set_column + \"1\",\n                )\n            )\n            .drop(set_column, axis=1)\n            .rename(columns={set_column + \"1\": set_column}),\n            columns=[set_column],\n        )\n        .groupby(counts_column, as_index=False)\n        .sum()\n    )\n    # remove \"{set_column}_\" from set column labels\n    dummies_df.columns = list(\n        map(\n            lambda x: \"\".join(x.split(\"_\")[1:]) if x.startswith(set_column) else x,\n            dummies_df.columns,\n        )\n    )\n    # remove any dots as they interfere with altairs plotting.\n    dummies_df.columns = dummies_df.columns.str.replace(\".\", \"\")\n    return dummies_df.drop(columns=[\"count\"], errors=\"ignore\")\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.imgbuffer_to_imgstr","title":"<code>imgbuffer_to_imgstr(imgbuffer, prefix='data:image/png;base64,', suffix='')</code>","text":"<p>Encode imagebuffer to string (default base64-encoded string). Example: imgbuffer_to_imgstr(mol_to_bytes(mol)), prefix=\"'\")</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def imgbuffer_to_imgstr(imgbuffer, prefix=\"data:image/png;base64,\", suffix=\"\"):\n    \"\"\"\n    Encode imagebuffer to string (default base64-encoded string).\n    Example: imgbuffer_to_imgstr(mol_to_bytes(mol)), prefix=\"&lt;img src='data:image/png;base64,\", suffix=\"'/&gt;'\")\n    \"\"\"\n    str_equivalent_image = base64.b64encode(imgbuffer.getvalue()).decode()\n    img_tag = prefix + str_equivalent_image + suffix\n    return img_tag\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.inchi_to_imgstr","title":"<code>inchi_to_imgstr(inchi)</code>","text":"<p>Converts a inchi string to a base64 encoded image string (e.g. for plotting in altair). It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(), use these if you want more fine grained control over the format of the returned string. Example: df[\"image\"] = df[\"inchi\"].apply(lambda inchi: inchi_to_imgstr(inchi))</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def inchi_to_imgstr(inchi):\n    \"\"\"\n    Converts a inchi string to a base64 encoded image string (e.g. for plotting in altair).\n    It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(),\n    use these if you want more fine grained control over the format of the returned string.\n    Example: df[\"image\"] = df[\"inchi\"].apply(lambda inchi: inchi_to_imgstr(inchi))\n    \"\"\"\n    return imgbuffer_to_imgstr(mol_to_bytes(Chem.MolFromInchi(inchi)))\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.map_96_to_384","title":"<code>map_96_to_384(df_row, rowname, colname, q_name)</code>","text":"<p>Maps the rows and columns of 4 96-Well plates into a single 384-Well plate.</p> <ul> <li>Maps in Z order.</li> <li>Takes row, column and quadrant (each of the 96-well plates is one quadrant) of a well from 4 96-well plates and maps it to the corresponding well in a 384-well plate</li> </ul> <p>Returns the 384-Well plate row and column. Example: <code>df[\"Row_384\"], df[\"Col_384\"] = zip(*df.apply(map_96_to_384, axis=1))</code></p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def map_96_to_384(\n    df_row: pd.Series,\n    rowname: str,\n    colname: str,\n    q_name: str,\n) -&gt; tuple[pd.Series, pd.Series]:\n    \"\"\"\n    Maps the rows and columns of 4 96-Well plates into a single 384-Well plate.\n\n    - Maps in Z order.\n    - Takes row, column and quadrant (each of the 96-well plates is one quadrant) of a well from 4 96-well plates and maps it to the corresponding well in a 384-well plate\n\n    Returns the 384-Well plate row and column.\n    Example: `df[\"Row_384\"], df[\"Col_384\"] = zip(*df.apply(map_96_to_384, axis=1))`\n    \"\"\"\n    # TODO: Write tests for this mapping function\n\n    row = df_row[rowname]  # 96-well plate row\n    col = df_row[colname]  # 96-well plate column\n    quadrant = df_row[q_name]  # which of the 4 96-well plate\n\n    rowmapping = dict(\n        zip(\n            string.ascii_uppercase[0:8],\n            np.array_split(list(string.ascii_uppercase)[0:16], 8),\n        )\n    )\n    colmapping = dict(zip(list(range(1, 13)), np.array_split(list(range(1, 25)), 12)))\n    row_384 = rowmapping[row][0 if quadrant in [1, 2] else 1]\n    col_384 = colmapping[col][0 if quadrant in [1, 3] else 1]\n    return row_384, col_384\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.mapapply_96_to_384","title":"<code>mapapply_96_to_384(df, rowname='Row_96', colname='Column_96', q_name='Quadrant')</code>","text":"<p>Apply to a DataFrame the mapping of 96-well positions to 384-well positions.</p> <ul> <li>Maps in Z order. The DataFrame has to have columns with:</li> <li>96-well plate row positions</li> <li>96-well plate column positions</li> <li>96-well plate to 384-well plate quadrants (4 96-well plates fit into 1 384-well plate)</li> </ul> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def mapapply_96_to_384(\n    df: pd.DataFrame,\n    rowname: str = \"Row_96\",\n    colname: str = \"Column_96\",\n    q_name: str = \"Quadrant\",\n) -&gt; pd.DataFrame:\n    \"\"\"Apply to a DataFrame the mapping of 96-well positions to 384-well positions.\n\n    - Maps in Z order.\n    The DataFrame has to have columns with:\n    - 96-well plate row positions\n    - 96-well plate column positions\n    - 96-well plate to 384-well plate quadrants\n    *(4 96-well plates fit into 1 384-well plate)*\n    \"\"\"\n    df[\"Row_384\"], df[\"Col_384\"] = zip(\n        *df.apply(\n            lambda row: map_96_to_384(\n                row, rowname=rowname, colname=colname, q_name=q_name\n            ),\n            axis=1,\n        )\n    )\n    return df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.mic_assaytransfer_mapping","title":"<code>mic_assaytransfer_mapping(position, orig_barcode, ast_platemapping)</code>","text":"<p>This is a rather unfinished function to map 96 well motherplates to 384 well assay transfer (AsT) plates.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def mic_assaytransfer_mapping(position, orig_barcode, ast_platemapping):\n    \"\"\"\n    This is a rather unfinished function to map 96 well motherplates to 384 well assay transfer (AsT) plates.\n\n    \"\"\"\n    row = position[0]\n    col = int(position[1:])\n    orig_barcode = str(orig_barcode)\n    rowmapping = dict(\n        zip(\n            string.ascii_uppercase[0:8],\n            np.array_split(list(string.ascii_uppercase)[0:16], 8),\n        )\n    )\n    colmapping = dict(zip(list(range(1, 13)), [1, 2] * 13))\n    mapping = {\n        1: 0,\n        2: 0,\n        3: 1,\n        4: 1,\n        5: 0,\n        6: 0,\n        7: 1,\n        8: 1,\n        9: 0,\n        10: 0,\n        11: 1,\n        12: 1,\n    }\n\n    row_384 = rowmapping[row][mapping[col]]\n    col_384 = colmapping[col]\n\n    # TODO: sometimes only the middle (5-8) or the last part of a motherplate is taken...\n    # Currently this would lead to an index error since A5 -&gt; ast_of_3 = 1 but if this is the only AsT plate, the first AsT plate is \"missing\"...\n    # Possible solution would be to try - except decreasing the position by 4 iteratively...\n    # try A5 except IndexError: A5 - 4 -&gt; try A1 success etc.\n    # or A12 -&gt; ast_of_3 = 2 -&gt; IndexError. A12 - 4 -&gt; A8 -&gt; IndexError. A8 - 4 -&gt; A4 works...\n    # seems like a bad solution though.\n    # num_of_ast_plates = len(ast_platemapping[orig_barcode][0])\n    # if num_of_ast_plates &lt; 3:\n    #     print(\"Did someone just take one third of a motherplate (Only one or two AsT plates for one MP)?\")\n\n    if col in [1, 2, 3, 4]:\n        ast_of_3 = 0\n    elif col in [5, 6, 7, 8]:\n        ast_of_3 = 1\n    else:\n        ast_of_3 = 2\n    # print(\"orig_barcode\", orig_barcode, \"ast_of_3\", ast_of_3)\n    # print(\"ast_platemapping\", ast_platemapping)\n    # print(\"col\", col)\n    # print(ast_platemapping[orig_barcode][0])\n    barcode_384_ast = ast_platemapping[orig_barcode][0][ast_of_3]\n    return str(row_384), str(col_384), barcode_384_ast\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.position_to_rowcol","title":"<code>position_to_rowcol(pos)</code>","text":"<p>Splits a position like \"A1\" into row and col e.g. (\"A\", \"1\").</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def position_to_rowcol(pos: str) -&gt; tuple[str, int]:\n    \"\"\"\n    Splits a position like \"A1\" into row and col e.g. (\"A\", \"1\").\n    \"\"\"\n    return pos[0], int(pos[1:])\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.prepare_visualization","title":"<code>prepare_visualization(df, by_id='Internal ID', whisker_width=1, exclude_negative_zfactors=True, threshold=50.0)</code>","text":"<p>Does formatting for the facet lineplots.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def prepare_visualization(\n    df,\n    by_id=\"Internal ID\",\n    whisker_width=1,\n    exclude_negative_zfactors=True,\n    threshold=50.0,\n):\n    \"\"\"\n    Does formatting for the facet lineplots.\n    \"\"\"\n    df = df.copy()\n    if exclude_negative_zfactors:\n        df = df[df[\"Z-Factor\"] &gt; 0]\n    df.loc[:, \"Used Replicates\"] = df.groupby([by_id, \"Concentration\", \"Organism\"])[\n        [\"Replicate\"]\n    ].transform(\"count\")\n    df.loc[:, \"Mean Relative Optical Density\"] = (\n        df.groupby([by_id, \"Concentration\", \"Organism\"])[[\"Relative Optical Density\"]]\n        .transform(\"mean\")\n        .round(2)\n    )\n    df.loc[:, \"Std. Relative Optical Density\"] = (\n        df.groupby([by_id, \"Concentration\", \"Organism\"])[[\"Relative Optical Density\"]]\n        .transform(\"std\")\n        .round(2)\n    )\n    df.loc[:, \"uerror\"] = (\n        df[\"Mean Relative Optical Density\"] + df[\"Std. Relative Optical Density\"]\n    )\n    df.loc[:, \"lerror\"] = (\n        df[\"Mean Relative Optical Density\"] - df[\"Std. Relative Optical Density\"]\n    )\n    tmp_list = []\n    for _, grp in df.groupby([by_id, \"Organism\"]):\n        # use replicate == 1 as the meaned OD is the same in all 3 replicates anyways\n        # print(grp)\n        maxconc_below_threshold = (\n            grp[\n                (grp[\"Replicate\"] == 1)\n                &amp; (grp[\"Concentration\"] == grp[\"Concentration\"].max())\n            ][\"Mean Relative Optical Density\"]\n            &lt; threshold\n        )\n        grp[\"max_conc_below_threshold\"] = list(maxconc_below_threshold)[0]\n        tmp_list.append(grp)\n        # .sort_values(by=[\"Concentration\"], ascending=False)\n        # grp_sorted[grp_sorted[\"Concentration\"] == 50][\"Mean Relative Optical Density\"]\n        # print(grp.aggregate())\n    df = pd.concat(tmp_list)\n    # df[\"highest_conc_bigger_50\"] = df.groupby([by_id, \"Organism\"])[\n    #     [\"Mean Relative Optical Density\"]\n    # ].transform(\n    #     lambda meas_per_conc: list(meas_per_conc)[0] &gt; 50\n    # )\n    # print(df)\n\n    df[\"at_all_conc_bigger_50\"] = df.groupby([by_id, \"Organism\"])[\n        [\"Mean Relative Optical Density\"]\n    ].transform(lambda meas_per_conc: all([x &gt; 50 for x in list(meas_per_conc)]))\n    # Bin observations into artificial categories for plotting later:\n    plot_groups = pd.DataFrame()\n    for _, grp in df.groupby([\"AsT Barcode 384\"]):\n        # divide the observations per plate into chunks\n        # number of chunks is defined by using a maximum of 10 colors/observations per plot\n        num_chunks = math.ceil(len(grp[by_id].unique()) / 10)\n        for nr, chunk in enumerate(list(chunks(grp[by_id].unique(), num_chunks))):\n            plot_groups = pd.concat(\n                [\n                    plot_groups,\n                    pd.DataFrame(\n                        {\n                            by_id: chunk,\n                            \"AsT Plate Subgroup\": sum([[nr] * len(chunk)], []),\n                        }\n                    ),\n                ]\n            ).reset_index(drop=True)\n    df = pd.merge(df, plot_groups)\n    return df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.read_sdf_withproperties","title":"<code>read_sdf_withproperties(sdf_filepath)</code>","text":"<p>Reads a SDF file and returns a DataFrame containing the molecules as rdkit molobjects as well as all the encoded properties in the SDF block.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def read_sdf_withproperties(sdf_filepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads a SDF file and returns a DataFrame containing the molecules as rdkit molobjects\n    as well as all the encoded properties in the SDF block.\n    \"\"\"\n    suppl = Chem.SDMolSupplier(sdf_filepath)\n    mols = []\n    nonmol_counter = 0\n    for mol in suppl:\n        if not mol:\n            nonmol_counter += 1\n            continue\n        mol_props = {\"mol\": mol}\n        propnames = mol.GetPropNames()\n        for prop in propnames:\n            mol_props[prop] = mol.GetProp(prop)\n        mols.append(mol_props)\n    if nonmol_counter &gt; 0:\n        print(f\"Ignored molecules: {nonmol_counter}\")\n    return pd.DataFrame(mols)\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.save_plot_per_dataset","title":"<code>save_plot_per_dataset(data, plotfunc, location, plotname=None, saveformats=['svg', 'html'])</code>","text":"<p>This is a convenience function which splits a dataframe into each dataset given in the 'Dataset' column. Then it applies the given plotting function 'plotfunc' to these splits, automatically creates folders if non existent and saves the plots to the corresponding dataset folders. Examples:     save_plot_per_dataset(preprocessed_data, rda.lineplots_facet, \"../figures/\")</p> <pre><code># if an anonymous function (lambda) is used, a plotname has to be provided:\nsave_plot_per_dataset(mic_results_long, lambda x: rda.mic_hitstogram(x, \"MIC50 in \u00b5M\"), \"../figures/\", plotname=\"MIC_Hits_Distribution\")\n</code></pre> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def save_plot_per_dataset(\n    data: pd.DataFrame,\n    plotfunc,\n    location: str,\n    plotname: str | None = None,\n    saveformats: list[str] = [\"svg\", \"html\"],\n) -&gt; None:\n    \"\"\"\n    This is a convenience function which splits a dataframe into each dataset given in the 'Dataset' column.\n    Then it applies the given plotting function 'plotfunc' to these splits,\n    automatically creates folders if non existent and saves the plots to the corresponding dataset folders.\n    Examples:\n        save_plot_per_dataset(preprocessed_data, rda.lineplots_facet, \"../figures/\")\n\n        # if an anonymous function (lambda) is used, a plotname has to be provided:\n        save_plot_per_dataset(mic_results_long, lambda x: rda.mic_hitstogram(x, \"MIC50 in \u00b5M\"), \"../figures/\", plotname=\"MIC_Hits_Distribution\")\n    \"\"\"\n    if plotname is None:\n        plotname = plotfunc.__name__\n        if plotname == \"&lt;lambda&gt;\":\n            raise TypeError(\"Please provide a plotname when using a lambda function.\")\n\n    data = data.loc[\n        (data[\"Dataset\"] != \"Negative Control\") &amp; (data[\"Dataset\"] != \"Blank\")\n    ]\n    reference_df = data.loc[data[\"Dataset\"] == \"Reference\"]\n    for dataset in filter(lambda x: x != \"Reference\", data[\"Dataset\"].unique()):\n        dataset_data = data.loc[data[\"Dataset\"] == dataset]\n        if \"AcD Barcode 384\" in dataset_data:\n            dataset_barcodes = list(dataset_data[\"AcD Barcode 384\"].unique())\n            dataset_references = reference_df.loc[\n                (reference_df[\"AcD Barcode 384\"].isin(dataset_barcodes)),\n                :,\n            ]\n        else:\n            dataset_references = pd.DataFrame()\n        set_plot = plotfunc(pd.concat([dataset_data, dataset_references]))\n        folder_location = os.path.join(location, dataset)\n        pathlib.Path(folder_location).mkdir(parents=True, exist_ok=True)\n        for fformat in saveformats:\n            print(\n                \"Saving: \",\n                os.path.join(\n                    folder_location,\n                    f\"{dataset}_{plotname}.{fformat}\",\n                ),\n            )\n            set_plot.save(\n                os.path.join(\n                    folder_location,\n                    f\"{dataset}_{plotname}.{fformat}\",\n                )\n            )\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.smiles_to_imgstr","title":"<code>smiles_to_imgstr(smiles)</code>","text":"<p>Converts a smiles string to a base64 encoded image string (e.g. for plotting in altair). It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(), use these if you want more fine grained control over the format of the returned string. Example: df[\"image\"] = df[\"smiles\"].apply(lambda smiles: smiles_to_imgstr(smiles))</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def smiles_to_imgstr(smiles):\n    \"\"\"\n    Converts a smiles string to a base64 encoded image string (e.g. for plotting in altair).\n    It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(),\n    use these if you want more fine grained control over the format of the returned string.\n    Example: df[\"image\"] = df[\"smiles\"].apply(lambda smiles: smiles_to_imgstr(smiles))\n    \"\"\"\n    return imgbuffer_to_imgstr(mol_to_bytes(Chem.MolFromSmiles(smiles)))\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.split_position","title":"<code>split_position(df, position='Position', row='Row_384', col='Col_384')</code>","text":"<p>Split a position like \"A1\" into row and column positions (\"A\", 1) and adds them as columns to the DataFrame. Hint: Remove NAs before applying this function. E.g. <code>split_position(df.dropna(subset=\"Position\"))</code></p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def split_position(\n    df: pd.DataFrame,\n    position: str = \"Position\",\n    row: str = \"Row_384\",\n    col: str = \"Col_384\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Split a position like \"A1\" into row and column positions (\"A\", 1) and adds them as columns to the DataFrame.\n    Hint: Remove NAs before applying this function. E.g. `split_position(df.dropna(subset=\"Position\"))`\n    \"\"\"\n    df[row] = df[position].apply(lambda x: str(x[0]))\n    df[col] = df[position].apply(lambda x: str(x[1:]))\n    return df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.to_excel_molimages","title":"<code>to_excel_molimages(df, filename, desired_columns, mol_col='mol')</code>","text":"<p>Writes a dataframe containing RDKit molecule objects to an excel file containing the molecular structures as PNG images. Needs a column in df with RDKit mol object (e.g. rdkit.Chem.MolFromInchi, MolFromMolBlock, MolFromSmiles etc.)</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def to_excel_molimages(\n    df: pd.DataFrame, filename: str, desired_columns: list[str], mol_col: str = \"mol\"\n):\n    \"\"\"\n    Writes a dataframe containing RDKit molecule objects to an excel file containing the molecular structures as PNG images.\n    Needs a column in df with RDKit mol object (e.g. rdkit.Chem.MolFromInchi, MolFromMolBlock, MolFromSmiles etc.)\n    \"\"\"\n    writer = pd.ExcelWriter(filename, engine=\"xlsxwriter\")\n    workbook = writer.book\n    # workbook = xlsxwriter.Workbook(\"images_bytesio.xlsx\")\n    worksheet = workbook.add_worksheet()\n    df[\"PIL_img\"] = df[mol_col].map(Draw.MolToImage)\n\n    def get_imgbuffer(image):\n        stream = BytesIO()\n        image.save(stream, format=\"PNG\")\n        return stream\n\n    for i, imgbuf in enumerate(df[\"PIL_img\"].map(get_imgbuffer), start=1):\n        worksheet.set_column(0, 0, 20)\n        worksheet.set_row(i, 120)\n        worksheet.insert_image(\n            f\"A{i+1}\", \"img.png\", {\"image_data\": imgbuf, \"x_scale\": 0.5, \"y_scale\": 0.5}\n        )\n\n    df.loc[:, desired_columns].to_excel(writer, startcol=1, index=False)\n    workbook.close()\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.write_excel_MolImages","title":"<code>write_excel_MolImages(df, filename, molcol_header)</code>","text":"<p>Writes images (.png) of molecules structures into an excel file derived from the given dataframe.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def write_excel_MolImages(df: pd.DataFrame, filename: str, molcol_header: str):\n    \"\"\"\n    Writes images (.png) of molecules structures into an excel file derived from the given dataframe.\n    \"\"\"\n    if molcol_header not in df:\n        raise ValueError(\n            f\"Missing {molcol_header} column in df.\"\n        )\n\n    df[\"img_buf\"] = df[molcol_header].apply(lambda x: mol_to_bytes(x) if x else None)\n    writer = pd.ExcelWriter(filename, engine=\"xlsxwriter\")\n\n    workbook = writer.book\n    worksheet = workbook.add_worksheet()\n    for i, imgbuf in enumerate(list(df[\"img_buf\"]), start=1):\n        worksheet.set_column(0, 0, 20)\n        worksheet.set_row(i, 120)\n        worksheet.insert_image(\n            f\"A{i+1}\", \"img.png\", {\"image_data\": imgbuf, \"x_scale\": 0.5, \"y_scale\": 0.5}\n        )\n\n    df.drop(columns=[\"img_buf\"]).to_excel(writer, startcol=1, index=False)\n    workbook.close()\n</code></pre>"}]}