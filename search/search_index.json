{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Toolbox for Robotic-assisted Discovery of Antiinfectives","text":"<p>rda-toolbox is a high-level Python package which aims to provide collection of functions to read and analyze laboratory data in the field of drug discovery.</p> <p> Please note:   If you stumbled across this package by chance, it's probably not for you.   The use case is very specifically tailored to the robotics system of the robotics-assisted discovery of new anti-infectives working group at the Leibniz-HKI. </p>"},{"location":"input/","title":"Input Specifications","text":"<p>Some functions tightly adhere to these specifications and require a certain setup to work. Most experiments require a specific input excel sheet containing information about assay parameters.</p> <p>Because reproducibility is a very important topic, especially in science:</p> <ul> <li>It is highly recommended to use uv for environments</li> <li>Create a virtual environment for each project (Screen)</li> <li>Always at least provide a requirements.txt (<code>uv pip freeze &gt; requirements.txt</code>) for each finished project</li> </ul>"},{"location":"input/#example-project-folder-structure","title":"Example Project Folder Structure","text":"<pre><code>&lt;YYYYMMDD_Assay_OrganismType&gt;\n\u251c\u2500\u2500 code/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 .venv/ # uv venv .venv\n\u251c\u2500\u2500 data/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 input/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 &lt;Assay&gt;_Input.xlsx\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 meta/\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 logs/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 processed/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 raw/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 results/\n\u251c\u2500\u2500 figures/\n\u251c\u2500\u2500 methods/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 module_01/\n\u251c\u2500\u2500 readme.md\n\u2514\u2500\u2500 report/\n</code></pre> <ul> <li>MIC Input File</li> <li>Primary Input File</li> <li></li> </ul>"},{"location":"mic/","title":"Minimal Inhibitory Concentration (MIC) Assay","text":""},{"location":"mic/#read-the-inputs-initialize-the-assay-class","title":"Read the inputs, initialize the assay class","text":"<pre><code>import rda_toolbox as rda\n\nmic = rda.MIC(\n    \"../data/raw/\",  # Folderpath for rawfiles\n    \"../data/input/MIC_Input.xlsx\",  # Input excel table\n    \"../data/input/DiS_MP_AsT_2024-12-02.txt\",  # Mapping file from Motherplates to AssayTransfer plates\n    \"../data/input/AmA_AsT_AcD_20241204.txt\",  # Mapping file from AssayTransfer to ActivityDetermination plates\n    plate_type=384,\n    measurement_label=\"Raw Optical Density\",\n    negative_controls=\"Organism + Medium\",  # Label of negative controls ('Bacteria + Medium', 'Fungi + Medium', 'Organism + Medium', 'Negative Controls' etc.)\n    precip_exclude_outlier=True,  # Exclude outliers from the precipitation\n    precipitation_rawfilepath = \"../data/raw/Precipitation_measurements/\",  # Folderpath for precipitation rawfiles\n)\n</code></pre>"},{"location":"mic/#save-the-results","title":"Save the results","text":"<pre><code>mic.save_results(&lt;tables path&gt;, &lt;figures path&gt;, &lt;processed data path&gt;, figureformats=[\"svg\", \"html\"], tableformats=[\"xlsx\", \"csv\"])\n</code></pre> <p>If everything went well you can stop now.</p> <p>If errors occured you may inspect in-between results and debug from there.</p> <p>(Check your inputs!)</p>"},{"location":"mic/#in-between-inspection","title":"In-between inspection:","text":""},{"location":"mic/#view-in-between-results-eg-in-a-notebook","title":"View in-between results (e.g. in a notebook)","text":"<p>Its possible to inspect the assay object:</p> <pre><code>mic.__dict__\n</code></pre> <p>Show how plates are related to each other (hirarchical dictionary): <pre><code>mic._mapping_dict\n</code></pre></p>"},{"location":"mic/#tables","title":"Tables","text":"<pre><code>mic.mapped_input_df\n</code></pre> <pre><code>mic.processed\n</code></pre> <pre><code>mic.results\n</code></pre>"},{"location":"mic/#visualizations","title":"Visualizations","text":"<pre><code>mic.plateheatmap\n</code></pre>"},{"location":"mic/#save-the-results-separately","title":"Save the results separately","text":"<pre><code>mic.save_tables(\"../data/results/\")\n</code></pre> <pre><code>mic.save_figures(\"../figures/\")\n</code></pre>"},{"location":"precipitation/","title":"Precipitation Test","text":"<p>Quality control method to check for precipitation of substances. A precipitating substance can be incorrectly recognized as active. This assay helps us to mark this false positive result.</p> <p>Precipitation testing is usually done additionally on an experiment such as MIC or primary screen. Therefore, the following examples are for the evaluation of a MIC assay with precipitation measurement.</p> <p>The threshold for a substance to be precipitated is the limit of quantification(LoQ) [1],[2]:</p> <p>LoQ = mean(background) + 10 * standarddeviation(background)</p> <p>There is an option to exclude outliers from the precipitation background samples. For now, outliers are detected by checking for values 2 times above the median of the background samples. <pre><code>mic = rda.MIC(\n    \"../data/raw/\",\n    \"../data/input/MIC_Input.xlsx\",\n    \"../data/input/DiS_MP_AsT_2024-12-02.txt\",\n    \"../data/input/AmA_AsT_AcD_20241204.txt\",\n    precipitation_rawfilepath = \"../data/raw/Pr\u00e4zipitationsmessung/\",\n    precip_exclude_outlier=True,\n)\n</code></pre></p>"},{"location":"precipitation/#specifying-background-samples","title":"Specifying background samples:","text":"<p>Using a list of positions (all plates): - Useful for specifying other background rows or columns for all plates - Shown example is the default, column 24 of each 384 plate is defined as background samples <pre><code>mic = rda.MIC(\n    ...\n    precipitation_rawfilepath = \"../data/raw/Pr\u00e4zipitationsmessung/\",\n    background_locations=[f\"{row}24\" for row in string.ascii_uppercase[:16]]\n)\n</code></pre></p> <p>Using a dictionary of positions with barcodes (plate specific): - Useful for specifying very specific background samples per plates <pre><code>mic = rda.MIC(\n    ...\n    precipitation_rawfilepath = \"../data/raw/Pr\u00e4zipitationsmessung/\",\n    background_locations={}\n)\n</code></pre></p>"},{"location":"precipitation/#precipitation-results","title":"Precipitation Results","text":"<p>The precipitation results are visible in the end-results tables as well as in the <code>.../figures/QualityControl</code> folder.</p> <p></p> <p>A heatmap of each precipitation test plate is written. On the top left corner the calculated \"Limit of Quantification\" (\"Nachweisgrenze\") is shown.</p>"},{"location":"precipitation/#limit-of-quantification","title":"Limit of Quantification","text":"<p>The limit of quantification (LoQ, or LOQ) is the lowest value of a signal (or concentration, activity, response...) that can be quantified with acceptable precision and accuracy [1].</p> <p>Some substances may precipitate at certain concentrations. This may interfere with our method of detecting activity.</p> <p>We calculate the LoQ like this:</p> <p>$$ LoQ = y_B + (10 * s_B) $$ - \\(y_B\\): Mean of the background - \\(s_B\\): Standarddeviation of the background</p>"},{"location":"primary/","title":"Primary Screen Assay","text":""},{"location":"primary/#read-the-inputs-initialize-the-assay-class","title":"Read the inputs, initialize the assay class","text":"<pre><code>import rda_toolbox as rda\n\nprimary = rda.PrimaryScreen(\n    \"../data/raw/\",  # Folder where the raw readerfiles are located\n    \"../data/input/PrS_Input.xlsx\",  # Assay specific Input sheet\n    \"../data/input/AmA_AsT_AcD_20241204.txt\",  # Mapping file (MP -&gt; AcD plates)\n    map_rowname=\"Row 96\",\n    map_colname=\"Col 96\",\n    # Folder where the raw readerfiles for precipitation test are located\n    precipitation_rawfilepath = \"../data/raw/Precipitation_measurements/\"\n)\n</code></pre>"},{"location":"primary/#compounds-already-in-384-well-plates","title":"Compounds already in 384-well plates","text":"<p>Example without mapping from 96 to 384 (motherplate barcode is reused as AsT Barcode):</p> <pre><code>import rda_toolbox as rda\n\nprimary = rda.PrimaryScreen(\n    \"../data/raw/\",\n    \"../data/input/PrS_Input.xlsx\",\n    mappingfile_path = \"../data/input/AmA_AsT_AcD_20251202.txt\",  # Contains mapping of AsT -&gt; AcD plates\n    needs_mapping=False,\n    precipitation_rawfilepath=\"../data/raw/Precipitation/\",\n    substance_id=\"Internal ID\",\n    ast_barcode_header=\"MP Barcode 384\"  # Motherplate is reused as AsT Barcode\n)\n</code></pre> <p>Its possible to inspect the assay object:</p> <pre><code>primary.__dict__\n</code></pre>"},{"location":"primary/#view-in-between-results-eg-in-a-notebook","title":"View in-between results (e.g. in a notebook)","text":""},{"location":"primary/#tables","title":"Tables","text":"<pre><code>primary.mapped_input_df\n</code></pre> <pre><code>primary.processed\n</code></pre> <pre><code>primary.results\n</code></pre>"},{"location":"primary/#visualizations","title":"Visualizations","text":"<pre><code>primary.plateheatmap\n</code></pre>"},{"location":"primary/#save-the-results","title":"Save the results","text":"<pre><code># Save all tables\nprimary.save_tables(\"../data/results/\")\n# Save all figures\nprimary.save_figures(\"../figures/\")\n# Save results (figures and tables)\nprimary.save_results(&lt;tables path&gt;, &lt;figures path&gt;, &lt;processed data path&gt;, figureformats=[\"svg\", \"html\"], tableformats=[\"xlsx\", \"csv\"])\n</code></pre>"},{"location":"primary/#primary-screen-results","title":"Primary Screen Results","text":""},{"location":"setup_marimo/","title":"Setting Up An Experiment","text":"<p>In this example I will use marimo notebooks. <code>rda-toolbox</code> should work with any other notebook or virtual environment utility as well as in plain python files.</p>"},{"location":"setup_marimo/#install-uv","title":"Install UV","text":"<p>UV Installation Docs</p> <p>Start a notebook using a sandboxed environment: <pre><code>marimo edit --sandbox Analysis.py\n</code></pre></p>"},{"location":"tutorial_mappingfiles/","title":"Tutorial Mappingfiles","text":"<ul> <li>Mappingfile A (96-Well Motherplate -&gt; 384-Well Assay Transfer plate)</li> <li>Mappingfile B (384-Well Assay Transfer plate -&gt; 384-Well Activity Determination plate)</li> </ul> <p>Use the function <code>parse_mappingfile</code> to parse mappingfiles generated by the barcode reader <pre><code>import pandas as pd\n\ninput_excel = pd.read_excel(\"\")\n\nmapping_a = parse_mappingfile(\"\")\ninput_excel = pd.merge(input_excel, mapping_a, on=\"MP_Barcode\")\n\nmapping_b = parse_mappingfile(\"\")\ninput_excel = pd.merge(input_excel, mapping_b, on=\"AsT_Barcode\")\n</code></pre></p>"},{"location":"utility_functions/","title":"Utility Functions","text":"<p>The functions described here make up the main part of this library. They are useful if you want to do only certain parts of the analysis pipeline, or, for example, if you want to write a pipeline for a new experiment.</p>"},{"location":"utility_functions/#parse-cytation10-reader-files-into-a-dataframe","title":"Parse Cytation10 reader files into a dataframe","text":"<pre><code>import rda_toolbox as rda\n\nrawdata = rda.parser.parse_readerfiles(\"&lt;rawfiles_path&gt;\")\n</code></pre> Row_384 Col_384 Raw Optical Density AcD Barcode 384 A 1 1.123 001PrS01003 B 1 2.234 001PrS01003 ... ... ... ..."},{"location":"reference/classes/","title":"(Assay) Classes","text":""},{"location":"reference/classes/#rda_toolbox.experiment_classes.Experiment","title":"<code>Experiment</code>","text":"<p>Superclass for all experiments. Reads rawdata into a DataFrame.</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.Experiment--attributes","title":"Attributes","text":"<p>rawdata : pd.DataFrame     DataFrame containing the rawdata</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.Experiment--methods","title":"Methods","text":"<p>save_plots     Save all the resulting plots to figuredir save_tables     Save all the resulting tables to tabledir save     Save all plots and tables to resultdir</p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>class Experiment:\n    \"\"\"\n    Superclass for all experiments.\n    Reads rawdata into a DataFrame.\n\n    Attributes\n    ----------\n    rawdata : pd.DataFrame\n        DataFrame containing the rawdata\n\n    Methods\n    ----------\n    save_plots\n        Save all the resulting plots to figuredir\n    save_tables\n        Save all the resulting tables to tabledir\n    save\n        Save all plots and tables to resultdir\n    \"\"\"\n\n    def __init__(self, rawfiles_folderpath: Optional[str], plate_type: int):\n        self._plate_type = plate_type\n        self._rows, self._columns = get_rows_cols(plate_type)\n        self._rawfiles_folderpath = rawfiles_folderpath\n        # If no path is provided, initialize empty placeholders instead of calling parse_readerfiles\n        if not rawfiles_folderpath:\n            self.rawdata = pd.DataFrame()\n            self.metadata = {}\n        else:\n            self.rawdata, self.metadata = parse_readerfiles(\n                rawfiles_folderpath\n            )  # Get rawdata, this will later be overwritten by adding precipitation, if available\n</code></pre>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.MIC","title":"<code>MIC</code>","text":"<p>               Bases: <code>Experiment</code></p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>class MIC(Experiment):  # Minimum Inhibitory Concentration\n    def __init__(\n        self,\n        rawfiles_folderpath,\n        inputfile_path,\n        mp_ast_mapping_filepath,\n        ast_acd_mapping_filepath,\n        plate_type=384,  # Define default plate_type for experiment\n        measurement_label: str = \"Raw Optical Density\",\n        map_rowname: str = \"Row_96\",\n        map_colname: str = \"Col_96\",\n        q_name: str = \"Quadrant\",\n        mp_barcode_header: str = \"MP Barcode 96\",\n        mp_position_header: str = \"MP Position 96\",\n        # ast_barcode_header: str = \"AsT Barcode 384\",\n        substance_id: str = \"Internal ID\",\n        negative_controls: str = \"Bacteria + Medium\",\n        blanks: str = \"Medium\",\n        norm_by_barcode: str = \"AcD Barcode 384\",\n        thresholds: list[float] | None = None,\n        exclude_negative_zfactors: bool = False,\n        precipitation_rawfilepath: str | None = None,\n        precip_background_locations: pd.DataFrame | list[str] = [\n            f\"{row}24\" for row in string.ascii_uppercase[:16]\n        ],\n        precip_exclude_outlier: bool = False,\n        precip_conc_multiplicator: float = 2.0,\n        molecule_df: pd.DataFrame | None = None,\n        molecule_external_id_column: str = \"External ID\",\n        molecule_column: str = \"mol\",\n    ):\n        super().__init__(rawfiles_folderpath, plate_type)\n        self._inputfile_path = inputfile_path\n        self._mp_ast_mapping_filepath = mp_ast_mapping_filepath\n        self._ast_acd_mapping_filepath = ast_acd_mapping_filepath\n        self._measurement_label = measurement_label\n        self._mp_barcode_header = mp_barcode_header\n        self._mp_position_header = mp_position_header\n        self._molecule_df = molecule_df\n        self._molecule_external_id_column = molecule_external_id_column\n        self._molecule_column = molecule_column\n        self.precipitation = (\n            Precipitation(\n                precipitation_rawfilepath,\n                background_locations=precip_background_locations,\n                exclude_outlier=precip_exclude_outlier,\n            )\n            # if precipitation_rawfilepath\n            # else None\n        )\n        self.precip_conc_multiplicator = precip_conc_multiplicator\n        self.rawdata = (  # Overwrite rawdata if precipitation data is available\n            # self.rawdata\n            # if self.precipitation is None\n            # else\n            add_precipitation(\n                self.rawdata, self.precipitation.results, self._mapping_dict\n            )\n        )\n        self._substances_unmapped, self._organisms, self._dilutions, self._controls = (\n            read_inputfile(inputfile_path, substance_id)\n        )\n\n        self._negative_controls = negative_controls\n        self._blanks = blanks\n        self._norm_by_barcode = norm_by_barcode\n        if thresholds is None:\n            thresholds = [50.0]\n        self.thresholds = thresholds\n        self._processed_only_substances = (\n            self.processed[  # Negative Control is still there!\n                (self.processed[\"Dataset\"] != \"Reference\")\n                &amp; (self.processed[\"Dataset\"] != \"Positive Control\")\n                &amp; (self.processed[\"Dataset\"] != \"Blank\")\n            ]\n        )\n        self._references_results = self.processed.loc[\n            self.processed[\"Dataset\"] == \"Reference\"\n        ]\n        self.substances_precipitation = (\n            None\n            if self.precipitation.results.empty\n            else (\n                self._processed_only_substances[\n                    self._processed_only_substances[\"Dataset\"] != \"Negative Control\"\n                ]\n                .drop_duplicates(\n                    [\"Internal ID\", \"AsT Barcode 384\", \"Row_384\", \"Col_384\"]\n                )\n                .loc[\n                    :,\n                    [\n                        \"Internal ID\",\n                        \"AsT Barcode 384\",\n                        \"Row_384\",\n                        \"Col_384\",\n                        \"Concentration\",\n                        \"Precipitated\",\n                    ],\n                ]\n                .reset_index(drop=True)\n            )\n        )\n        def get_min_precip_conc_df(self):\n            if (self.precipitation.results.empty) and (not self.substances_precipitation):\n                return None\n            else:\n                precip_grps = []\n                # precip_df = self.substances_precipitation\n                for (int_id, ast_barcode), grp in self.substances_precipitation.groupby(\n                    [\"Internal ID\", \"AsT Barcode 384\"]\n                    ):\n                    grp = grp.sort_values(\"Concentration\")\n                    min_precip_conc = None\n                    if grp.Precipitated.any():\n                        min_precip_conc = grp[\"Concentration\"][grp[\"Precipitated\"].idxmax()] * self.precip_conc_multiplicator\n                    grp[\"Minimum Precipitation Concentration\"] = min_precip_conc\n                    precip_grps.append(grp)\n                precip_df = pd.concat(precip_grps)\n                precip_df = precip_df[[\"Internal ID\", \"Minimum Precipitation Concentration\"]]\n                return precip_df\n        self.substances_minimum_precipitation_conc = get_min_precip_conc_df(self)\n        self._exclude_negative_zfactor = exclude_negative_zfactors\n        self.mic_df = self.get_mic_df(\n                # self.processed.copy()\n            df = self.processed[\n                (self.processed[\"Dataset\"] != \"Negative Control\") &amp; (self.processed[\"Dataset\"] != \"Blank\")\n            ].dropna(subset=[\"Concentration\"]).copy()\n        ).reset_index(drop=True)\n\n\n\n    def _validate_mapping_dicts(self, mp_ast_mapping_dict, ast_acd_mapping_dict):\n        invalid_mp_ast = sorted(\n            {\n                str(ast_barcode)\n                for ast_barcodes in mp_ast_mapping_dict.values()\n                for ast_barcode in ast_barcodes\n                if pd.isna(ast_barcode) or not str(ast_barcode).strip()\n            }\n        )\n        invalid_ast_acd_keys = sorted(\n            {\n                str(ast_barcode)\n                for ast_barcode in ast_acd_mapping_dict\n                if pd.isna(ast_barcode) or not str(ast_barcode).strip()\n            }\n        )\n        invalid_acd_barcodes = sorted(\n            {\n                str(acd_barcode)\n                for acd_barcodes in ast_acd_mapping_dict.values()\n                for acd_barcode in acd_barcodes\n                if pd.isna(acd_barcode) or not str(acd_barcode).strip()\n            }\n        )\n        ast_acd_keys = set(ast_acd_mapping_dict)\n        missing_ast_mappings = sorted(\n            {\n                str(ast_barcode)\n                for ast_barcodes in mp_ast_mapping_dict.values()\n                for ast_barcode in ast_barcodes\n                if ast_barcode not in ast_acd_keys\n            }\n        )\n\n        if (\n            invalid_mp_ast\n            or invalid_ast_acd_keys\n            or invalid_acd_barcodes\n            or missing_ast_mappings\n        ):\n            details = []\n            if missing_ast_mappings:\n                details.append(\n                    \"AsT barcodes missing in AsT -&gt; AcD mapping: \"\n                    + \", \".join(map(str, missing_ast_mappings[:10]))\n                )\n            if invalid_mp_ast:\n                details.append(\n                    \"Invalid AsT barcodes in MP -&gt; AsT mapping: \"\n                    + \", \".join(map(str, invalid_mp_ast[:10]))\n                )\n            if invalid_ast_acd_keys:\n                details.append(\n                    \"Invalid AsT barcodes in AsT -&gt; AcD mapping: \"\n                    + \", \".join(map(str, invalid_ast_acd_keys[:10]))\n                )\n            if invalid_acd_barcodes:\n                details.append(\n                    \"Invalid AcD barcodes in AsT -&gt; AcD mapping: \"\n                    + \", \".join(map(str, invalid_acd_barcodes[:10]))\n                )\n            raise ValueError(\n                \"Inconsistent mapping between MP -&gt; AsT and AsT -&gt; AcD mapping files. \"\n                \"Please check the mapping .txt files.\\n\"\n                + \"\\n\".join(details)\n            )\n\n    @property\n    def _mapping_dict(self):\n        mp_ast_mapping_dict = get_mapping_dict(\n            parse_mappingfile(\n                self._mp_ast_mapping_filepath,\n                motherplate_column=self._mp_barcode_header, \n                childplate_column=\"AsT Barcode 384\",\n            ),\n            mother_column=self._mp_barcode_header,\n            child_column=\"AsT Barcode 384\",\n        )\n        ast_acd_mapping_dict = get_mapping_dict(\n            parse_mappingfile(\n                self._ast_acd_mapping_filepath,\n                motherplate_column=\"AsT Barcode 384\",\n                childplate_column=\"AcD Barcode 384\",\n            ),\n            mother_column=\"AsT Barcode 384\",\n            child_column=\"AcD Barcode 384\",\n        )\n        mapping_dict = {}\n        # print(\"MP -&gt; AsT mapping: \", mp_ast_mapping_dict)\n        # print(\"AsT -&gt; AcD mapping: \", ast_acd_mapping_dict)\n        self._validate_mapping_dicts(mp_ast_mapping_dict, ast_acd_mapping_dict)\n        for mp_barcode, ast_barcodes in mp_ast_mapping_dict.items():\n            tmp_dict = {}\n            for ast_barcode in ast_barcodes:\n                tmp_dict[ast_barcode] = ast_acd_mapping_dict[ast_barcode]\n            mapping_dict[mp_barcode] = tmp_dict\n        return mapping_dict\n\n    @cached_property\n    def mapped_input_df(self):\n        \"\"\"\n        Does mapping of the inputfile describing the tested substances with the\n        corresponding mappingfile(s).\n        \"\"\"\n\n        # Sorting of organisms via Rack is **very** important, otherwise data gets attributed to wrong organisms\n        organisms = list(self._organisms.sort_values(by=\"Rack\")[\"Organism\"])\n        formatted_organisms = list(self._organisms.sort_values(by=\"Rack\")[\"Organism formatted\"])\n\n\n        orig_barcodes = list(map(str, self._substances_unmapped[self._mp_barcode_header].unique()))\n        with open(self._mp_ast_mapping_filepath) as file:\n            filecontents = file.read().splitlines()\n        ast_platemapping, _ = read_platemapping(\n            filecontents,\n            orig_barcodes,\n        )\n        # Do some sanity checks:\n        necessary_columns = [\n            \"Dataset\",\n            \"Internal ID\",\n            self._mp_barcode_header,\n            self._mp_position_header,\n        ]\n        # Check if all necessary column are present in the input table:\n        if not all(\n            column in self._substances_unmapped.columns for column in necessary_columns\n        ):\n            raise ValueError(\n                f\"Not all necessary columns are present in the input table.\\n(Necessary columns: {necessary_columns})\"\n            )\n        # Check if all of the necessary column are complete:\n        # TODO: be more precise on what is missing\n        if self._substances_unmapped[necessary_columns].isnull().values.any():\n            raise ValueError(\"Input table incomplete, contains NA (missing) values.\")\n        # Check if there are duplicates in the internal IDs (apart from references)\n        if any(\n            self._substances_unmapped[\n                self._substances_unmapped[\"Dataset\"] != \"Reference\"\n            ][\"Internal ID\"].duplicated()\n        ):\n            raise ValueError(\"Duplicate Internal IDs.\")\n\n        # Map AssayTransfer barcodes to the motherplate barcodes:\n        (\n            self._substances_unmapped[\"Row_384\"],\n            self._substances_unmapped[\"Col_384\"],\n            self._substances_unmapped[\"AsT Barcode 384\"],\n        ) = zip(\n            *self._substances_unmapped.apply(\n                lambda row: mic_assaytransfer_mapping(\n                    row[self._mp_position_header],\n                    row[self._mp_barcode_header],\n                    ast_platemapping,\n                ),\n                axis=1,\n            )\n        )\n        orig_barcodes = list(map(str, self._substances_unmapped[\"AsT Barcode 384\"].unique()))\n        with open(self._ast_acd_mapping_filepath) as file:\n            filecontents = file.read().splitlines()\n        acd_platemapping, replicates_dict = read_platemapping(\n            filecontents,\n            orig_barcodes,\n        )\n        num_replicates = list(set(replicates_dict.values()))[0]\n\n        single_subst_concentrations = []\n        for dataset in self._substances_unmapped[\"Dataset\"].unique():\n            for substance, subst_row in self._substances_unmapped[self._substances_unmapped[\"Dataset\"] == dataset].groupby(\"Internal ID\"):\n                # Collect the concentrations each as rows for a single substance:\n                single_subst_conc_rows = []\n                init_pos = int(subst_row[\"Col_384\"].iloc[0]) - 1\n                col_positions_384 = [list(range(1, 23, 2)), list(range(2, 23, 2))]\n                for col_i, conc in enumerate(\n                    list(self._dilutions[self._dilutions[\"Dataset\"] == dataset][\"Concentration\"].unique())\n                ):\n                    # Add concentration:\n                    subst_row[\"Concentration\"] = conc\n                    # Add corresponding column:\n                    subst_row[\"Col_384\"] = int(col_positions_384[init_pos][col_i])\n                    single_subst_conc_rows.append(subst_row.copy())\n\n                # Concatenate all concentrations rows for a substance in a dataframe\n                if single_subst_conc_rows:\n                    single_subst_concentrations.append(pd.concat(single_subst_conc_rows))\n        # Concatenate all self._substances_unmapped dataframes to one whole\n        input_w_concentrations = pd.concat(single_subst_concentrations)\n\n        acd_dfs_list = []\n        for ast_barcode, ast_plate in input_w_concentrations.groupby(\"AsT Barcode 384\"):\n            controls_with_barcode = self._controls.assign(**{\"AsT Barcode 384\": ast_barcode})\n            ast_plate = pd.concat([ast_plate, controls_with_barcode], ignore_index=True)\n\n            for org_i, organism in enumerate(organisms):\n                for replicate in range(num_replicates):\n                    # Add the AcD barcode\n                    ast_plate[\"AcD Barcode 384\"] = acd_platemapping[ast_barcode][\n                        replicate\n                    ][org_i]\n\n                    ast_plate[\"Replicate\"] = replicate + 1\n                    # Add the scientific Organism name\n                    ast_plate[\"Organism formatted\"] = formatted_organisms[org_i]\n                    ast_plate[\"Organism\"] = organism\n                    acd_dfs_list.append(ast_plate.copy())\n                    # Add concentrations:\n        acd_single_concentrations_df = pd.concat(acd_dfs_list)\n\n        # merge rawdata with input specifications\n        df = pd.merge(self.rawdata, acd_single_concentrations_df, how=\"outer\").dropna(subset=[\"Internal ID\"])\n        if self._molecule_df is not None:\n            df = add_molecule_data(\n                df,\n                self._molecule_df,\n                external_id=self._molecule_external_id_column,\n                mol_column=self._molecule_column,\n            )\n        return df\n\n    @cached_property\n    def processed(self):\n        return preprocess(\n            self.mapped_input_df,\n            substance_id=\"Internal ID\",\n            measurement=self._measurement_label.strip(\n                \"Raw \"\n            ),  # I know this is weird, its because of how background_normalize_zfactor works,\n            negative_controls=self._negative_controls,\n            blanks=self._blanks,\n            norm_by_barcode=self._norm_by_barcode,\n        )\n\n    @cached_property\n    def plateheatmap(self):\n        return plateheatmaps(\n            self.processed,\n            substance_id=\"Internal ID\",\n            barcode=self._norm_by_barcode,\n            negative_control=self._negative_controls,\n            blank=self._blanks,\n        )\n\n    # def lineplots_facet(self):\n    #    return lineplots_facet(self.processed)\n\n    @cached_property\n    def _resultfigures(self) -&gt; list[Result]:\n        result_figures = []\n        result_figures.append(\n            Result(\"QualityControl\", \"plateheatmaps\", figure=self.plateheatmap)\n        )\n        result_figures.append(\n            Result(\"QualityControl\", \"zfactor_heatmap\", figure=get_zfactor_heatmap(self.processed))\n        )\n        if (self.substances_precipitation is not None) and (\n            not self.substances_precipitation.empty\n        ):\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Precipitation_Heatmap\",\n                    figure=self.precipitation.plateheatmap(),\n                )\n            )\n\n        # Save plots per dataset:\n        processed_negative_zfactor = self._processed_only_substances[\n            self._processed_only_substances[\"Z-Factor\"] &lt; 0\n        ]\n        if (\n            not processed_negative_zfactor.empty\n            and self._exclude_negative_zfactor == True\n        ):\n            print(\n                f\"{len(processed_negative_zfactor[\"AsT Barcode 384\"].unique())} plate(s) with negative Z-Factor detected for organisms '{\", \".join(processed_negative_zfactor[\"Organism formatted\"].unique())}'.\\n\",\n                \"These plates will be excluded from the lineplots visualization!\\n (If you want to include them, use the `exclude_negative_zfactors=False` flag of the MIC class)\",\n            )\n\n        for dataset, dataset_data in self._processed_only_substances.groupby(\"Dataset\"):\n            dataset_name = str(dataset)\n            # Look for and add the corresponding references for each dataset:\n            if \"AcD Barcode 384\" in dataset_data:\n                dataset_barcodes = list(dataset_data[\"AcD Barcode 384\"].unique())\n                corresponding_dataset_references = self._references_results.loc[\n                    (\n                        self._references_results[\"AcD Barcode 384\"].isin(\n                            dataset_barcodes\n                        )\n                    ),\n                    :,\n                ]\n            else:\n                corresponding_dataset_references = pd.DataFrame()\n\n            lineplots_input_df = pd.concat(\n                [dataset_data, corresponding_dataset_references]\n            )\n            lineplots_input_df = lineplots_input_df.dropna(\n                subset=[\"Concentration\"]\n            ).loc[\n                (lineplots_input_df[\"Dataset\"] != \"Negative Control\")\n                &amp; (lineplots_input_df[\"Dataset\"] != \"Blank\"),\n                :,\n            ]\n            if not lineplots_input_df.empty:\n                for threshold in self.thresholds:\n                    result_figures.append(\n                        Result(\n                            dataset_name,\n                            f\"{dataset_name}_lineplots_facet_thrsh{threshold}_InternalID\",\n                            figure=lineplots_facet(\n                                lineplots_input_df,\n                                by_id=\"Internal ID\",\n                                exclude_negative_zfactors=self._exclude_negative_zfactor,\n                                threshold=threshold,\n                            ),\n                        )\n                    )\n                    result_figures.append(\n                        Result(\n                            dataset_name,\n                            f\"{dataset_name}_lineplots_facet_thrsh{threshold}_ExternalID\",\n                            figure=lineplots_facet(\n                                lineplots_input_df,\n                                by_id=\"External ID\",\n                                exclude_negative_zfactors=self._exclude_negative_zfactor,\n                                threshold=threshold,\n                            ),\n                        )\n                    )\n\n        # Save plots per threshold:\n        for threshold in self.thresholds:\n            for dataset, sub_df in self.mic_df.groupby(\"Dataset\"):\n                dataset_name = str(dataset)\n                # print(sub_df)\n                sub_df = sub_df.dropna(subset=f\"MIC{threshold} in \u00b5M\")\n                if sub_df.empty:\n                    print(f\"No MICs for dataset: {dataset_name}, threshold: {threshold}\")\n                    continue\n                dummy_df = get_upsetplot_df(\n                    sub_df,\n                    counts_column=\"Internal ID\",\n                    set_column=\"Organism\",\n                )\n\n                result_figures.append(\n                    Result(\n                        dataset_name,\n                        f\"{dataset_name}_UpSetPlot\",\n                        figure=UpSetAltair(dummy_df, title=dataset_name),\n                    )\n                )\n                result_figures.append(\n                    Result(\n                        dataset_name,\n                        f\"{dataset_name}_PotencyDistribution\",\n                        figure=potency_distribution(sub_df, threshold, dataset_name),\n                    )\n                )\n        return result_figures\n\n    def get_mic_df(self, df):\n\n        pivot_df = pd.pivot_table(\n            df,\n            values=[\"Relative Optical Density\", \"Replicate\", \"Z-Factor\", \"Robust Z-Factor\"],\n            index=[\n                \"Internal ID\",\n                # \"External ID\",\n                \"Organism formatted\",\n                \"Organism\",\n                \"Concentration\",\n                \"Dataset\",\n            ],\n            aggfunc={\n                \"Relative Optical Density\": [\"mean\"],\n                \"Replicate\": [\"count\"],\n                \"Z-Factor\": [\"mean\", \"std\"],\n                \"Robust Z-Factor\": [\"mean\", \"std\"],\n            },\n            # margins=True\n            fill_value=0 # This might result in confusion, if there are no replicates (1)\n        ).reset_index()\n\n        pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n\n        mic_records = []\n        for group_names, grp in pivot_df.groupby(\n            [\"Internal ID\", \"Organism formatted\", \"Dataset\"]\n        ):\n            internal_id, organism_formatted, dataset = group_names\n            # Sort by concentration just to be sure:\n            grp = grp[\n                [\n                    \"Concentration\",\n                    \"Relative Optical Density mean\",\n                    \"Z-Factor mean\",\n                    \"Z-Factor std\",\n                    \"Robust Z-Factor mean\",\n                    \"Robust Z-Factor std\",\n                ]\n            ].sort_values(by=[\"Concentration\"])\n\n            # Get rows where the OD is below the given threshold:\n            record = {\n                \"Internal ID\": internal_id,\n                \"Organism formatted\": organism_formatted,\n                \"Dataset\": dataset,\n                \"Z-Factor mean\": list(grp[\"Z-Factor mean\"])[0],\n                \"Z-Factor std\": list(grp[\"Z-Factor std\"])[0],\n                \"Robust Z-Factor mean\": list(grp[\"Robust Z-Factor mean\"])[0],\n                \"Robust Z-Factor std\": list(grp[\"Robust Z-Factor std\"])[0],\n            }\n\n            for threshold in self.thresholds:\n                values_below_threshold = grp[\n                    grp[\"Relative Optical Density mean\"] &lt; threshold\n                ]\n                # thx to jonathan - check if the OD at maximum concentration is below threshold (instead of any concentration)\n                max_conc_below_threshold = list(\n                    grp[grp[\"Concentration\"] == max(grp[\"Concentration\"])][\n                        \"Relative Optical Density mean\"\n                    ]\n                    &lt; threshold\n                )[0]\n                if not max_conc_below_threshold:\n                    mic = None\n                else:\n                    mic = values_below_threshold.iloc[0][\"Concentration\"]\n                record[f\"MIC{threshold} in \u00b5M\"] = mic\n            mic_records.append(record)\n        # Drop entries where no MIC could be determined\n        mic_df = pd.DataFrame.from_records(mic_records)\n        # Merge inconsistent (but maybe necessary) columns again\n        merge_columns = [\"Internal ID\", \"External ID\"] + [\n            col for col in [\"InChI\", \"InChI-Key\"] if col in df.columns\n        ]\n        mic_df = pd.merge(mic_df, df[merge_columns], on=[\"Internal ID\"])\n        mic_df = pd.merge(mic_df, self._organisms[[\"Organism\", \"Organism formatted\"]], on=[\"Organism formatted\"])\n        unit_values = self._dilutions.get(\"Unit\")\n        mic_df[\"Unit\"] = unit_values.dropna().iloc[0] if unit_values is not None and not unit_values.dropna().empty else None\n        mic_df = mic_df.drop_duplicates()\n        return mic_df\n\n\n    @cached_property\n    def _resulttables(self) -&gt; list[Result]:\n        \"\"\"\n        Retrieves result tables and returns them like list[Result]\n        where Resulttable is a dataclass collecting meta information about the plot.\n        \"\"\"\n        result_tables = []\n        df = self.processed.copy()\n\n\n        # mic_df = self.get_mic_df(df)\n        references_mic_results = self.get_mic_df(\n            self.processed[self.processed[\"Dataset\"] == \"Reference\"].copy()\n        ).reset_index(drop=True)\n\n        result_tables.append(\n            Result(\n                \"Reference\",\n                \"References_MIC_results_eachRefID\",\n                table=references_mic_results,\n            )\n        )\n\n        mic_df = self.mic_df\n        # If precipitation has been done, merge MPC results on long mic_df\n        if not self.precipitation.results.empty and not self.substances_minimum_precipitation_conc is None:\n            mic_df = pd.merge(self.mic_df, self.substances_minimum_precipitation_conc, on=\"Internal ID\", how=\"left\")\n\n        result_tables.append(\n            Result(\"All\", \"MIC_Results_AllDatasets_longformat\", table=self.mic_df)\n        )\n\n        for dataset, dataset_grp in mic_df.groupby(\"Dataset\"):\n            dataset_name = str(dataset)\n            print(f\"Preparing tables for dataset: {dataset_name}\")\n\n            # dataset_grp = dataset_grp.fillna(\"NA\")\n            values_list = [f\"MIC{threshold} in \u00b5M\" for threshold in self.thresholds] + [\n                \"Z-Factor mean\",\n                \"Z-Factor std\",\n            ]\n            # create pivot without resetting index so we can manipulate the MultiIndex columns\n            pivot_df = pd.pivot_table(\n                dataset_grp,\n                values=values_list,\n                index=[\"Internal ID\", \"Dataset\"],\n                columns=\"Organism\",\n            )\n\n            # ensure columns are a MultiIndex of (value, organism) even if only one value was pivoted\n            if not isinstance(pivot_df.columns, pd.MultiIndex):\n                # single value case: pivot_df.columns are organisms, create first level from the single value name\n                first_value_name = values_list[0]\n                pivot_df.columns = pd.MultiIndex.from_product([[first_value_name], pivot_df.columns.tolist()])\n\n            # determine full set of organisms we want to keep (preserve order from self._organisms if available)\n            try:\n                organisms = list(self._organisms.sort_values(by=\"Rack\")[\"Organism\"].tolist())\n            except Exception:\n                organisms = list(self._organisms[\"Organism\"].unique())\n\n            # build expected full MultiIndex and reindex to keep columns that were all-NaN\n            expected_columns = pd.MultiIndex.from_product([values_list, organisms])\n            pivot_df = pivot_df.reindex(columns=expected_columns)\n\n            # finally reset index to get the same shape as before\n            pivot_multiindex_df = pivot_df.reset_index()\n\n            for threshold in self.thresholds:\n\n                if pivot_multiindex_df.empty:\n                    continue\n\n                organisms_thresholded_mics = pivot_multiindex_df[\n                    [\"Internal ID\", f\"MIC{threshold} in \u00b5M\"]\n                ]\n                cols = list(organisms_thresholded_mics.columns.droplevel())\n                cols[0] = \"Internal ID\"\n                organisms_thresholded_mics.columns = cols\n                organisms_thresholded_mics = organisms_thresholded_mics.sort_values(\n                    by=list(organisms_thresholded_mics.columns)[1:],\n                    na_position=\"last\",\n                )\n\n                # Fill with nan if not available\n                organisms_thresholded_mics = organisms_thresholded_mics.round(2)\n                organisms_thresholded_mics = organisms_thresholded_mics.astype(str)\n                id_info_columns = [\"Internal ID\", \"External ID\"] + [\n                    col for col in [\"InChI\", \"InChI-Key\"] if col in self.mic_df.columns\n                ]\n                organisms_thresholded_mics = pd.merge(\n                    organisms_thresholded_mics,\n                    self.mic_df[id_info_columns],\n                    on=[\"Internal ID\"],\n                    how=\"left\",\n                )\n                # organisms_thresholded_mics.fillna(\"NA\", inplace=True)\n\n                if not self.precipitation.results.empty and not self.substances_minimum_precipitation_conc is None:\n                    organisms_thresholded_mics = pd.merge(\n                        organisms_thresholded_mics,\n                        self.substances_minimum_precipitation_conc,\n                        how=\"left\"\n                    )\n                organisms_thresholded_mics = organisms_thresholded_mics.reset_index(drop=True)\n                organisms_thresholded_mics = organisms_thresholded_mics.drop_duplicates()\n                # Add unit column\n                unit_values = self._dilutions.get(\"Unit\")\n                organisms_thresholded_mics[\"Unit\"] = unit_values.dropna().iloc[0] if unit_values is not None and not unit_values.dropna().empty else None\n                # Reorder columns\n                desired_order = [\"Internal ID\", \"External ID\"] + [\n                    col for col in [\"InChI\", \"InChI-Key\"] if col in organisms_thresholded_mics.columns\n                ]\n                remaining_cols = [col for col in organisms_thresholded_mics.columns if col not in desired_order]\n                organisms_thresholded_mics = organisms_thresholded_mics[desired_order + remaining_cols]\n                organisms_thresholded_mics = organisms_thresholded_mics.replace(\"nan\", \"NA\").fillna(\"NA\")\n                result_tables.append(\n                    Result(\n                        dataset_name,\n                        f\"{dataset_name}_MIC{int(round(threshold))}_results\",\n                        table=organisms_thresholded_mics.reset_index(drop=True)\n                    )\n                )\n\n        return result_tables\n\n    @cached_property\n    def results(self):\n        \"\"\"\n        Retrieves result tables (from self._resulttables)\n        and returns them in a dictionary like:\n            {\"&lt;filepath&gt;\": pd.DataFrame}\n        \"\"\"\n        return {tbl.file_basename: tbl.table for tbl in self._resulttables}\n\n    def save_figures(self, result_path, fileformats: list[str] = [\"svg\", \"html\"]):\n        _save_figures(result_path, self._resultfigures, fileformats=fileformats)\n\n    def save_tables(\n        self, result_path, processed_path, fileformats: list[str] = [\"xlsx\", \"csv\"]\n    ):\n        # Create folder if not existent:\n        pathlib.Path(processed_path).mkdir(parents=True, exist_ok=True)\n        self.processed.to_csv(os.path.join(processed_path, \"processed.csv\"))\n        _save_tables(result_path, self._resulttables, fileformats=fileformats)\n\n    def save_results(\n        self,\n        tables_path: str,\n        figures_path: str,\n        processed_path: str,\n        figureformats: list[str] = [\"svg\", \"html\"],\n        tableformats: list[str] = [\"xlsx\", \"csv\"],\n    ):\n        self.save_figures(figures_path, fileformats=figureformats)\n        self.save_tables(tables_path, processed_path, fileformats=tableformats)\n</code></pre>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.MIC.mapped_input_df","title":"<code>mapped_input_df</code>  <code>cached</code> <code>property</code>","text":"<p>Does mapping of the inputfile describing the tested substances with the corresponding mappingfile(s).</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.MIC.results","title":"<code>results</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves result tables (from self._resulttables) and returns them in a dictionary like:</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen","title":"<code>PrimaryScreen</code>","text":"<p>               Bases: <code>Experiment</code></p> <p>Primary screen experiment. Usually done using only 1 concentration.</p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>class PrimaryScreen(Experiment):\n    \"\"\"\n    Primary screen experiment. Usually done using only 1 concentration.\n    \"\"\"\n\n    def __init__(\n        self,\n        rawfiles_folderpath: str,\n        inputfile_path: str,\n        mappingfile_path: str,\n        plate_type: int = 384,  # Define default plate_type for experiment\n        measurement_label: str = \"Raw Optical Density\",\n        map_rowname: str = \"Row_96\",\n        map_colname: str = \"Col_96\",\n        q_name: str = \"Quadrant\",\n        substance_id: str = \"Internal ID\",\n        negative_controls: str = \"Bacteria + Medium\",\n        blanks: str = \"Medium\",\n        norm_by_barcode: str = \"AcD Barcode 384\",\n        ast_barcode_header: str = \"AsT Barcode 384\",\n        ast_position_header: str = \"AsT Position 384\",\n        thresholds: list[float] | None = None,\n        b_score_threshold: float = -3.0,\n        precipitation_rawfilepath: str | None = None,\n        background_locations: pd.DataFrame | list[str] | None = None,\n        precip_exclude_outlier: bool = False,\n        needs_mapping: bool = True,\n        molecule_df: pd.DataFrame | None = None,\n        molecule_external_id_column: str = \"External ID\",\n        molecule_column: str = \"mol\",\n    ):\n        super().__init__(rawfiles_folderpath, plate_type)\n        self._measurement_label = measurement_label\n        self._mappingfile_path = mappingfile_path\n        self._inputfile_path = inputfile_path\n        self._substances_unmapped, self._organisms, self._dilutions, self._controls = (\n            read_inputfile(inputfile_path, substance_id)\n        )\n        if needs_mapping and (\n            not map_rowname\n            or not map_colname\n            or map_rowname not in self._substances_unmapped.columns\n            or map_colname not in self._substances_unmapped.columns\n        ):\n            self._substances_unmapped = split_position(\n                self._substances_unmapped,\n                position=\"Origin Position 96\",\n                row=\"Row_96\",\n                col=\"Col_96\",\n                copy=False,\n            )\n            map_rowname = \"Row_96\"\n            map_colname = \"Col_96\"\n\n        self.substances = (\n            mapapply_96_to_384(\n                self._substances_unmapped,\n                rowname=map_rowname,\n                colname=map_colname,\n                q_name=q_name,\n            )\n            if needs_mapping\n            else split_position(\n                self._substances_unmapped,\n                position=ast_position_header,  # \"MP Position 384\",\n                row=\"Row_384\",\n                col=\"Col_384\"\n            )\n        )\n\n        self._mapping_df = parse_mappingfile(\n            mappingfile_path,\n            motherplate_column=ast_barcode_header,\n            childplate_column=norm_by_barcode,  # \"AcD Barcode 384\",\n        )\n        self._mapping_dict = get_mapping_dict(self._mapping_df, mother_column=ast_barcode_header)\n        # self._substance_id = substance_id\n        if negative_controls not in self._controls[\"Internal ID\"].values:\n            raise ValueError(\n                f\"negative_controls '{negative_controls}' not found in controls 'Internal ID' column.\\nConsider changing the 'negative_controls' keyword to a value in the input excel.\"\n            )\n        self._negative_controls = negative_controls\n        if blanks not in self._controls[\"Internal ID\"].values:\n            raise ValueError(\n                f\"blanks '{blanks}' not found in controls 'Internal ID' column.\\nConsider changing the 'blanks' keyword to a value in the input excel.\"\n            )\n        self._blanks = blanks\n        self._norm_by_barcode = norm_by_barcode\n        self._ast_barcode_header = ast_barcode_header\n        if thresholds is None:\n            thresholds = [50.0]\n        self.thresholds = thresholds\n        self.b_score_threshold = b_score_threshold\n        self._molecule_df = molecule_df\n        self._molecule_external_id_column = molecule_external_id_column\n        self._molecule_column = molecule_column\n        if background_locations is None:\n            background_locations = [\n                f\"{row}24\" for row in string.ascii_uppercase[:16]\n            ]\n\n        self.precipitation = (\n            Precipitation(\n                precipitation_rawfilepath,\n                background_locations=background_locations,\n                exclude_outlier=precip_exclude_outlier,\n            )\n            # if precipitation_rawfilepath\n            # else None\n        )\n        self.rawdata = (  # Overwrite rawdata if precipitation data is available\n            # self.rawdata\n            # if self.precipitation is None\n            # else\n            add_precipitation(\n                self.rawdata, self.precipitation.results, self._mapping_dict\n            )\n        )\n        self._processed_only_substances = self.processed[\n            (self.processed[\"Dataset\"] != \"Reference\")\n            &amp; (self.processed[\"Dataset\"] != \"Positive Control\")\n            &amp; (self.processed[\"Dataset\"] != \"Blank\")\n        ]\n        self.substances_precipitation = (\n            None\n            if self.precipitation.results.empty\n            else (\n                self._processed_only_substances[\n                    self._processed_only_substances[\"Dataset\"] != \"Negative Control\"\n                ]\n                .drop_duplicates(\n                    [\"Internal ID\", self._ast_barcode_header, \"Row_384\", \"Col_384\"]\n                )\n                .loc[\n                    :,\n                    [\n                        \"Internal ID\",\n                        # \"AsT Barcode 384\",\n                        # \"Row_384\",\n                        # \"Col_384\",\n                        \"Concentration\",\n                        \"Precipitated\",\n                        f\"Precipitated at {measurement_label}\",\n                    ],\n                ]\n                .reset_index(drop=True)\n            )\n        )\n\n    def check_substances(self):\n        \"\"\"\n        Do some sanity checks for the substances table.\n        - Check if all necessary columns are present.\n        - Check if substances contains missing values.\n        - Check if there are duplicate Internal IDs (references excluded)\n        \"\"\"\n\n\n    @cached_property\n    def mapped_input_df(self):\n        \"\"\"\n        Does mapping of the inputfile describing the tested substances with the\n        corresponding mappingfile(s).\n        *Basically replaces rda.process.primary_process_inputs() function so all the variables and intermediate results are available via the class*\n        \"\"\"\n        control_wbarcodes = []\n        # multiply controls with number of AsT plates to later merge them with substances df\n        for origin_barcode in list(self.substances[self._ast_barcode_header].unique()):\n            controls_subdf = self._controls.copy()\n            controls_subdf[self._ast_barcode_header] = origin_barcode\n            control_wbarcodes.append(controls_subdf)\n        controls_n_barcodes = pd.concat(control_wbarcodes)\n\n        ast_plate_df = pd.merge(\n            pd.concat([self.substances, controls_n_barcodes]), # concatenate substances and controls\n            self._dilutions,  # merge with dilutions on column \"Dataset\"\n            how=\"outer\",\n            on=\"Dataset\",  # Explicitly define on which column to merge\n        )\n\n        mapped_organisms = pd.merge(self._mapping_df, self._organisms, on=\"Rack\")\n        rawdata_mapped_organism = pd.merge(mapped_organisms, self.rawdata)\n\n        result_df = pd.concat(\n            [\n                pd.merge(org_df, ast_plate_df, how=\"inner\")\n                for _, org_df in rawdata_mapped_organism.groupby(\n                    \"Organism formatted\"\n                )\n            ]\n        )\n        if result_df.empty:\n            raise ValueError(\n                \"After mapping the input substances to the rawdata, the resulting DataFrame is empty.\\nThis means that no data points could be attributed to any substance.\\nPlease check if the mappingfiles and inputfile are correct and consistent with each other.\"\n            )\n\n        for ast_barcode, ast_plate in result_df.groupby(self._ast_barcode_header):\n            logger.info(\n                f\"AsT Plate {ast_barcode} has size: {\n                    len(ast_plate) // len(ast_plate['AcD Barcode 384'].unique())\n                }\"\n            )\n            logger.info(f\"{ast_barcode} -&gt; {ast_plate['AcD Barcode 384'].unique()}\")\n\n        if self._molecule_df is not None:\n            result_df = add_molecule_data(\n                result_df,\n                self._molecule_df,\n                external_id=self._molecule_external_id_column,\n                mol_column=self._molecule_column,\n            )\n        # result_df = result_df.rename({self._substance_id: \"Internal ID\"}) # rename whatever substance ID was given to Internal ID\n        return result_df\n\n    @cached_property\n    def processed(self):\n        processed = preprocess(\n            self.mapped_input_df,\n            substance_id=\"Internal ID\",\n            measurement=self._measurement_label.strip(\n                \"Raw \"\n            ),  # I know this is weird, its because of how background_normalize_zfactor works,\n            negative_controls=self._negative_controls,\n            blanks=self._blanks,\n            norm_by_barcode=self._norm_by_barcode,\n        )\n\n        # Add B-Scores to plates without negative controls and blanks\n        proc_wo_controls = processed[\n            ~processed[\"Internal ID\"].isin([self._negative_controls, self._blanks])\n        ]\n        # We add b_scores here since we only want them in a primary screen and preprocess() is used generally\n        b_scores = (\n            proc_wo_controls.groupby(self._norm_by_barcode)[\n                [self._norm_by_barcode, \"Row_384\", \"Col_384\", self._measurement_label]\n            ]\n            .apply(lambda plate_grp: add_b_score(plate_grp))\n            .reset_index(drop=True)\n        )\n        processed = pd.merge(processed, b_scores, how=\"outer\")\n        return processed\n\n    @cached_property\n    def plateheatmap(self):\n        return plateheatmaps(\n            self.processed.fillna(\"\"),\n            substance_id=\"Internal ID\",\n            negative_control=self._negative_controls,\n            blank=self._blanks,\n            barcode=self._norm_by_barcode,\n        )\n\n    @cached_property\n    def _resultfigures(self):\n        result_figures = []\n        # Add QualityControl overview of the plates as heatmaps:\n        result_figures.append(\n            Result(\"QualityControl\", \"plateheatmaps\", figure=self.plateheatmap)\n        )\n\n        result_figures.append(\n            Result(\"QualityControl\", \"zfactor_heatmap\", figure=get_zfactor_heatmap(self.processed, y_rows=self._ast_barcode_header))\n        )\n        # If precipitation testing was done, add it to QC result figures:\n        if not self.precipitation.results.empty:\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Heatmap_Precipitation\",\n                    figure=self.precipitation.plateheatmap(),\n                )\n            )\n\n        for threshold in self.thresholds:\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Scatter_Measurement_vs_BScore_Substances\",\n                    figure=measurement_vs_bscore_scatter(\n                        self._processed_only_substances,\n                        measurement_header=\"Relative Optical Density\",\n                        measurement_title=\"Relative Optical Density\",\n                        bscore_header=\"b_scores\",\n                        bscore_title=\"B-Score\",\n                        color_header=\"Dataset\",\n                        measurement_threshold=threshold,\n                        b_score_threshold=self.b_score_threshold,\n                    ).facet(row=\"Organism\", column=\"Dataset\"),\n                )\n            )\n            result_figures.append(\n                Result(\n                    \"QualityControl\",\n                    \"Scatter_Measurement_vs_BScore_References\",\n                    figure=measurement_vs_bscore_scatter(\n                        self.processed[\n                            self.processed[\"Dataset\"] == \"Reference\"\n                        ].replace({np.nan: None}),\n                        measurement_header=\"Relative Optical Density\",\n                        measurement_title=\"Relative Optical Density\",\n                        bscore_header=\"b_scores\",\n                        bscore_title=\"B-Score\",\n                        color_header=\"Dataset\",\n                        measurement_threshold=threshold,\n                        b_score_threshold=self.b_score_threshold,\n                    ).facet(row=\"Organism\", column=\"Dataset\"),\n                )\n            )\n\n            subset = get_thresholded_subset(\n                self._processed_only_substances,\n                id_column=\"Internal ID\",\n                negative_controls=self._negative_controls,\n                blanks=self._blanks,\n                threshold=threshold,\n            )\n            for dataset, sub_df in subset.groupby(\"Dataset\"):\n                dataset_name = str(dataset)\n                dummy_df = get_upsetplot_df(sub_df, counts_column=\"Internal ID\")\n\n                result_figures.append(\n                    Result(\n                        dataset_name,\n                        f\"UpSetPlot_{dataset_name}\",\n                        figure=UpSetAltair(dummy_df, title=dataset_name),\n                    )\n                )\n                # ---\n                only_actives = self.results[f\"{dataset_name}_all_results\"][\n                    self.results[f\"{dataset_name}_all_results\"]\n                    .groupby(\"Organism\")[\"Relative Optical Density mean\"]\n                    .transform(lambda x: x &lt; threshold)\n                ]\n                result_figures.append(\n                    Result(\n                        dataset_name,\n                        f\"Scatterplot_BScores_{dataset_name}\",\n                        figure=measurement_vs_bscore_scatter(\n                            only_actives, show_area=False\n                        ),\n                    )\n                )\n        return result_figures\n\n    @cached_property\n    def _resulttables(self):\n        \"\"\"\n        Retrieves result tables and returns them like list[Resulttable]\n        where Resulttable is a dataclass collecting meta information about the plot.\n        \"\"\"\n\n        # result_plots = dict() # {\"filepath\": plot}\n        result_tables = []\n        # result_tables.append(Result(\"All\", ))\n\n        df = self.processed.copy().round(2)\n        df = df[\n            #(df[\"Dataset\"] != \"Reference\")\n            (df[\"Dataset\"] != \"Positive Control\")\n            &amp; (df[\"Dataset\"] != \"Blank\")\n        ].dropna(subset=[\"Concentration\"])\n\n        pivot_df = pd.pivot_table(\n            df,\n            values=[\n                \"Relative Optical Density\",\n                \"Replicate\",\n                \"Z-Factor\",\n                \"Robust Z-Factor\",\n                \"b_scores\",\n            ],\n            index=[\n                \"Internal ID\",\n                \"Organism formatted\",\n                \"Organism\",\n                \"Concentration\",\n                \"Unit\",\n                \"Dataset\",\n            ],\n            aggfunc={\n                # We need lists here for MultiIndex, otherwise the returned DataFrame is flat\n                \"Relative Optical Density\": [\"mean\"],\n                \"Replicate\": [\"count\"],\n                \"b_scores\": [\"mean\"],\n                \"Z-Factor\": [\"mean\"],\n                \"Robust Z-Factor\": [\"mean\"],\n            },\n        ).reset_index().round(2)\n        pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n        molecule_columns = [col for col in [\"InChI\", \"InChI-Key\"] if col in df.columns]\n        molecule_info_df = (\n            df[[\"Internal ID\"] + molecule_columns]\n            .dropna(subset=[\"Internal ID\"])\n            .drop_duplicates(\"Internal ID\")\n            if molecule_columns\n            else None\n        )\n\n        for threshold in self.thresholds:\n            # Apply Threshold to % Growth:\n            for dataset, dataset_grp in pivot_df.groupby(\"Dataset\"):\n                dataset_name = str(dataset)\n                if molecule_info_df is not None:\n                    dataset_grp = pd.merge(\n                        dataset_grp,\n                        molecule_info_df,\n                        how=\"left\",\n                        on=\"Internal ID\",\n                    )\n                if not self.precipitation.results.empty and not self.substances_precipitation is None:\n                    dataset_grp = pd.merge(dataset_grp, self.substances_precipitation, how=\"outer\")\n                    dataset_grp = dataset_grp[dataset_grp[\"Relative Optical Density mean\"].notna()]\n\n                result_tables.append(\n                    Result(dataset_name, f\"{dataset_name}_all_results\", table=dataset_grp)\n                )\n\n                # Apply threshold conditions:\n                thresholded_dataset_grp = dataset_grp.groupby(\"Internal ID\").filter(\n                    lambda x: check_activity_conditions(\n                        x[\"Relative Optical Density mean\"],\n                        x[\"b_scores mean\"],\n                        threshold,\n                        self.b_score_threshold,\n                    )\n                )\n\n                # Pivot the long table for excel viewability:\n                pivot_multiindex_df = pd.pivot_table(\n                    thresholded_dataset_grp,\n                    values=[\"Relative Optical Density mean\", \"Z-Factor mean\"],\n                    index=[\"Internal ID\", \"Dataset\", \"Concentration\", \"Unit\"],\n                    columns=\"Organism formatted\",\n                ).reset_index()\n\n\n                # Sort rows by mean between the organisms (lowest mean measurement first)\n                results_sorted_by_mean_activity = pivot_multiindex_df.loc[\n                    pivot_multiindex_df.loc[\n                        :,\n                        list(\n                            filter(\n                                lambda x: x[0].startswith(\"Relative Optical Density\"),\n                                pivot_multiindex_df.columns,\n                            )\n                        ),\n                    ]\n                    .mean(axis=1)\n                    .argsort()\n                ]\n                # Only try to merge if precipitation results exist and the precipitation dataframe is present\n                if not self.precipitation.results.empty and self.substances_precipitation is not None:\n                    # If pivot_table produced MultiIndex columns, flatten them to single level so pandas.merge works\n                    if isinstance(results_sorted_by_mean_activity.columns, pd.MultiIndex):\n                        results_sorted_by_mean_activity.columns = [\n                            \" \".join(col).strip() if isinstance(col, tuple) else col\n                            for col in results_sorted_by_mean_activity.columns\n                        ]\n                    # Merge explicitly on Internal ID to avoid ambiguous/level-mismatch merges\n                    results_sorted_by_mean_activity = pd.merge(\n                        results_sorted_by_mean_activity,\n                        self.substances_precipitation,\n                        how=\"left\",\n                        on=[\"Internal ID\", \"Concentration\"],\n                    )\n                if molecule_info_df is not None:\n                    results_sorted_by_mean_activity = pd.merge(\n                        results_sorted_by_mean_activity,\n                        molecule_info_df,\n                        how=\"left\",\n                        on=\"Internal ID\",\n                    )\n\n                # Correct \"mean\" header if its only one replicate (remove 'mean')\n                if sum(thresholded_dataset_grp[\"Replicate count\"].unique()) == 1:\n                    results_sorted_by_mean_activity = results_sorted_by_mean_activity.rename(\n                        columns={\n                            \"Relative Optical Density mean\": \"Relative Optical Density\",\n                            # \"b_scores mean\": \"B-Score\",\n                        }\n                    )\n\n                # results_sorted_by_mean_activity = (\n                #     results_sorted_by_mean_activity.rename(\n                #         columns={\"b_scores mean\": \"B-Score mean\"}\n                #     )\n                # )\n\n                results_sorted_by_mean_activity = (\n                    results_sorted_by_mean_activity.fillna(\"NA\")\n                )  # Fill NA for better excel readability\n\n                # Add Concentration Unit column if available\n                # unit_values = self._dilutions.get(\"Unit\")\n                # unit_val = unit_values.dropna().iloc[0] if unit_values is not None and not unit_values.dropna().empty else None\n                # if \"Concentration\" in results_sorted_by_mean_activity.columns:\n                #     concentration_idx = results_sorted_by_mean_activity.columns.get_loc(\"Concentration\")\n                #     if not isinstance(concentration_idx, (int, np.integer)):\n                #         raise ValueError(\n                #             \"Expected exactly one 'Concentration' column when building the results table.\"\n                #         )\n                #     concentration_idx = int(concentration_idx)\n                #     before_cols = list(results_sorted_by_mean_activity.columns[: concentration_idx + 1])\n                #     after_cols = list(results_sorted_by_mean_activity.columns[concentration_idx + 1 :])\n                #     results_sorted_by_mean_activity = results_sorted_by_mean_activity.reindex(columns=before_cols + [\"Concentration Unit\"] + after_cols)\n                #     results_sorted_by_mean_activity[\"Concentration Unit\"] = unit_val\n                # else:\n                #     results_sorted_by_mean_activity[\"Concentration Unit\"] = unit_val\n\n\n                result_tables.append(\n                    Result(\n                        dataset_name,\n                        f\"{dataset_name}_threshold{round(threshold)}_results\",\n                        table=results_sorted_by_mean_activity,\n                    )\n                )\n        return result_tables\n\n    @cached_property\n    def results(self):\n        \"\"\"\n        Retrieves result tables (from self._resulttables)\n        and returns them in a dictionary like:\n            {\"&lt;filepath&gt;\": pd.DataFrame}\n        \"\"\"\n        return {tbl.file_basename: tbl.table for tbl in self._resulttables}\n\n    def save_figures(self, resultpath, fileformats: list[str] = [\"svg\", \"html\"]):\n        _save_figures(resultpath, self._resultfigures, fileformats=fileformats)\n\n    def save_tables(\n        self, result_path, processed_path, fileformats: list[str] = [\"xlsx\", \"csv\"]\n    ):\n        pathlib.Path(processed_path).mkdir(parents=True, exist_ok=True)\n        self.processed.to_csv(os.path.join(processed_path, \"processed.csv\"))\n        _save_tables(result_path, self._resulttables, fileformats=fileformats)\n\n    def save_results(\n        self,\n        tables_path: str,\n        figures_path: str,\n        processed_path: str,\n        figureformats: list[str] = [\"svg\", \"html\"],\n        tableformats: list[str] = [\"xlsx\", \"csv\"],\n    ):\n        self.save_figures(figures_path, fileformats=figureformats)\n        self.save_tables(tables_path, processed_path, fileformats=tableformats)\n</code></pre>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen.mapped_input_df","title":"<code>mapped_input_df</code>  <code>cached</code> <code>property</code>","text":"<p>Does mapping of the inputfile describing the tested substances with the corresponding mappingfile(s). Basically replaces rda.process.primary_process_inputs() function so all the variables and intermediate results are available via the class</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen.results","title":"<code>results</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves result tables (from self._resulttables) and returns them in a dictionary like:</p>"},{"location":"reference/classes/#rda_toolbox.experiment_classes.PrimaryScreen.check_substances","title":"<code>check_substances()</code>","text":"<p>Do some sanity checks for the substances table. - Check if all necessary columns are present. - Check if substances contains missing values. - Check if there are duplicate Internal IDs (references excluded)</p> Source code in <code>rda_toolbox/experiment_classes.py</code> <pre><code>def check_substances(self):\n    \"\"\"\n    Do some sanity checks for the substances table.\n    - Check if all necessary columns are present.\n    - Check if substances contains missing values.\n    - Check if there are duplicate Internal IDs (references excluded)\n    \"\"\"\n</code></pre>"},{"location":"reference/parser/","title":"Parsing Functions","text":""},{"location":"reference/parser/#rda_toolbox.parser.collect_metadata","title":"<code>collect_metadata(filedicts)</code>","text":"<p>Helperfunction to collect the metadata from all reader files into a dataframe.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def collect_metadata(filedicts: list[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Helperfunction to collect the metadata from all reader files into a dataframe.\n    \"\"\"\n    allmetadata_df = pd.DataFrame()\n    for filedict in filedicts:\n        meta_df = pd.DataFrame(filedict[\"metadata\"], index=[0])\n        meta_df[\"Barcode\"] = filedict[\"Barcode\"]\n        allmetadata_df = pd.concat([allmetadata_df, meta_df], ignore_index=True)\n    return allmetadata_df\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.collect_results","title":"<code>collect_results(filedicts)</code>","text":"<p>Collect and merge results from the readerfiles.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def collect_results(filedicts: list[dict]) -&gt; pd.DataFrame:\n    \"\"\"\n    Collect and merge results from the readerfiles.\n    \"\"\"\n    allresults_df = pd.DataFrame(\n        {\"Row\": [], \"Column\": [], \"Raw Optical Density\": [], \"Overflow\": []}\n    )  # , \"Layout\": [], \"Concentration\": []})\n    platetype_s = list(set(fd[\"plate_type\"] for fd in filedicts))\n    if len(platetype_s) == 1:\n        platetype = platetype_s[0]\n    else:\n        raise Exception(f\"Different plate types used {platetype_s}\")\n\n    for filedict in filedicts:\n\n        long_rawdata_df = pd.melt(\n            filedict[\"Raw Optical Density\"].reset_index(names=\"Row\"),\n            id_vars=[\"Row\"],\n            var_name=\"Column\",\n            value_name=\"Raw Optical Density\",\n        )\n        long_overflow_df = pd.melt(\n            filedict[\"Overflow Raw Optical Density\"].reset_index(names=\"Row\"),\n            id_vars=[\"Row\"],\n            var_name=\"Column\",\n            value_name=\"Overflow\",\n        )\n        long_rawdata_df[\"Overflow\"] = long_overflow_df[\"Overflow\"].astype(bool).values\n\n        long_rawdata_df[\"Barcode\"] = filedict[\"Barcode\"]\n        # df_merged = reduce(\n        #     lambda  left,right: pd.merge(left,right,on=['Row', 'Column'], how='outer'),\n        #     [long_rawdata_df, long_layout_df, long_concentrations_df]\n        # )\n        allresults_df = pd.concat([allresults_df, long_rawdata_df], axis=0)\n        platetype = filedict[\"plate_type\"]\n\n    allresults_df.rename(\n        columns={\"Row\": f\"Row_{platetype}\", \"Column\": f\"Col_{platetype}\"}, inplace=True\n    )\n    return allresults_df.reset_index(drop=True)\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.filepaths_to_filedicts","title":"<code>filepaths_to_filedicts(filepaths)</code>","text":"<p>Wrapper function to obtain a list of dictionaries which contain the raw files information like</p> <ul> <li>different entries of metadata<ul> <li>Plate Type</li> <li>Barcode</li> <li>Date</li> <li>Time</li> <li>etc.</li> </ul> </li> <li>Raw Optical Density (DataFrame)</li> <li>Concentration (DataFrame)</li> <li>Layout (DataFrame)</li> </ul> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def filepaths_to_filedicts(filepaths: list[str]) -&gt; list[dict]:\n    \"\"\"\n    Wrapper function to obtain a list of dictionaries which contain the raw files information like\n\n    - different entries of metadata\n        - Plate Type\n        - Barcode\n        - Date\n        - Time\n        - etc.\n    - Raw Optical Density (DataFrame)\n    - Concentration (DataFrame)\n    - Layout (DataFrame)\n    \"\"\"\n    filedicts = []\n    for path in filepaths:\n        try:\n            with open(path, encoding=\"utf-8\", errors=\"ignore\") as fh:\n                filedicts.append(readerfile_parser(basename(path), fh))\n        except OSError as exc:\n            raise OSError(f\"Failed to read {path!r}: {exc}\") from exc\n    return filedicts\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.parse_mappingfile","title":"<code>parse_mappingfile(filepath, motherplate_column='Origin Plate', childplate_column='AcD Barcode 384')</code>","text":"<p>Simple mappingfile parser function. Expects to start with a \"Motherplate\" line followed by corresponding \"Childplates\" in a single line.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def parse_mappingfile(\n    filepath: str,\n    motherplate_column: str = \"Origin Plate\",\n    childplate_column: str = \"AcD Barcode 384\",\n):\n    \"\"\"\n    Simple mappingfile parser function.\n    Expects to start with a \"Motherplate\" line followed by corresponding \"Childplates\" in a single line.\n    \"\"\"\n    filedict = dict()\n    with open(filepath) as file:\n        filecontents = file.read().splitlines()\n        key = None\n        for i, line in enumerate(filecontents):\n            line = line.split(\";\")\n            if i % 2 == 0:  # if i is even (expect MPs on even lines, alternating with childplates)\n            # if len(line) == 1:\n                key = line[0]\n            else:\n                if not key:\n                    raise ValueError(\n                        \"Motherplate barcode expected on first line.\"\n                    )\n                if key in filedict:\n                    filedict[key].append(line)\n                else:\n                    filedict[key] = [line]\n    mapping_df = pd.DataFrame(\n        [\n            (motherplate, childplate, rep_num, rack_nr)\n            for motherplate, replicates in filedict.items()\n            for rep_num, childplates in enumerate(replicates, start=1)\n            for rack_nr, childplate in enumerate(childplates, start=1)\n        ],\n        columns=[motherplate_column, childplate_column, \"Replicate\", \"Rack\"],\n    )\n    return mapping_df\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.parse_readerfiles","title":"<code>parse_readerfiles(path)</code>","text":"<p>Reads CytationC10 readerfiles (plain text files) and merges the results into two DataFrames (rawdata and metadata) which is returned. Wrapper for readerfiles_rawdf to keep backwards compatibility. Improves readerfiles_rawdf, provide a single path for convenience.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def parse_readerfiles(path: str | None) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Reads CytationC10 readerfiles (plain text files) and merges the results into\n    two DataFrames (rawdata and metadata) which is returned.\n    Wrapper for readerfiles_rawdf to keep backwards compatibility.\n    Improves readerfiles_rawdf, provide a single path for convenience.\n    \"\"\"\n    if not path:\n        return pd.DataFrame(), pd.DataFrame()\n    paths = [\n            os.path.join(path, f)\n            for f in os.listdir(path)\n            if os.path.isfile(os.path.join(path, f))\n    ]\n    df_raw = readerfiles_rawdf(paths)\n    df_raw[\"Col_384\"] = df_raw[\"Col_384\"].astype(int)\n    df_meta = readerfiles_metadf(paths)\n    return df_raw, df_meta\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.process_inputfile","title":"<code>process_inputfile(file_object)</code>","text":"Read Input excel file which should have the following columns <ul> <li>Barcode</li> <li>Organism</li> <li>Row_384</li> <li>Col_384</li> <li>ID</li> </ul> <p>Optional columns:     - Concentration in mg/mL (or other units)     - Cutoff</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def process_inputfile(file_object):\n    \"\"\"\n    Read Input excel file which should have the following columns:\n        - Barcode\n        - Organism\n        - Row_384\n        - Col_384\n        - ID\n    Optional columns:\n        - Concentration in mg/mL (or other units)\n        - Cutoff\n    \"\"\"\n    if not file_object:\n        return None\n    excel_file = pd.ExcelFile(file_object)\n    substance_df = pd.read_excel(excel_file, \"substances\")\n    layout_df = pd.read_excel(excel_file, \"layout\")\n    df = pd.merge(layout_df, substance_df, how=\"cross\")\n    df[\"ID\"] = df[\"ID\"].astype(str)\n    return df\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.read_platemapping","title":"<code>read_platemapping(filecontents, orig_barcodes)</code>","text":"<p>Reads a mappingfile generated by the barcode reader. We expect the mapping files to ALWAYS have a line with a single motherplate barcode followed by a single line with childplate barcode(s).</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def read_platemapping(filecontents: list, orig_barcodes: list[str]):\n    \"\"\"\n    Reads a mappingfile generated by the barcode reader.\n    We expect the mapping files to ALWAYS have a line with a single motherplate barcode followed by a single line with childplate barcode(s).\n    \"\"\"\n    filedict = dict()\n    origin_barcode: str | None = None\n    origin_replicates = []\n    for line_num, line in enumerate(filter(None, filecontents)):\n        line = line.split(\";\")\n        if line_num % 2 == 0 and len(line) == 1 and line[0] in orig_barcodes:  # singular entry indicates origin barcode\n            origin_barcode = line[0]\n            origin_replicates.append(origin_barcode)\n            if origin_barcode not in filedict:\n                filedict.setdefault(origin_barcode, [])\n        if origin_barcode is None:\n            continue\n        if line_num % 2 == 1: # childplates are on odd line numbers\n            filedict[origin_barcode].append(line)\n            origin_barcode = None\n    replicates_dict = {i:origin_replicates.count(i) for i in origin_replicates}\n    if sorted(list(filedict.keys())) != sorted(orig_barcodes):\n        raise ValueError(\n            f\"The origin barcodes from the mappingfile and MP barcodes in MIC_input.xlsx do not coincide.\"\n        )\n    return filedict, replicates_dict\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.readerfile_parser","title":"<code>readerfile_parser(filename, file_object, resulttable_header='Results')</code>","text":"<p>Parser for files created by the BioTek Cytation C10 Confocal Imaging Reader.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def readerfile_parser(\n    filename: str, file_object: IO[str], resulttable_header: str = \"Results\"\n) -&gt; dict:\n    \"\"\"\n    Parser for files created by the BioTek Cytation C10 Confocal Imaging Reader.\n    \"\"\"\n    lines = file_object.readlines()\n    lines = list(filter(None, map(lambda x: x.strip(\"\\n\").strip(\"\\r\"), lines)))\n    if len(lines) == 0:\n        raise ValueError(f\"Empty raw file {filename}.\")\n\n    # search the file for plate type definition and use it to derive number of rows and columns\n    found_plate_type = re.findall(r\"Plate Type;[A-z ]*([0-9]*)\", \"\".join(lines))\n    plate_type = 96  # define default plate type and let it be 96-well plate as this is what we started with\n    if found_plate_type:\n        plate_type = int(found_plate_type[0])\n\n    num_rows, num_columns = get_rows_cols(plate_type)\n\n    filedict = dict()\n    metadata = dict()\n    filedict[\"Reader Filename\"] = filename\n    filedict[\"plate_type\"] = plate_type\n    # TODO: get barcode via regex\n    barcode_found = re.findall(\n        r\"\\d{3}[A-Z][a-z]?[a-zA-Z]\\d{2}\\d{3}\", filedict[\"Reader Filename\"]\n    )\n    if not barcode_found:\n        filedict[\"Barcode\"] = filedict[\"Reader Filename\"]\n    else:\n        filedict[\"Barcode\"] = barcode_found[0]\n    # filedict[\"Barcode\"] = Path(filedict[\"Reader Filename\"]).stem.split(\"_\")[-1]\n\n    overflow_events = []\n    results = np.empty([num_rows, num_columns], dtype=float)\n    overflow_results = np.zeros([num_rows, num_columns], dtype=bool)\n    # using dtype=str results in unicode strings of length 1 ('U1'), therefore we use 'U25'\n    layout = np.empty([num_rows, num_columns], dtype=\"U25\")\n    concentrations = np.empty([num_rows, num_columns], dtype=float)\n\n    metadata_regex = r\";?([a-zA-Z0-9 \\/]*)[;:]+([a-zA-Z0-9 \\/\\\\:_.-]*),?\"\n    line_num = 0\n    while line_num &lt; len(lines):\n        if lines[line_num] == resulttable_header:\n            line_num += 1\n            header = list(\n                map(int, lines[line_num].strip(\"\\n\").split(\";\")[1:])\n            )  # get the header as a concrete list\n            index = [\"\"] * num_rows\n            for _row_num in range(num_rows):  # for the next num_rows, read result data\n                line_num += 1\n                res_line = lines[line_num].split(\";\")\n                # Split at ; and slice off rowlabel and excitation/emission value:\n                row_name = res_line[0]\n                index[_row_num] = row_name\n                parsed_row = []\n                overflow_row = []\n                for _col_idx, token in enumerate(res_line[1:-1]):\n                    col_value = header[_col_idx] if _col_idx &lt; len(header) else None\n                    parsed_value = _safe_float(\n                        token,\n                        filename,\n                        overflow_events=overflow_events,\n                        table=\"Raw Optical Density\",\n                        row=row_name,\n                        col=col_value,\n                    )\n                    parsed_row.append(parsed_value)\n                    overflow_row.append(token.strip().upper() == \"OVRFLW\")\n                results[_row_num] = parsed_row\n                overflow_results[_row_num] = overflow_row\n            # Initialize DataFrame from results and add it to filedict\n            filedict[\"Raw Optical Density\"] = pd.DataFrame(\n                data=results, index=index, columns=header\n            )\n            filedict[\"Overflow Raw Optical Density\"] = pd.DataFrame(\n                data=overflow_results, index=index, columns=header\n            )\n            line_num += 1\n        elif lines[line_num] == \"Layout\":  # For the next num_rows, read layout data\n            line_num += 1\n            header = list(\n                map(int, lines[line_num].strip(\"\\n\").split(\";\")[1:])\n            )  # Because we use header twice here, we collect it via list()\n            index = [\"\"] * num_rows\n            for _row_num in range(num_rows):\n                line_num += 1\n                layout_line = lines[line_num].split(\";\")\n                index[_row_num] = layout_line[0]\n                layout[_row_num] = layout_line[1:-1]\n                # Each second line yields a concentration layout line\n                line_num += 1\n                conc_line = lines[line_num].split(\";\")\n                concentrations[_row_num] = [\n                    _safe_float(\n                        x,\n                        filename,\n                        overflow_events=overflow_events,\n                        table=\"Concentration\",\n                        row=index[_row_num],\n                        col=header[_col_idx] if _col_idx &lt; len(header) else None,\n                    )\n                    for _col_idx, x in enumerate(conc_line[1:-1])\n                ]\n            # Add layouts to filedict\n            filedict[\"Layout\"] = pd.DataFrame(data=layout, index=index, columns=header)\n            filedict[\"Concentration\"] = pd.DataFrame(\n                data=concentrations, index=index, columns=header\n            )\n            line_num += 1\n        else:\n            metadata_pairs = re.findall(metadata_regex, lines[line_num])\n            line_num += 1\n            if not metadata_pairs:\n                continue\n            else:\n                for key, value in metadata_pairs:\n                    if not all(\n                        [key, value]\n                    ):  # if any of the keys or values are empty, skip\n                        continue\n                    else:\n                        metadata[key.strip(\" :\")] = value.strip(\" \")\n    filedict[\"metadata\"] = metadata\n    filedict[\"overflow_events\"] = overflow_events\n    return filedict\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.readerfiles_metadf","title":"<code>readerfiles_metadf(paths)</code>","text":"<p>Parses metadata from files declared by filepaths and merges the results into a DataFrame.</p> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def readerfiles_metadf(paths: list[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Parses metadata from files declared by filepaths and merges the results into a DataFrame.\n    \"\"\"\n    filedicts = filepaths_to_filedicts(paths)\n    return collect_metadata(filedicts)\n</code></pre>"},{"location":"reference/parser/#rda_toolbox.parser.readerfiles_rawdf","title":"<code>readerfiles_rawdf(paths)</code>","text":"<p>Parses data from files declared by filepaths and merges the results into a DataFrame :param paths: A list of filepaths corresponding to the raw reader files generated by Cytation10 :type paths: list[str] :return: A DataFrame in tidy and long format with the raw readerfile contents :rtype: pd.DataFrame</p> <p>:Example:</p> <pre><code>```Python\nimport glob\n\nrawdata_df = readerfiles_rawdf(glob.glob(\"path/to/raw/files/*\"))\n```\n</code></pre> Source code in <code>rda_toolbox/parser.py</code> <pre><code>def readerfiles_rawdf(paths: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Parses data from files declared by filepaths and merges the results into a DataFrame\n    :param paths: A list of filepaths corresponding to the raw reader files generated by Cytation10\n    :type paths: list[str]\n    :return: A DataFrame in tidy and long format with the raw readerfile contents\n    :rtype: pd.DataFrame\n\n    :Example:\n\n        ```Python\n        import glob\n\n        rawdata_df = readerfiles_rawdf(glob.glob(\"path/to/raw/files/*\"))\n        ```\n    \"\"\"\n    filedicts = filepaths_to_filedicts(paths)\n    rawdata = collect_results(filedicts)\n    overflow_count = int(rawdata[\"Overflow\"].sum()) if \"Overflow\" in rawdata.columns else 0\n    if overflow_count &gt; 0:\n        affected_files = {\n            event[\"Reader Filename\"]\n            for filedict in filedicts\n            for event in filedict.get(\"overflow_events\", [])\n        }\n        warnings.warn(\n            f\"Detected {overflow_count} OVRFLW value(s) in reader files \"\n            f\"({\", \".join(affected_files)})\"\n            f\"({len(affected_files)} file(s)). Values were set to NaN. \"\n            \"Inspect rows where rawdata['Overflow'] is True for details.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    rawdata[\"Col_384\"] = rawdata[\"Col_384\"].astype(str)\n    rawdata.rename(columns={\"Barcode\": \"AcD Barcode 384\"}, inplace=True)\n    return rawdata\n</code></pre>"},{"location":"reference/plot/","title":"Plotting Functions","text":""},{"location":"reference/plot/#rda_toolbox.plot.UpSetAltair","title":"<code>UpSetAltair(data=None, title='', subtitle='', sets=None, abbre=None, sort_by='frequency', sort_by_order='ascending', inter_degree_frequency='ascending', width=1200, height=700, height_ratio=0.6, horizontal_bar_chart_width=300, set_colors_dict=None, highlight_color='#777777', glyph_size=200, set_label_bg_size=1000, line_connection_size=2, horizontal_bar_size=20, vertical_bar_label_size=16, vertical_bar_padding=20, set_labelstyle='normal')</code>","text":"<p>This function generates Altair-based interactive UpSet plots.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Tabular data containing the membership of each element (row) in exclusive intersecting sets (column).</p> <code>None</code> <code>sets</code> <code>list</code> <p>List of set names of interest to show in the UpSet plots. This list reflects the order of sets to be shown in the plots as well.</p> <code>None</code> <code>abbre</code> <code>dict</code> <p>Dictionary mapping set names to abbreviated set names.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>\"frequency\" or \"degree\"</p> <code>'frequency'</code> <code>sort_by_order</code> <code>str</code> <p>\"ascending\" or \"descending\"</p> <code>'ascending'</code> <code>inter_degree_frequency</code> <code>str</code> <p>\"ascending\" or \"descending\", only makes sense if sort_by=\"degree\"</p> <code>'ascending'</code> <code>width</code> <code>int</code> <p>Vertical size of the UpSet plot.</p> <code>1200</code> <code>height</code> <code>int</code> <p>Horizontal size of the UpSet plot.</p> <code>700</code> <code>height_ratio</code> <code>float</code> <p>Ratio of height between upper and under views, ranges from 0 to 1.</p> <code>0.6</code> <code>horizontal_bar_chart_width</code> <code>int</code> <p>Width of horizontal bar chart on the bottom-right.</p> <code>300</code> <code>set_colors_dict</code> <code>dict</code> <p>Dictionary containing the sets as keys with corresponding colors as values</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color to encode intersecting sets upon mouse hover.</p> <code>'#777777'</code> <code>glyph_size</code> <code>int</code> <p>Size of UpSet glyph (\u2b24).</p> <code>200</code> <code>set_label_bg_size</code> <code>int</code> <p>Size of label background in the horizontal bar chart.</p> <code>1000</code> <code>line_connection_size</code> <code>int</code> <p>width of lines in matrix view.</p> <code>2</code> <code>horizontal_bar_size</code> <code>int</code> <p>Height of bars in the horizontal bar chart.</p> <code>20</code> <code>vertical_bar_label_size</code> <code>int</code> <p>Font size of texts in the vertical bar chart on the top.</p> <code>16</code> <code>vertical_bar_padding</code> <code>int</code> <p>Gap between a pair of bars in the vertical bar charts.</p> <code>20</code> <code>set_labelstyle</code> <code>str</code> <p>\"normal\" (default) or \"italic\"</p> <code>'normal'</code> <p>Run rda.utility.get_upsetplot_df() on the df before trying this function.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def UpSetAltair(\n    data: pd.DataFrame | None = None,\n    title: str = \"\",\n    subtitle: str = \"\",\n    sets: Sequence[str] | None = None,\n    abbre: Mapping[str, str] | None = None,\n    sort_by: str = \"frequency\",\n    sort_by_order: str = \"ascending\",\n    inter_degree_frequency: str = \"ascending\",\n    width: int = 1200,\n    height: int = 700,\n    height_ratio: float = 0.6,\n    horizontal_bar_chart_width: int = 300,\n    set_colors_dict: dict[str, str] | None = None,\n    highlight_color: str = \"#777777\",\n    glyph_size: int = 200,\n    set_label_bg_size: int = 1000,\n    line_connection_size: int = 2,\n    horizontal_bar_size: int = 20,\n    vertical_bar_label_size: int = 16,\n    vertical_bar_padding: int = 20,\n    set_labelstyle: str = \"normal\",\n) -&gt; ChartLike | None:\n    \"\"\"This function generates Altair-based interactive UpSet plots.\n\n    Parameters:\n          data (pandas.DataFrame): Tabular data containing the membership of each element (row) in exclusive intersecting sets (column).\n          sets (list): List of set names of interest to show in the UpSet plots. This list reflects the order of sets to be shown in the plots as well.\n          abbre (dict): Dictionary mapping set names to abbreviated set names.\n          sort_by (str): \"frequency\" or \"degree\"\n          sort_by_order (str): \"ascending\" or \"descending\"\n          inter_degree_frequency (str): \"ascending\" or \"descending\", only makes sense if sort_by=\"degree\"\n          width (int): Vertical size of the UpSet plot.\n          height (int): Horizontal size of the UpSet plot.\n          height_ratio (float): Ratio of height between upper and under views, ranges from 0 to 1.\n          horizontal_bar_chart_width (int): Width of horizontal bar chart on the bottom-right.\n          set_colors_dict (dict): Dictionary containing the sets as keys with corresponding colors as values\n          highlight_color (str): Color to encode intersecting sets upon mouse hover.\n          glyph_size (int): Size of UpSet glyph (\u2b24).\n          set_label_bg_size (int): Size of label background in the horizontal bar chart.\n          line_connection_size (int): width of lines in matrix view.\n          horizontal_bar_size (int): Height of bars in the horizontal bar chart.\n          vertical_bar_label_size (int): Font size of texts in the vertical bar chart on the top.\n          vertical_bar_padding (int): Gap between a pair of bars in the vertical bar charts.\n          set_labelstyle (str): \"normal\" (default) or \"italic\"\n\n    Run rda.utility.get_upsetplot_df() on the df before trying this function.\n    \"\"\"\n\n    if data is None:\n        print(\"No data and/or a list of sets are provided\")\n        return None\n    if sets is None:\n        sets = list(data.columns[1:])\n\n    if (height_ratio &lt; 0) or (1 &lt; height_ratio):\n        print(\"height_ratio set to 0.5\")\n        height_ratio = 0.5\n    if not abbre:\n        abbre = {set_name: set_name for set_name in sets}\n    if len(sets) != len(abbre):\n        abbre = {set_name: set_name for set_name in sets}\n        print(\n            \"Dropping the `abbre` list because the lengths of `sets` and `abbre` are not identical.\"\n        )\n    if not set_colors_dict:  # build default colors dict\n        colors = [  # observable10\n            \"#4269d0\",\n            \"#efb118\",\n            \"#ff725c\",\n            \"#6cc5b0\",\n            \"#3ca951\",\n            \"#ff8ab7\",\n            \"#a463f2\",\n            \"#97bbf5\",\n            \"#9c6b4e\",\n            \"#9498a0\",\n        ]\n        if len(sets) &gt; len(colors):\n            colors = colors * len(sets)\n        set_colors_dict = {key: value for key, value in zip(sets, colors[: len(sets)])}\n    else:\n        if sorted(list(set_colors_dict.keys())) != sorted(sets):\n            raise ValueError(\n                f\"Wrong set names, correct names are:\\n{dict((set, '') for set in sets)}\"\n            )\n    # filter set_colors_dict with the sets which are actually in the data df (sets)\n    # this might be needed if set_colors_dict if more comprehensive than the data\n    set_colors_dict = {\n        key: value for key, value in set_colors_dict.items() if key in sets\n    }\n    \"\"\"\n    Data Preprocessing\n    \"\"\"\n    data = data.copy()\n    data[\"count\"] = 0\n    data = data[list(sets) + [\"count\"]]\n    data = data.groupby(list(sets)).count().reset_index()\n\n    data[\"intersection_id\"] = data.index\n    data[\"degree\"] = data[sets].sum(axis=1)\n    data = data.sort_values(\n        by=[\"count\"],\n        ascending=True if inter_degree_frequency == \"ascending\" else False,\n    )\n\n    data = pd.melt(data, id_vars=[\"intersection_id\", \"count\", \"degree\"])\n    data = data.rename(columns={\"variable\": \"set\", \"value\": \"is_intersect\"})\n\n    set_to_abbre = pd.DataFrame(abbre.items(), columns=[\"set\", \"set_abbre\"])\n\n    set_to_order = (\n        data[data[\"is_intersect\"] == 1]\n        .groupby(\"set\")\n        .sum()\n        .reset_index()\n        .sort_values(by=\"count\", ascending=False)\n        .filter([\"set\"])\n    )\n    set_to_order[\"set_order\"] = list(range(len(sets)))\n\n    degree_calculation = \"\"\n    for s in sets:\n        degree_calculation += f\"(isDefined(datum['{s}']) ? datum['{s}'] : 0)\"\n        if sets[-1] != s:\n            degree_calculation += \"+\"\n    \"\"\"\n    Selections\n    \"\"\"\n    legend_selection = alt.selection_point(fields=[\"set\"], bind=\"legend\")\n    color_selection = alt.selection_point(\n        fields=[\"intersection_id\"], on=\"pointerover\", empty=False\n    )\n    opacity_selection = alt.selection_point(fields=[\"intersection_id\"])\n\n    \"\"\"\n    Styles\n    \"\"\"\n    vertical_bar_chart_height = height * height_ratio\n    matrix_height = height - vertical_bar_chart_height\n    matrix_width = width - horizontal_bar_chart_width\n\n    vertical_bar_size = min(\n        30,\n        width / len(data[\"intersection_id\"].unique().tolist()) - vertical_bar_padding,\n    )\n\n    main_color = \"#3A3A3A\"\n    brush_opacity = alt.condition(~opacity_selection, alt.value(1), alt.value(0.6))\n    brush_color = alt.condition(\n        color_selection, alt.value(highlight_color), alt.value(main_color)\n    )\n    is_show_horizontal_bar_label_bg = len(list(abbre.values())[0]) &lt;= 2\n    horizontal_bar_label_bg_color = (\n        \"white\" if is_show_horizontal_bar_label_bg else \"black\"\n    )\n\n    x_sort = alt.Sort(\n        field=\"count\" if sort_by == \"frequency\" else \"degree\",\n        order=sort_by_order,\n    )\n\n    tooltip = [\n        alt.Tooltip(\"max(count):Q\", title=\"Cardinality\"),\n        alt.Tooltip(\"degree:Q\", title=\"Degree\"),\n    ]\n    \"\"\"\n    Plots\n    \"\"\"\n    # To use native interactivity in Altair, we are using the data transformation functions\n    # supported in Altair.\n    base = (\n        alt.Chart(data)\n        .transform_pivot(\n            \"set\",\n            op=\"max\",\n            groupby=[\"intersection_id\", \"count\"],\n            value=\"is_intersect\",\n        )\n        .transform_aggregate(\n            # count, set1, set2, ...\n            count=\"sum(count)\",\n            groupby=list(sets),\n        )\n        .transform_calculate(\n            # count, set1, set2, ...\n            degree=degree_calculation\n        )\n        .transform_filter(\n            # count, set1, set2, ..., degree\n            alt.datum[\"degree\"]\n            != 0\n        )\n        .transform_window(\n            # count, set1, set2, ..., degree\n            intersection_id=\"row_number()\",\n            frame=[None, None],\n        )\n        .transform_fold(\n            # count, set1, set2, ..., degree, intersection_id\n            list(sets),\n            as_=[\"set\", \"is_intersect\"],\n        )\n        .transform_lookup(\n            # count, set, is_intersect, degree, intersection_id\n            lookup=\"set\",\n            from_=alt.LookupData(set_to_abbre, \"set\", [\"set_abbre\"]),\n        )\n        .transform_lookup(\n            # count, set, is_intersect, degree, intersection_id, set_abbre\n            lookup=\"set\",\n            from_=alt.LookupData(set_to_order, \"set\", [\"set_order\"]),\n        )\n        .transform_filter(\n            # Make sure to remove the filtered sets.\n            legend_selection\n        )\n        .transform_window(\n            # count, set, is_intersect, degree, intersection_id, set_abbre\n            set_order=\"distinct(set)\",\n            frame=[None, 0],\n            sort=[{\"field\": \"set_order\"}],\n        )\n        .transform_lookup(\n            lookup=\"set\",\n            from_=alt.LookupData(set_to_order, \"set\", [\"set_order\"]),\n        )\n    )\n\n    vertical_bar = (\n        base.mark_bar(color=main_color)  # , size=vertical_bar_size)\n        .encode(\n            x=alt.X(\n                \"intersection_id:N\",\n                axis=alt.Axis(grid=False, labels=False, ticks=False, domain=True),\n                sort=x_sort,\n                title=None,\n            ),\n            y=alt.Y(\n                \"max(count):Q\",\n                axis=alt.Axis(grid=False, tickCount=3, orient=\"right\"),\n                title=\"Intersection Size\",\n            ),\n            color=brush_color,\n            tooltip=tooltip,\n        )\n        .properties(width=matrix_width, height=vertical_bar_chart_height)\n    )\n    vertical_bar_text = vertical_bar.mark_text(\n        color=main_color, dy=-10, size=vertical_bar_label_size, fontSize=20\n    ).encode(text=alt.Text(\"count:Q\", format=\".0f\"))\n    vertical_bar_chart = (vertical_bar + vertical_bar_text).add_params(\n        color_selection,\n    )\n\n    circle_bg = (\n        vertical_bar.mark_circle(size=glyph_size, opacity=1)\n        .encode(\n            x=alt.X(\n                \"intersection_id:N\",\n                axis=alt.Axis(grid=False, labels=False, ticks=False, domain=False),\n                sort=x_sort,\n                title=None,\n            ),\n            y=alt.Y(\n                \"set_order:N\",\n                axis=alt.Axis(grid=False, labels=False, ticks=False, domain=False),\n                title=None,\n            ),\n            color=alt.value(\"#E6E6E6\"),\n        )\n        .properties(height=matrix_height)\n    )\n    rect_bg = (\n        circle_bg.mark_rect()\n        .transform_filter(alt.datum[\"set_order\"] % 2 == 1)\n        .encode(color=alt.value(\"#F7F7F7\"))\n    )\n    circle = circle_bg.transform_filter(alt.datum[\"is_intersect\"] == 1).encode(\n        color=brush_color\n    )\n    line_connection = (\n        circle_bg.mark_bar(size=line_connection_size, color=main_color)\n        .transform_filter(alt.datum[\"is_intersect\"] == 1)\n        .encode(\n            y=alt.Y(\"min(set_order):N\"),\n            y2=alt.Y2(\"max(set_order):N\"),\n            color=brush_color,\n        )\n    )\n    matrix_view = alt.layer(\n        circle + rect_bg + circle_bg + line_connection + circle\n    ).add_params(\n        # Duplicate `circle` is to properly show tooltips.\n        color_selection,\n    )\n\n    # Cardinality by sets (horizontal bar chart)\n    horizontal_bar_label_bg = base.mark_circle(size=set_label_bg_size).encode(\n        y=alt.Y(\n            \"set_order:N\",\n            axis=alt.Axis(grid=False, labels=False, ticks=False, domain=False),\n            title=None,\n        ),\n        color=alt.Color(\n            \"set:N\",\n            scale=alt.Scale(\n                domain=list(set_colors_dict.keys()),\n                range=list(set_colors_dict.values()),\n            ),\n            title=None,\n        ),\n        opacity=alt.value(1),\n    )\n    horizontal_bar_label = horizontal_bar_label_bg.mark_text(\n        align=(\"center\" if is_show_horizontal_bar_label_bg else \"center\"),\n        fontSize=20,\n        fontStyle=set_labelstyle,\n    ).encode(\n        text=alt.Text(\"set_abbre:N\"),\n        color=alt.value(horizontal_bar_label_bg_color),\n    )\n    horizontal_bar_axis = (\n        (horizontal_bar_label_bg + horizontal_bar_label)\n        if is_show_horizontal_bar_label_bg\n        else horizontal_bar_label\n    )\n\n    horizontal_bar = (\n        horizontal_bar_label_bg.mark_bar(size=horizontal_bar_size)\n        .transform_filter(alt.datum[\"is_intersect\"] == 1)\n        .encode(\n            x=alt.X(\n                \"sum(count):Q\",\n                axis=alt.Axis(grid=False, tickCount=3),\n                title=\"Set Size\",\n                # scale=alt.Scale(range=color_range)\n            ),\n            # color=alt.Color(None,legend=None), # remove interactivity, color and legend\n        )\n        .properties(width=horizontal_bar_chart_width)\n    )\n    horizontal_bar_text = horizontal_bar.mark_text(\n        align=\"left\", dx=2, fontSize=20\n    ).encode(text=\"sum(count):Q\")\n    horizontal_bar_chart = alt.layer(horizontal_bar, horizontal_bar_text)\n    # Concat Plots\n    upsetaltair = alt.vconcat(\n        vertical_bar_chart,\n        alt.hconcat(\n            matrix_view,\n            horizontal_bar_axis,\n            horizontal_bar_chart,\n            spacing=5,\n        ).resolve_scale(y=\"shared\"),\n        spacing=20,\n    ).add_params(\n        legend_selection,\n    )\n\n    # Apply top-level configuration\n    upsetaltair = upsetaltair_top_level_configuration(\n        upsetaltair,\n        legend_orient=\"top\",\n        legend_symbol_size=int(set_label_bg_size / 2.0),\n    ).properties(\n        title={\n            \"text\": title,\n            \"subtitle\": subtitle,\n            \"fontSize\": 20,\n            \"fontWeight\": 500,\n            \"subtitleColor\": main_color,\n            \"subtitleFontSize\": 14,\n        }\n    )\n    return upsetaltair\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.UpSet_per_dataset","title":"<code>UpSet_per_dataset(df, save_formats=('pdf', 'svg'), id_column='Internal ID')</code>","text":"<p>UpsetPlot wrapper function which applies threshold to processed data (without controls, references etc.). For each dataset present in the given df, create a dummy_df for rda.UpSetAltair() and save the UpSetPlot.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def UpSet_per_dataset(\n    df: pd.DataFrame,\n    save_formats: Sequence[str] = (\"pdf\", \"svg\"),\n    id_column: str = \"Internal ID\",\n) -&gt; None:\n    \"\"\"\n    UpsetPlot wrapper function which applies threshold to processed data (without controls, references etc.).\n    For each dataset present in the given df, create a dummy_df for rda.UpSetAltair() and save the UpSetPlot.\n    \"\"\"\n    subset = get_thresholded_subset(\n        df,\n        id_column=\"Internal ID\",\n        negative_controls=\"Bacteria + Medium\",\n        blanks=\"Medium\",\n        threshold=50,\n    )\n\n    for dataset, sub_df in subset.groupby(\"Dataset\"):\n        dataset_name = str(dataset)\n        dummy_df = get_upsetplot_df(sub_df, counts_column=id_column)\n        # Create dataset folder if non-existent\n        pathlib.Path(f\"../figures/{dataset_name}\").mkdir(parents=True, exist_ok=True)\n        for save_format in save_formats:\n            filename = f\"../figures/{dataset_name}/UpSetPlot_{dataset_name}.{save_format}\"\n            print(\"Saving\", filename)\n            dataset_upsetplot = UpSetAltair(dummy_df, title=dataset_name)\n            if dataset_upsetplot is not None:\n                dataset_upsetplot.save(filename)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.lineplots_facet","title":"<code>lineplots_facet(df, hline_y=50, by_id='Internal ID', whisker_width=10, exclude_negative_zfactors=True, threshold=50.0)</code>","text":"<p>Assay: MIC Input: processed_df Output: Altair Chart with faceted lineplots. Negative controls and blanks are dropped inside the function.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def lineplots_facet(\n    df: pd.DataFrame,\n    hline_y: int = 50,\n    by_id: str = \"Internal ID\",\n    whisker_width: int = 10,\n    exclude_negative_zfactors: bool = True,\n    threshold: float = 50.0,\n) -&gt; alt.HConcatChart:\n    \"\"\"\n    Assay: MIC\n    Input: processed_df\n    Output: Altair Chart with faceted lineplots.\n    Negative controls and blanks are dropped inside the function.\n    \"\"\"\n    df = prepare_visualization(\n        df, by_id=by_id, exclude_negative_zfactors=exclude_negative_zfactors, threshold=threshold\n    )\n    organism_columns = []\n\n    color = alt.condition(\n        alt.datum.max_conc_below_threshold,\n        alt.Color(f\"{by_id}:N\"),\n        alt.value(\"lightgray\"),\n    )\n    for organism, org_data in df.groupby([\"Organism\"]):\n        base = alt.Chart(org_data).encode(color=color)  # , title=organism)\n        lineplot = base.mark_line(point=True, size=0.8).encode(\n            x=alt.X(\n                \"Concentration:O\",\n                title=\"Concentration in \u00b5M\",\n                axis=alt.Axis(labelAngle=-45, format=\".2e\", formatType=\"number\"),\n            ),\n            y=alt.Y(\n                \"Mean Relative Optical Density:Q\",\n                title=\"Relative Optical Density\",\n                scale=alt.Scale(domain=[-20, 160], clamp=True),\n            ),\n            shape=alt.Shape(f\"{by_id}:N\", legend=None),\n            tooltip=[\n                \"Internal ID\",\n                \"External ID\",\n                \"Organism\",\n                \"Dataset\",\n                \"Concentration\",\n                \"Used Replicates\",\n                \"Raw Optical Density\",\n                \"Mean Relative Optical Density\",\n                r\"Std\\. Relative Optical Density\",\n                \"Z-Factor\",\n            ],\n        )\n\n        error_bars = base.mark_rule().encode(\n            x=\"Concentration:O\",\n            y=\"uerror:Q\",\n            y2=\"lerror:Q\",\n        )\n        uerror_whiskers = base.mark_tick(size=whisker_width).encode(\n            x=\"Concentration:O\",\n            y=\"uerror:Q\",\n        )\n        lerror_whiskers = base.mark_tick(size=whisker_width).encode(\n            x=\"Concentration:O\",\n            y=\"lerror:Q\",\n        )\n\n        hline = base.mark_rule(strokeDash=[3, 2]).encode(\n            y=alt.datum(hline_y),\n            # x=[alt.value(0), alt.value(50)],\n            color=alt.value(\"black\"),\n        )\n\n        org_column = (\n            alt.layer(lineplot, error_bars, uerror_whiskers, lerror_whiskers, hline)\n            .facet(\n                row=\"AsT Barcode 384\",\n                column=\"AsT Plate Subgroup\",\n                title=alt.Title(organism, anchor=\"middle\"),\n            )\n            .resolve_axis(x=\"independent\")\n            .resolve_scale(color=\"independent\", shape=\"independent\")\n            # .add_params(selection)\n        )\n\n        organism_columns.append(org_column)\n    return alt.hconcat(*organism_columns).configure_point(size=60)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.measurement_vs_bscore_scatter","title":"<code>measurement_vs_bscore_scatter(df, measurement_header='Relative Optical Density mean', measurement_title='Relative Optical Density', bscore_header='b_scores mean', bscore_title='B-Score', color_header='Organism', show_area=True, measurement_threshold=50, b_score_threshold=-3)</code>","text":"<p>Creates a scatter plot for Primary Screens plotting the raw measurement values against B-Scores. Dont forget to exclude controls from the given DF.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def measurement_vs_bscore_scatter(\n    df: pd.DataFrame,\n    measurement_header: str = \"Relative Optical Density mean\",\n    measurement_title: str = \"Relative Optical Density\",\n    bscore_header: str = \"b_scores mean\",\n    bscore_title: str = \"B-Score\",\n    color_header: str = \"Organism\",\n    show_area: bool = True,\n    measurement_threshold: float = 50,\n    b_score_threshold: float = -3,\n) -&gt; alt.LayerChart:\n    \"\"\"\n    Creates a scatter plot for Primary Screens plotting the raw measurement values against B-Scores.\n    Dont forget to exclude controls from the given DF.\n    \"\"\"\n    chart_df = df.copy()\n    # Add values for thresholds\n    chart_df[\"Growth Threshold\"] = measurement_threshold\n    chart_df[\"B-Score Threshold\"] = b_score_threshold\n    tooltip_fields: list = []\n    # for name in [\"Internal ID\", \"External ID\", \"Dataset\", \"Organism\"]:\n    #     if name in chart_df.columns:\n    #         tooltip_fields.append(alt.Tooltip(f\"{name}:N\"))\n    # if measurement_header in chart_df.columns:\n    #     tooltip_fields.append(alt.Tooltip(f\"{measurement_header}:Q\", title=measurement_title))\n    # if bscore_header in chart_df.columns:\n    #     tooltip_fields.append(alt.Tooltip(f\"{bscore_header}:Q\", title=bscore_title))\n    # if \"InChI\" in chart_df.columns:\n    #     tooltip_fields.append(alt.Tooltip(\"InChI:N\"))\n    # if \"InChI-Key\" in chart_df.columns:\n    #     tooltip_fields.append(alt.Tooltip(\"InChI-Key:N\"))\n    # if \"mol_img\" in chart_df.columns:\n    #     tooltip_fields.append(alt.Tooltip([\"mol_img:N\"], title=\"mol_img\"))\n\n    base = alt.Chart(chart_df, width=600)\n    chart = base.mark_circle().encode(\n        x=alt.X(f\"{bscore_header}:Q\", title=bscore_title),\n        y=alt.Y(\n            f\"{measurement_header}:Q\",\n            scale=alt.Scale(reverse=True),\n            title=measurement_title,\n        ),\n        color=f\"{color_header}:N\",\n        # tooltip=tooltip_fields,\n    )\n    growth_threshold_rule = base.mark_rule(color=\"blue\", strokeDash=[4.4]).encode(\n        y=\"Growth Threshold:Q\"\n    )\n    bscore_threshold_rule = base.mark_rule(color=\"red\", strokeDash=[4.4]).encode(\n        x=\"B-Score Threshold:Q\"\n    )\n\n    rect = base.mark_rect(color=\"blue\").encode(\n        y=f\"min({measurement_header}):Q\",\n        y2=\"Growth Threshold:Q\",\n        x=\"B-Score Threshold:Q\",\n        x2=f\"min({bscore_header}):Q\",\n        opacity=alt.value(0.2),\n    )\n\n    if show_area:\n        return alt.layer(chart, growth_threshold_rule, bscore_threshold_rule, rect)\n    else:\n        return alt.layer(chart, growth_threshold_rule, bscore_threshold_rule)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.mic_hitstogram","title":"<code>mic_hitstogram(data, mic_col, title='Count Distribution of Hits over Concentration')</code>","text":"<p>It's a Hi(t)stogram... Plots distribution of hits over determined MICs. Example: mic_distribution_overview(mic_results_long, 'MIC50 in \u00b5M')</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def mic_hitstogram(\n    data: pd.DataFrame, mic_col: str, title: str = \"Count Distribution of Hits over Concentration\"\n) -&gt; alt.LayerChart:\n    \"\"\"\n    It's a Hi(t)stogram...\n    Plots distribution of hits over determined MICs.\n    Example: mic_distribution_overview(mic_results_long, 'MIC50 in \u00b5M')\n    \"\"\"\n    data = data.dropna(subset=[mic_col])\n    bars = (\n        alt.Chart(data, title=alt.Title(title))\n        .mark_bar()\n        .encode(\n            x=alt.X(f\"{mic_col}:O\"),\n            y=alt.Y(\"count(Internal ID):Q\"),\n            xOffset=\"Organism:N\",\n            color=\"Organism:N\",\n        )\n    )\n    text = (\n        alt.Chart(data)\n        .mark_text(dx=0, dy=-5)\n        .encode(\n            x=alt.X(f\"{mic_col}:O\"),\n            y=alt.Y(\"count(Internal ID):Q\"),\n            text=alt.Text(\"count(Internal ID):Q\"),\n            xOffset=\"Organism:N\",\n            color=\"Organism:N\",\n        )\n    )\n\n    return alt.layer(bars, text)\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.plateheatmaps","title":"<code>plateheatmaps(df, substance_id='ID', measurement='Raw Optical Density', barcode='Barcode', negative_control='Negative Control', blank='Medium')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with relevant data</p> required <code>substance_id</code> <code>str</code> <p>column name in df containing the unique substance id</p> <code>'ID'</code> <code>measurement</code> <code>str</code> <p>column name in df with the measurements to colorize via heatmaps</p> <code>'Raw Optical Density'</code> <code>negative_control</code> <code>str</code> <p>controls with organism + medium</p> <code>'Negative Control'</code> <code>blank</code> <code>str</code> <p>controls with only medium (no organism and therefore no growth)</p> <code>'Medium'</code> <p>Plots heatmaps of the plates from df in a gridlike manner. Exclude unwanted plates, for example Blanks from the df outside this function, like so <code>df[df[\"Organism\"] != \"Blank\"]</code> before plotting, otherwise it will appear as an extra plate.</p> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def plateheatmaps(\n    df: pd.DataFrame,\n    substance_id: str = \"ID\",\n    measurement: str = \"Raw Optical Density\",\n    barcode: str = \"Barcode\",\n    negative_control: str = \"Negative Control\",\n    blank: str = \"Medium\",\n) -&gt; alt.HConcatChart:\n    \"\"\"\n    Parameters:\n        df (pandas.DataFrame): Dataframe with relevant data\n        substance_id (str): column name in df containing the unique substance id\n        measurement (str): column name in df with the measurements to colorize via heatmaps\n        negative_control (str): controls with organism + medium\n        blank (str): controls with only medium (no organism and therefore no growth)\n\n    Plots heatmaps of the plates from df in a gridlike manner.\n    Exclude unwanted plates, for example Blanks from the df outside this function, like so\n    `df[df[\"Organism\"] != \"Blank\"]`\n    before plotting, otherwise it will appear as an extra plate.\n    \"\"\"\n    df = df.copy()\n    df[\"Col_384\"] = df[\"Col_384\"].astype(int)\n    plots = []\n    for _, _organism_df in df.groupby(\"Organism\"):\n        plots.append(\n            get_heatmap(\n                _organism_df,\n                substance_id,\n                measurement,\n                negative_control,\n                blank,\n            )\n            .facet(\n                row=alt.Row(f\"{barcode}:N\"),\n                column=alt.Column(\"Replicate:N\"),\n                title=alt.Title(\n                    _organism_df[\"Organism\"].unique()[0],\n                    orient=\"top\",\n                    anchor=\"middle\",\n                    dx=-20,\n                ),\n            )\n            .resolve_scale(color=\"shared\")\n            .resolve_axis(x=\"independent\", y=\"independent\")\n        )\n\n    plate_heatmaps = (\n        alt.hconcat(*plots).resolve_scale(color=\"independent\").resolve_axis(y=\"shared\")\n    )\n    return plate_heatmaps\n</code></pre>"},{"location":"reference/plot/#rda_toolbox.plot.potency_distribution","title":"<code>potency_distribution(dataset_grp, threshold, dataset, intervals=(0.05, 0.1, 0.78, 6.25, 50), title='Potency Distribution', ylabel='Number of Compounds', xlabel='MIC Interval', legendlabelorient='bottom')</code>","text":"<p>Input: MIC.results[\"MIC_Results_AllDatasets_longformat\"]</p> <p>Returns a potency distribution (histogram if MIC intervals) plot.</p> <p>Example: Obtain a list of potency distribution plots. One plot per dataset and threshold. <pre><code>plots_per_dataset = []\nthresholds = [50.0]\nfor threshold in thresholds:\n    for dataset, dataset_grp in mic_df.groupby(\"Dataset\"):\n        plots_per_dataset.append(potency_distribution(dataset_grp, threshold, dataset))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dataset_grp</code> <code>DataFrame</code> <p>Group DataFrame from grouping via Datasets.</p> required <code>threshold</code> <code>float</code> <p>single threshold value (usually from a list of thresholds).</p> required <code>dataset</code> <code>str</code> <p>The name of the dataset.</p> required <code>intervals</code> <code>list[float]</code> <p>the upper limits for the interval bins. Interval example: (x, y] -&gt; open below x, &lt;= y</p> <code>(0.05, 0.1, 0.78, 6.25, 50)</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Potency Distribution'</code> <code>ylabel</code> <code>str</code> <p>Y-Axis label.</p> <code>'Number of Compounds'</code> <code>xlabel</code> <code>str</code> <p>X-Axis label.</p> <code>'MIC Interval'</code> <code>legendlabelorient</code> <code>str</code> <p>Position of the legend (options: \"left\", \"right\", \"top\", \"bottom\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\", \"none\" (Default))</p> <code>'bottom'</code> Source code in <code>rda_toolbox/plot.py</code> <pre><code>def potency_distribution(\n    dataset_grp: pd.DataFrame,\n    threshold: float,\n    dataset: str,\n    intervals: Sequence[float] = (0.05, 0.1, 0.78, 6.25, 50),\n    title: str = \"Potency Distribution\",\n    ylabel: str = \"Number of Compounds\",\n    xlabel: str = \"MIC Interval\",\n    legendlabelorient: str = \"bottom\",\n) -&gt; alt.LayerChart:\n    \"\"\"\n    Input: MIC.results[\"MIC_Results_AllDatasets_longformat\"]\n\n    Returns a potency distribution (histogram if MIC intervals) plot.\n\n    Example: Obtain a list of potency distribution plots. One plot per dataset and threshold.\n    ```\n    plots_per_dataset = []\n    thresholds = [50.0]\n    for threshold in thresholds:\n        for dataset, dataset_grp in mic_df.groupby(\"Dataset\"):\n            plots_per_dataset.append(potency_distribution(dataset_grp, threshold, dataset))\n    ```\n\n    Parameters:\n        dataset_grp (pd.DataFrame): Group DataFrame from grouping via Datasets.\n        threshold (float): single threshold value (usually from a list of thresholds).\n        dataset (str): The name of the dataset.\n        intervals (list[float]): the upper limits for the interval bins. Interval example: (x, y] -&gt; open below x, &lt;= y\n        title (str): Plot title.\n        ylabel (str): Y-Axis label.\n        xlabel (str): X-Axis label.\n        legendlabelorient (str): Position of the legend (options: \"left\", \"right\", \"top\", \"bottom\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\", \"none\" (Default))\n    \"\"\"\n    no_mic = (\n        dataset_grp[dataset_grp[f\"MIC{threshold} in \u00b5M\"].isna()][\"Organism\"]\n        .value_counts()\n        .reset_index(name=ylabel)\n    )\n    no_mic[xlabel] = f\"&gt;{max(intervals)}\"\n    sub_df = (\n        dataset_grp.groupby(\"Organism\")[f\"MIC{threshold} in \u00b5M\"]\n        .value_counts(bins=intervals, dropna=False)\n        .rename_axis([\"Organism\", xlabel])\n        .reset_index(name=ylabel)\n    )\n    sub_df[xlabel] = sub_df[xlabel].astype(str)\n    sub_df = pd.concat([no_mic, sub_df])\n    legendcolumns = None\n    if legendlabelorient == \"bottom\":\n        legendcolumns = 3\n    base = alt.Chart(sub_df, title=alt.Title(title, subtitle=[f\"Dataset: {dataset}\"]))\n    bar = base.mark_bar(stroke=\"white\").encode(\n        alt.X(f\"{xlabel}:N\").axis(labelAngle=0),\n        y=alt.Y(f\"{ylabel}:Q\").scale(domain=[0, max(sub_df[ylabel])+2]),\n        color=alt.Color(\"Organism:N\").legend(\n            orient=legendlabelorient,\n            labelLimit=200,\n            fillColor=\"white\",\n            columns=legendcolumns,\n        ),\n        xOffset=\"Organism:N\",\n    )\n    text = base.mark_text(dy=-5).encode(\n        alt.X(f\"{xlabel}:N\"),\n        y=f\"{ylabel}:Q\",\n        xOffset=\"Organism:N\",\n        text=f\"{ylabel}:Q\",\n    )\n    return alt.layer(bar, text)\n</code></pre>"},{"location":"reference/process/","title":"Processing Functions","text":""},{"location":"reference/process/#rda_toolbox.process.add_b_score","title":"<code>add_b_score(plate_df, measurement_header='Raw Optical Density', row_header='Row_384', col_header='Col_384')</code>","text":"<p>Expects a Dataframe comprising a whole plate (without controls!).</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def add_b_score(\n    plate_df: pd.DataFrame,\n    measurement_header: str = \"Raw Optical Density\",\n    row_header: str = \"Row_384\",\n    col_header: str = \"Col_384\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Expects a Dataframe comprising a **whole** plate (without controls!).\n    \"\"\"\n    # We could also collect iterations of the median polish function and plot the results to show progress of normalization\n    plate_df = median_polish_df(plate_df)\n    mad_value = median_absolute_deviation(plate_df[measurement_header], scale=1.4826)\n    if not np.isfinite(mad_value) or np.isclose(mad_value, 0.0):\n        raise ValueError(\"Median absolute deviation is zero; cannot compute B-scores.\")\n    plate_df[\"b_scores\"] = plate_df[measurement_header] / mad_value\n    return plate_df.drop(\n        columns=[\"row_effect\", \"col_effect\", \"row_median\", \"col_median\", measurement_header]\n        ).round({\"b_scores\": 2})\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.background_normalize_zfactor","title":"<code>background_normalize_zfactor(grp, substance_id, measurement, negative_controls, blanks, norm_by_barcode)</code>","text":"<p>This function is supposed to be applied to a grouped DataFrame. It does the following operations: - Background subtraction by subtracting the mean of the blanks per plate - Normalization by applying max-normalization using the 'Negative Controls' - Z-Factor calculation using negative controls and blanks</p> <p><code>negative_controls</code> are controls with organism (e.g. bacteria) and medium and are labeled in the input DataFrame as 'Negative Controls'. <code>blanks</code> are controls with only medium and are labeled in the input DataFrame as 'Medium'.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def background_normalize_zfactor(\n    grp: pd.DataFrame,\n    substance_id,\n    measurement,\n    negative_controls,\n    blanks,\n    norm_by_barcode,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    This function is supposed to be applied to a grouped DataFrame.\n    It does the following operations:\n    - Background subtraction by subtracting the mean of the blanks per plate\n    - Normalization by applying max-normalization using the 'Negative Controls'\n    - Z-Factor calculation using negative controls and blanks\n\n    *`negative_controls` are controls with organism (e.g. bacteria) and medium*\n    *and are labeled in the input DataFrame as 'Negative Controls'.*\n    *`blanks` are controls with only medium and are labeled*\n    *in the input DataFrame as 'Medium'.*\n    \"\"\"\n\n    # Work on a copy and ensure the measurement column is numeric so subtraction is supported\n    grp = grp.copy()\n    raw_col = f\"Raw {measurement}\"\n    if raw_col not in grp:\n        raise KeyError(f\"Column '{raw_col}' not found in input DataFrame.\")\n    grp[raw_col] = pd.to_numeric(grp[raw_col], errors=\"coerce\")\n    plate_neg_controls = grp[grp[substance_id] == negative_controls][raw_col]\n    plate_blank_controls = grp[grp[substance_id] == blanks][raw_col]\n\n    # Check inputs :)\n    if len(plate_neg_controls) == 0:\n        raise KeyError(\"Please check if keyword 'negative_controls' is matching with input table.\")\n    if len(plate_blank_controls) == 0:\n        raise KeyError(\"Please check if keyword 'blanks' is matching with input table.\")\n    if grp[raw_col].isna().all():\n        raise ValueError(\"Raw measurement column contains no numeric values.\")\n\n    plate_blanks_mean = plate_blank_controls.mean()\n    if not np.isfinite(plate_blanks_mean):\n        raise ValueError(\"Blank controls contain non-finite values.\")\n    # Subtract background noise:\n    grp[f\"Denoised {measurement}\"] = grp[f\"Raw {measurement}\"] - plate_blanks_mean\n    plate_denoised_negative_mean = grp[grp[substance_id] == negative_controls][\n        f\"Denoised {measurement}\"\n    ].mean()\n    plate_denoised_blank_mean = grp[grp[substance_id] == blanks][\n        f\"Denoised {measurement}\"\n    ].mean()\n    if not np.isfinite(plate_denoised_negative_mean) or np.isclose(\n        plate_denoised_negative_mean, 0.0\n    ):\n        plate_label = grp[norm_by_barcode].iat[0] if norm_by_barcode in grp else \"Unknown\"\n        raise ValueError(\n            f\"Plate {plate_label} cannot be normalized: negative controls after background subtraction have near-zero mean.\"\n        )\n    # Normalize:\n    grp[f\"Relative {measurement}\"] = (\n        grp[f\"Denoised {measurement}\"] / plate_denoised_negative_mean\n    ) * 100\n    # Z-Factor:\n    grp[\"Z-Factor\"] = zfactor(plate_neg_controls, plate_blank_controls)\n\n    # Robust Z-Factor using median instead of mean:\n    grp[\"Robust Z-Factor\"] = zfactor_median(plate_neg_controls, plate_blank_controls)\n\n    return grp\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.get_thresholded_subset","title":"<code>get_thresholded_subset(df, id_column='ID', negative_controls='Negative Control', blanks='Medium', blankplate_organism='Blank', threshold=None)</code>","text":"<p>Expects a DataFrame with a mic_cutoff column</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def get_thresholded_subset(\n    df: pd.DataFrame,\n    id_column=\"ID\",\n    negative_controls: str = \"Negative Control\",\n    blanks: str = \"Medium\",\n    blankplate_organism: str = \"Blank\",\n    threshold=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Expects a DataFrame with a mic_cutoff column\n    \"\"\"\n    # TODO: hardcode less columns\n\n    # Use only substance entries, no controls, no blanks etc.:\n    substance_df = df.loc[\n        (df[id_column] != blanks)\n        &amp; (df[id_column] != negative_controls)\n        &amp; (df[\"Organism\"] != blankplate_organism),\n        :,\n    ].copy()\n    # Apply threshold:\n    if threshold:\n        substance_df[\"Cutoff\"] = threshold\n    else:\n        if \"mic_cutoff\" not in substance_df:\n            raise KeyError(\"No 'mic_cutoff' column in Input.xlsx\")\n    selection = substance_df[\n        substance_df[\"Relative Optical Density\"] &lt; substance_df[\"Cutoff\"]\n    ]\n    # Apply mean and std in case of replicates:\n    result = selection.groupby([id_column, \"Organism\", \"Dataset\"], as_index=False).agg(\n        {\n            \"Relative Optical Density\": [\"mean\", \"std\"],\n            id_column: [\"first\", \"count\"],\n            \"Organism\": \"first\",\n            \"Cutoff\": \"first\",\n            \"Dataset\": \"first\",\n        }\n    )\n    result.columns = [\n        \"Relative Optical Density mean\",\n        \"Relative Optical Density std\",\n        id_column,\n        \"Replicates\",\n        \"Organism\",\n        \"Cutoff\",\n        \"Dataset\",\n    ]\n    return result\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.mic_results","title":"<code>mic_results(df, filepath, thresholds=[20, 50])</code>","text":"<p>Expects the results from rda.preprocess() function. Means measurements between replicates and obtains the MIC values per substance and organism. Saves excel files per dataset and sheets per organism with Minimum Inhibitory Concentrations (MICs) at the given thresholds.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def mic_results(df, filepath, thresholds=[20, 50]):\n    \"\"\"\n    Expects the results from rda.preprocess() function.\n    Means measurements between replicates and obtains the MIC values per substance and organism.\n    Saves excel files per dataset and sheets per organism with Minimum Inhibitory Concentrations (MICs)\n    at the given thresholds.\n    \"\"\"\n\n    df = df[(df[\"Dataset\"] != \"Negative Control\") &amp; (df[\"Dataset\"] != \"Blank\")].dropna(\n        subset=[\"Concentration\"]\n    )\n    # the above should remove entries where Concentration == NAN\n\n    # Pivot table to get the aggregated values:\n    pivot_df = pd.pivot_table(\n        df,\n        values=[\"Relative Optical Density\", \"Replicate\", \"Z-Factor\"],\n        index=[\n            \"Internal ID\",\n            \"External ID\",\n            \"Organism\",\n            \"Concentration\",\n            \"Dataset\",\n        ],\n        aggfunc={\n            \"Relative Optical Density\": [\"mean\"],\n            \"Replicate\": [\"count\"],\n            \"Z-Factor\": [\"mean\", \"std\"],  # does this make sense? with std its usable.\n            # \"Z-Factor\": [\"std\"],\n        },\n    ).reset_index()\n\n    # merge pandas hirarchical column index (wtf is this pandas!?)\n    pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n\n    mic_records = []\n    for group_names, grp in pivot_df.groupby(\n        [\"Internal ID\", \"External ID\", \"Organism\", \"Dataset\"]\n    ):\n        internal_id, external_id, organism, dataset = group_names\n        # Sort by concentration just to be sure:\n        grp = grp[\n            [\n                \"Concentration\",\n                \"Relative Optical Density mean\",\n                \"Z-Factor mean\",\n                \"Z-Factor std\",\n            ]\n        ].sort_values(by=[\"Concentration\"])  # This sorting is very important for the mic determination!\n        # Get rows where the OD is below the given threshold:\n        record = {\n            \"Internal ID\": internal_id,\n            \"External ID\": external_id,\n            \"Organism\": organism,\n            \"Dataset\": dataset,\n            \"Z-Factor mean\": list(grp[\"Z-Factor mean\"])[0],\n            \"Z-Factor std\": list(grp[\"Z-Factor std\"])[0],\n        }\n\n        for threshold in thresholds:\n            values_below_threshold = grp[\n                grp[\"Relative Optical Density mean\"] &lt; threshold\n            ]\n            # thx to jonathan - check if the OD at maximum concentration is below threshold (instead of any concentration)\n            max_conc_below_threshold = list(\n                grp[grp[\"Concentration\"] == max(grp[\"Concentration\"])][\n                    \"Relative Optical Density mean\"\n                ]\n                &lt; threshold\n            )[0]\n            if max_conc_below_threshold:\n                mic = values_below_threshold.iloc[0][\"Concentration\"] # the maximum concentration which is below the threshold\n            else:\n                mic = None\n            record[f\"MIC{threshold} in \u00b5M\"] = mic\n        mic_records.append(record)\n    # Drop entries where no MIC could be determined\n    mic_df = pd.DataFrame.from_records(mic_records)\n    # mic_df.dropna(\n    #     subset=[f\"MIC{threshold} in \u00b5M\" for threshold in thresholds],\n    #     how=\"all\",\n    #     inplace=True,\n    # )\n    mic_df.round(2).to_excel(\n        os.path.join(filepath, \"MIC_Results_AllDatasets_longformat.xlsx\"), index=False\n    )\n    for dataset, dataset_grp in mic_df.groupby([\"Dataset\"]):\n        pivot_multiindex_df = pd.pivot_table(\n            dataset_grp,\n            values=[f\"MIC{threshold} in \u00b5M\" for threshold in thresholds]\n            + [\"Z-Factor mean\", \"Z-Factor std\"],\n            index=[\"Internal ID\", \"External ID\", \"Dataset\"],\n            columns=\"Organism\",\n        ).reset_index()\n\n        resultpath = os.path.join(filepath, dataset[0])\n\n        # References special case\n        if dataset[0] == \"Reference\":\n            references_mic_results(df, resultpath, thresholds=thresholds)\n            continue  # skip for references\n\n        pathlib.Path(resultpath).mkdir(parents=True, exist_ok=True)\n        for threshold in thresholds:\n            organisms_thresholded_mics = pivot_multiindex_df[\n                [\"Internal ID\", \"External ID\", f\"MIC{threshold} in \u00b5M\"]\n            ]\n            cols = list(organisms_thresholded_mics.columns.droplevel())\n            cols[0] = \"Internal ID\"\n            cols[1] = \"External ID\"\n            organisms_thresholded_mics.columns = cols\n            organisms_thresholded_mics = organisms_thresholded_mics.sort_values(\n                by=list(organisms_thresholded_mics.columns)[2:],\n                na_position=\"last\",\n            )\n            # organisms_thresholded_mics.dropna(\n            #     subset=list(organisms_thresholded_mics.columns)[2:],\n            #     how=\"all\",\n            #     inplace=True,\n            # )\n            organisms_thresholded_mics.fillna(\"NA\", inplace=True)\n            organisms_thresholded_mics.to_excel(\n                os.path.join(resultpath, f\"{dataset[0]}_MIC{threshold}_results.xlsx\"),\n                index=False,\n            )\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.preprocess","title":"<code>preprocess(df, substance_id='ID', measurement='Optical Density', negative_controls='Negative Control', blanks='Blank', norm_by_barcode='Barcode')</code>","text":"<ul> <li>raw_df: raw reader data obtained with <code>rda.readerfiles_rawdf()</code></li> <li>input_df: input specifications table with required columns:<ul> <li>Dataset (with specified references as their own dataset 'Reference')</li> <li>ID (substance_id) (with specified blanks and negative_controls)</li> <li>Assay Transfer Barcode</li> <li>Row_384 (or Row_96)</li> <li>Col_384 (or Col_96)</li> <li>Concentration</li> <li>Replicate (specifying replicate number)</li> <li>Organism (scientific organism name i.e. with strain)</li> </ul> </li> </ul> <p>Processing function which merges raw reader data (raw_df) with input specifications table (input_df) and then normalizes, calculates Z-Factor per plate (norm_by_barcode) and rounds to sensible decimal places.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def preprocess(\n    df: pd.DataFrame,  # mapped inputs\n    substance_id: str = \"ID\",\n    measurement: str = \"Optical Density\",\n    negative_controls: str = \"Negative Control\",\n    blanks: str = \"Blank\",\n    norm_by_barcode=\"Barcode\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    - raw_df: raw reader data obtained with `rda.readerfiles_rawdf()`\n    - input_df: input specifications table with required columns:\n        - Dataset (with specified references as their own dataset 'Reference')\n        - ID (substance_id) (with specified blanks and negative_controls)\n        - Assay Transfer Barcode\n        - Row_384 (or Row_96)\n        - Col_384 (or Col_96)\n        - Concentration\n        - Replicate (specifying replicate number)\n        - Organism (scientific organism name i.e. with strain)\n    ---\n    Processing function which merges raw reader data (raw_df)\n    with input specifications table (input_df) and then\n    normalizes, calculates Z-Factor per plate (norm_by_barcode)\n    and rounds to sensible decimal places.\n    \"\"\"\n    df[substance_id] = df[substance_id].astype(str)\n    df = (\n        df.groupby(norm_by_barcode)[df.columns]\n        .apply(\n            lambda grp: background_normalize_zfactor(\n                grp,\n                substance_id,\n                measurement,\n                negative_controls,\n                blanks,\n                norm_by_barcode,\n            )\n        )\n        .reset_index(drop=True)\n    )\n\n    # df[substance_id] = df[substance_id].astype(str)\n\n    # detect and report NA values (defined in input, not in raw data)\n    orgs_w_missing_data = df[df[f\"Raw {measurement}\"].isna()][\"Organism formatted\"].unique()\n    if orgs_w_missing_data.size &gt; 0:\n        warnings.warn(\n            f\"\"\"Processed data:\n      Organisms with missing data, excluded from processed data: {orgs_w_missing_data}.\n      If this is not intended, please check the Input.xlsx or if raw data files are complete.\n              \"\"\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n        df = df.dropna(subset=[f\"Raw {measurement}\"])\n    # Report missing\n    # Remove missing from \"processed\" dataframe\n    return df.round(\n        {\n            \"Denoised Optical Density\": 2,\n            \"Relative Optical Density\": 2,\n            \"Z-Factor\": 2,\n            \"Robust Z-Factor\": 2,\n            # \"Concentration\": 2,\n        }\n    )\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.primary_results","title":"<code>primary_results(df, substance_id, filepath='../data/results/', thresholds=[50])</code>","text":"<p>Expects the results from rda.preprocess() function.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def primary_results(\n    df: pd.DataFrame,\n    substance_id,\n    filepath=\"../data/results/\",\n    thresholds: list[float] = [50],\n):\n    \"\"\"\n    Expects the results from rda.preprocess() function.\n    \"\"\"\n    df = df[\n        (df[\"Dataset\"] != \"Reference\")\n        &amp; (df[\"Dataset\"] != \"Positive Control\")\n        &amp; (df[\"Dataset\"] != \"Blank\")\n    ].dropna(subset=[\"Concentration\"])\n\n    pivot_df = pd.pivot_table(\n        df,\n        values=[\"Relative Optical Density\", \"Replicate\", \"Z-Factor\"],\n        index=[\n            substance_id,\n            \"Organism\",\n            \"Concentration\",\n            \"Dataset\",\n        ],\n        aggfunc={\n            \"Relative Optical Density\": [\"mean\"],\n            \"Replicate\": [\"count\"],\n        },\n    ).reset_index()\n    pivot_df.columns = [\" \".join(x).strip() for x in pivot_df.columns.ravel()]\n\n    for threshold in thresholds:\n        pivot_df[f\"Relative Growth &lt; {threshold}\"] = pivot_df.groupby(\n            [substance_id, \"Organism\", \"Dataset\"]\n        )[\"Relative Optical Density mean\"].transform(lambda x: x &lt; threshold)\n\n        for dataset, dataset_grp in pivot_df.groupby([\"Dataset\"]):\n            dataset = dataset[0]\n            resultpath = os.path.join(filepath, dataset)\n            pathlib.Path(resultpath).mkdir(parents=True, exist_ok=True)\n\n            print(\n                \"Saving\",\n                os.path.join(resultpath, f\"{dataset}_all_results.xlsx\"),\n            )\n            dataset_grp.to_excel(\n                os.path.join(resultpath, f\"{dataset}_all_results.xlsx\"),\n                index=False,\n            )\n            print(\n                \"Saving\",\n                os.path.join(resultpath, f\"{dataset}_all_results.csv\"),\n            )\n            dataset_grp.to_csv(\n                os.path.join(resultpath, f\"{dataset}_all_results.csv\"),\n                index=False,\n            )\n\n            pivot_multiindex_df = pd.pivot_table(\n                dataset_grp,\n                values=[f\"Relative Optical Density mean\"],\n                index=[substance_id, \"Dataset\", \"Concentration\"],\n                columns=\"Organism\",\n            ).reset_index()\n            cols = list(pivot_multiindex_df.columns.droplevel())\n            cols[:3] = list(map(lambda x: x[0], pivot_multiindex_df.columns[:3]))\n            pivot_multiindex_df.columns = cols\n\n            # Apply threshold (active in any organism)\n            thresholded_pivot = pivot_multiindex_df.iloc[\n                list(\n                    pivot_multiindex_df.iloc[:, 3:].apply(\n                        lambda x: any(list(map(lambda i: i &lt; threshold, x))), axis=1\n                    )\n                )\n            ]\n\n            # Sort by columns each organism after the other\n            # return pivot_multiindex_df.sort_values(by=cols[3:])\n\n            # Sort rows by mean between the organisms (lowest mean activity first)\n            results_sorted_by_mean_activity = thresholded_pivot.iloc[\n                thresholded_pivot.iloc[:, 3:].mean(axis=1).argsort()\n            ]\n            print(\n                \"Saving\",\n                os.path.join(\n                    resultpath, f\"{dataset}_threshold{threshold}_results.xlsx\"\n                ),\n            )\n            results_sorted_by_mean_activity.to_excel(\n                os.path.join(\n                    resultpath, f\"{dataset}_threshold{threshold}_results.xlsx\"\n                ),\n                index=False,\n            )\n            print(\n                \"Saving\",\n                os.path.join(resultpath, f\"{dataset}_threshold{threshold}_results.csv\"),\n            )\n            results_sorted_by_mean_activity.to_csv(\n                os.path.join(resultpath, f\"{dataset}_threshold{threshold}_results.csv\"),\n                index=False,\n            )\n</code></pre>"},{"location":"reference/process/#rda_toolbox.process.references_mic_results","title":"<code>references_mic_results(preprocessed_data, resultpath, thresholds=[20, 50])</code>","text":"<p>This function saves an excel file for the reference substances. Since reference substances have duplicate Internal IDs (since they are used multiple times), they would be meaned between duplicates. To circumvent this, this function exists which gets the MIC for each reference per (AcD) plate instead of per Internal ID.</p> Source code in <code>rda_toolbox/process.py</code> <pre><code>def references_mic_results(\n    preprocessed_data,\n    resultpath,\n    thresholds=[20, 50],\n):\n    \"\"\"\n    This function saves an excel file for the reference substances.\n    Since reference substances have duplicate Internal IDs\n    (since they are used multiple times), they would be meaned between duplicates.\n    To circumvent this, this function exists which gets the MIC\n    for each reference **per (AcD) plate** instead of per Internal ID.\n    \"\"\"\n    only_references = preprocessed_data[preprocessed_data[\"Dataset\"] == \"Reference\"]\n    mic_records = []\n    for group_names, grp in only_references.groupby(\n        [\n            \"Internal ID\",\n            \"External ID\",\n            \"Organism\",\n            \"Dataset\",\n            \"AcD Barcode 384\",\n        ]\n    ):\n        internal_id, external_id, organism, dataset, acd_barcode = group_names\n        grp = grp.copy().sort_values(by=[\"Concentration\"])\n        record = {\n            \"Internal ID\": internal_id,\n            \"External ID\": external_id,\n            \"Organism\": organism,\n            \"Dataset\": dataset,\n            \"AcD Barcode 384\": acd_barcode,\n            \"Z-Factor\": list(grp[\"Z-Factor\"])[0],\n        }\n        for threshold in thresholds:\n            values_below_threshold = grp[grp[\"Relative Optical Density\"] &lt; threshold]\n            # thx to jonathan - check if the OD at maximum concentration is below threshold (instead of any concentration)\n            max_conc_below_threshold = list(\n                grp[grp[\"Concentration\"] == max(grp[\"Concentration\"])][\n                    \"Relative Optical Density\"\n                ]\n                &lt; threshold\n            )[0]\n            if not max_conc_below_threshold:\n                mic = None\n            else:\n                mic = values_below_threshold.iloc[0][\"Concentration\"]\n            record[f\"MIC{threshold} in \u00b5M\"] = mic\n        mic_records.append(record)\n    mic_df = pd.DataFrame.from_records(mic_records)\n    mic_df.sort_values(by=[\"External ID\", \"Organism\"]).to_excel(\n        os.path.join(resultpath, \"References_MIC_results_eachRefID.xlsx\"), index=False\n    )\n</code></pre>"},{"location":"reference/utility/","title":"Utility Functions","text":""},{"location":"reference/utility/#rda_toolbox.utility.chunks","title":"<code>chunks(l, n)</code>","text":"<p>Useful function if you want to put a certain amount of observations into one plot. Yield n number of striped chunks from l.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def chunks(l, n):\n    \"\"\"\n    Useful function if you want to put a certain amount\n    of observations into one plot.\n    Yield n number of striped chunks from l.\n    \"\"\"\n    for i in range(0, n):\n        yield l[i::n]\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.format_organism_name","title":"<code>format_organism_name(raw_organism_name)</code>","text":"<p>Create internal, formatted orgnames but keep external for plots...</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def format_organism_name(raw_organism_name: str) -&gt; str:\n    \"\"\"\n    Create internal, formatted orgnames but keep external for plots...\n\n    \"\"\"\n    # I dont know how many spaces people introduce...\n    normalized_spaces = re.sub(r'\\s+', ' ', raw_organism_name).strip()\n    # I dont know which organism names people think of... (MRSA ST033793 vs. Acinetobacter Baumannii ATCC 17978)\n    # Or which other evil things people come up with\n    return normalized_spaces.lower()\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.generate_inputtable","title":"<code>generate_inputtable(readout_df=None, platetype=384)</code>","text":"<p>Generates an input table for the corresponding readout dataframe. If not readout df is provided, create a minimal input df.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def generate_inputtable(readout_df=None, platetype: int = 384):\n    \"\"\"\n    Generates an input table for the corresponding readout dataframe.\n    If not readout df is provided, create a minimal input df.\n    \"\"\"\n    if readout_df is None:\n        barcodes = [\"001PrS01001\"]\n    else:\n        barcodes = readout_df[\"Barcode\"].unique()\n\n    substance_df = pd.DataFrame(\n        {\n            \"ID\": [f\"Substance {i}\" for i in range(1, platetype + 1)],\n            f\"Row_{platetype}\": [*list(string.ascii_uppercase[:16]) * 24],\n            f\"Col_{platetype}\": sum([[i] * 16 for i in range(1, 24 + 1)], []),\n            \"Concentration in mg/mL\": 1,\n        }\n    )\n    layout_df = pd.DataFrame(\n        {\n            \"Barcode\": barcodes,\n            \"Replicate\": [1] * len(barcodes),\n            \"Organism\": [\n                f\"Placeholder Organism {letter}\"\n                for letter in string.ascii_uppercase[: len(barcodes)]\n            ],\n        }\n    )\n    df = pd.merge(layout_df, substance_df, how=\"cross\")\n    return df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.get_rows_cols","title":"<code>get_rows_cols(platetype)</code>","text":"<p>Obtain number of rows and columns as tuple for corresponding plate type.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def get_rows_cols(platetype: int) -&gt; tuple[int, int]:\n    \"\"\"\n    Obtain number of rows and columns as tuple for corresponding plate type.\n    \"\"\"\n    match platetype:\n        case 96:\n            return 8, 12\n        case 384:\n            return 16, 24\n        case _:\n            raise ValueError(\"Not a valid plate type\")\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.get_selection","title":"<code>get_selection(df, threshold_value, x_column='Relative Optical Density')</code>","text":"<p>Apply this ahead of get_upsetplot_df (to obtain dummies df). After all the above, apply UpSetAltair.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def get_selection(df, threshold_value, x_column=\"Relative Optical Density\"):\n    \"\"\"\n    Apply this ahead of get_upsetplot_df (to obtain dummies df).\n    After all the above, apply UpSetAltair.\n    \"\"\"\n    selection_results = df[df[x_column] &lt; threshold_value].copy()\n    selection_results[\"threshold\"] = f\"&lt;{threshold_value}\"\n\n    return selection_results\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.get_upsetplot_df","title":"<code>get_upsetplot_df(df, set_column='Organism', counts_column='ID')</code>","text":"<p>Function to obtain a correctly formatted DataFrame. According to UpSetR-shiny this table is supposed to be encoded in binary and set up so that each column represents a set, and each row represents an element. If an element is in the set it is represented as a 1 in that position. If an element is not in the set it is represented as a 0.</p> <p>Thanks to: https://stackoverflow.com/questions/37381862/get-dummies-for-pandas-column-containing-list</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def get_upsetplot_df(df, set_column=\"Organism\", counts_column=\"ID\"):\n    \"\"\"\n    Function to obtain a correctly formatted DataFrame.\n    According to [UpSetR-shiny](https://github.com/hms-dbmi/UpSetR-shiny)\n    this table is supposed to be encoded in binary and set up so that each column represents a set, and each row represents an element.\n    If an element is in the set it is represented as a 1 in that position. If an element is not in the set it is represented as a 0.\n\n    *Thanks to: https://stackoverflow.com/questions/37381862/get-dummies-for-pandas-column-containing-list*\n    \"\"\"\n    tmp_df = (\n        df.groupby(counts_column)[set_column].apply(lambda x: x.unique()).reset_index()\n    )\n    dummies_df = (\n        pd.get_dummies(\n            tmp_df.join(\n                pd.Series(\n                    tmp_df[set_column]\n                    .apply(pd.Series)\n                    .stack()\n                    .reset_index(1, drop=True),\n                    name=set_column + \"1\",\n                )\n            )\n            .drop(set_column, axis=1)\n            .rename(columns={set_column + \"1\": set_column}),\n            columns=[set_column],\n        )\n        .groupby(counts_column, as_index=False)\n        .sum()\n    )\n    # remove \"{set_column}_\" from set column labels\n    dummies_df.columns = list(\n        map(\n            lambda x: \"\".join(x.split(\"_\")[1:]) if x.startswith(set_column) else x,\n            dummies_df.columns,\n        )\n    )\n    # remove any dots as they interfere with altairs plotting.\n    dummies_df.columns = dummies_df.columns.str.replace(\".\", \"\")\n    return dummies_df.drop(columns=[\"count\"], errors=\"ignore\")\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.imgbuffer_to_imgstr","title":"<code>imgbuffer_to_imgstr(imgbuffer, prefix='data:image/png;base64,', suffix='')</code>","text":"<p>Encode imagebuffer to string (default base64-encoded string). Example: imgbuffer_to_imgstr(mol_to_bytes(mol)), prefix=\"'\")</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def imgbuffer_to_imgstr(imgbuffer, prefix=\"data:image/png;base64,\", suffix=\"\"):\n    \"\"\"\n    Encode imagebuffer to string (default base64-encoded string).\n    Example: imgbuffer_to_imgstr(mol_to_bytes(mol)), prefix=\"&lt;img src='data:image/png;base64,\", suffix=\"'/&gt;'\")\n    \"\"\"\n    str_equivalent_image = base64.b64encode(imgbuffer.getvalue()).decode()\n    img_tag = prefix + str_equivalent_image + suffix\n    return img_tag\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.inchi_to_imgstr","title":"<code>inchi_to_imgstr(inchi)</code>","text":"<p>Converts a inchi string to a base64 encoded image string (e.g. for plotting in altair). It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(), use these if you want more fine grained control over the format of the returned string. Example: df[\"image\"] = df[\"inchi\"].apply(lambda inchi: inchi_to_imgstr(inchi))</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def inchi_to_imgstr(inchi):\n    \"\"\"\n    Converts a inchi string to a base64 encoded image string (e.g. for plotting in altair).\n    It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(),\n    use these if you want more fine grained control over the format of the returned string.\n    Example: df[\"image\"] = df[\"inchi\"].apply(lambda inchi: inchi_to_imgstr(inchi))\n    \"\"\"\n    return imgbuffer_to_imgstr(mol_to_bytes(Chem.MolFromInchi(inchi)))\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.map_96_to_384","title":"<code>map_96_to_384(df_row, rowname, colname, q_name)</code>","text":"<p>Maps the rows and columns of 4 96-Well plates into a single 384-Well plate.</p> <ul> <li>Maps in Z order.</li> <li>Takes row, column and quadrant (each of the 96-well plates is one quadrant) of a well from 4 96-well plates and maps it to the corresponding well in a 384-well plate</li> </ul> <p>Returns the 384-Well plate row and column. Example: <code>df[\"Row_384\"], df[\"Col_384\"] = zip(*df.apply(map_96_to_384, axis=1))</code></p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def map_96_to_384(\n    df_row: pd.Series,\n    rowname: str,\n    colname: str,\n    q_name: str,\n) -&gt; tuple[pd.Series, pd.Series]:\n    \"\"\"\n    Maps the rows and columns of 4 96-Well plates into a single 384-Well plate.\n\n    - Maps in Z order.\n    - Takes row, column and quadrant (each of the 96-well plates is one quadrant) of a well from 4 96-well plates and maps it to the corresponding well in a 384-well plate\n\n    Returns the 384-Well plate row and column.\n    Example: `df[\"Row_384\"], df[\"Col_384\"] = zip(*df.apply(map_96_to_384, axis=1))`\n    \"\"\"\n    # TODO: Write tests for this mapping function\n\n    row = df_row[rowname]  # 96-well plate row\n    col = df_row[colname]  # 96-well plate column\n    quadrant = df_row[q_name]  # which of the 4 96-well plate\n\n    rowmapping = dict(\n        zip(\n            string.ascii_uppercase[0:8],\n            np.array_split(list(string.ascii_uppercase)[0:16], 8),\n        )\n    )\n    colmapping = dict(zip(list(range(1, 13)), np.array_split(list(range(1, 25)), 12)))\n    row_384 = rowmapping[row][0 if quadrant in [1, 2] else 1]\n    col_384 = colmapping[col][0 if quadrant in [1, 3] else 1]\n    return row_384, col_384\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.mapapply_96_to_384","title":"<code>mapapply_96_to_384(df, rowname='Row_96', colname='Column_96', q_name='Quadrant')</code>","text":"<p>Apply to a DataFrame the mapping of 96-well positions to 384-well positions.</p> <ul> <li>Maps in Z order. The DataFrame has to have columns with:</li> <li>96-well plate row positions</li> <li>96-well plate column positions</li> <li>96-well plate to 384-well plate quadrants (4 96-well plates fit into 1 384-well plate)</li> </ul> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def mapapply_96_to_384(\n    df: pd.DataFrame,\n    rowname: str = \"Row_96\",\n    colname: str = \"Column_96\",\n    q_name: str = \"Quadrant\",\n) -&gt; pd.DataFrame:\n    \"\"\"Apply to a DataFrame the mapping of 96-well positions to 384-well positions.\n\n    - Maps in Z order.\n    The DataFrame has to have columns with:\n    - 96-well plate row positions\n    - 96-well plate column positions\n    - 96-well plate to 384-well plate quadrants\n    *(4 96-well plates fit into 1 384-well plate)*\n    \"\"\"\n    df[\"Row_384\"], df[\"Col_384\"] = zip(\n        *df.apply(\n            lambda row: map_96_to_384(\n                row, rowname=rowname, colname=colname, q_name=q_name\n            ),\n            axis=1,\n        )\n    )\n    return df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.mic_assaytransfer_mapping","title":"<code>mic_assaytransfer_mapping(position, orig_barcode, ast_platemapping, *, strict=False)</code>","text":"<p>Map a 96-well motherplate position to a 384-well AsT plate position.</p>"},{"location":"reference/utility/#rda_toolbox.utility.mic_assaytransfer_mapping--parameters","title":"Parameters","text":"<p>position : str     Well position on the 96-well plate in the form 'A1'..'H12'. orig_barcode : Any     Identifier of the 96-well motherplate. Will be cast to str. ast_platemapping : mapping     Mapping from motherplate barcode -&gt; sequence of AsT plate barcodes.     It should support <code>ast_platemapping[orig_barcode]</code>.</p> <pre><code>Common layouts:\n- dict[str, Sequence[str]]\n- pandas.Series/row where `row[0]` holds a Sequence[str]\n</code></pre> bool, default False <p>If True, raise a ValueError when the requested third of the plate (\u2153, \u2154, 3/3) does not exist in <code>ast_platemapping</code>. If False, fall back to the last available AsT plate and continue.</p>"},{"location":"reference/utility/#rda_toolbox.utility.mic_assaytransfer_mapping--returns","title":"Returns","text":"<p>(row_384, col_384, ast_barcode) : (str, str, str)     384-well plate row (A\u2013P), column (1\u201324, here only 1\u20132 used),     and the corresponding AsT plate barcode.</p>"},{"location":"reference/utility/#rda_toolbox.utility.mic_assaytransfer_mapping--notes","title":"Notes","text":"<ul> <li>96-well plate: rows A\u2013H, cols 1\u201312.</li> <li>384-well plate: rows A\u2013P, cols 1\u201324.   This function maps each 96-well to one of two 384 rows (2x upscaling)   and to column 1 or 2 (alternating).</li> <li>The 96-well plate is conceptually split into three vertical thirds:     cols  1\u20134  -&gt; AsT plate index 0     cols  5\u20138  -&gt; AsT plate index 1     cols  9\u201312 -&gt; AsT plate index 2</li> </ul> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def mic_assaytransfer_mapping(\n    position: str,\n    orig_barcode: Any,\n    ast_platemapping: dict,\n    *,\n    strict: bool = False,\n) -&gt; Tuple[str, str, str]:\n    \"\"\"\n    Map a 96-well motherplate position to a 384-well AsT plate position.\n\n    Parameters\n    ----------\n    position : str\n        Well position on the 96-well plate in the form 'A1'..'H12'.\n    orig_barcode : Any\n        Identifier of the 96-well motherplate. Will be cast to str.\n    ast_platemapping : mapping\n        Mapping from motherplate barcode -&gt; sequence of AsT plate barcodes.\n        It should support `ast_platemapping[orig_barcode]`.\n\n        Common layouts:\n        - dict[str, Sequence[str]]\n        - pandas.Series/row where `row[0]` holds a Sequence[str]\n\n    strict : bool, default False\n        If True, raise a ValueError when the requested third of the plate\n        (1/3, 2/3, 3/3) does not exist in `ast_platemapping`.\n        If False, fall back to the last available AsT plate and continue.\n\n    Returns\n    -------\n    (row_384, col_384, ast_barcode) : (str, str, str)\n        384-well plate row (A\u2013P), column (1\u201324, here only 1\u20132 used),\n        and the corresponding AsT plate barcode.\n\n    Notes\n    -----\n    - 96-well plate: rows A\u2013H, cols 1\u201312.\n    - 384-well plate: rows A\u2013P, cols 1\u201324.\n      This function maps each 96-well to one of two 384 rows (2x upscaling)\n      and to column 1 or 2 (alternating).\n    - The 96-well plate is conceptually split into three vertical thirds:\n        cols  1\u20134  -&gt; AsT plate index 0\n        cols  5\u20138  -&gt; AsT plate index 1\n        cols  9\u201312 -&gt; AsT plate index 2\n    \"\"\"\n\n    # ---- Normalize and validate input ----\n    if not isinstance(position, str):\n        raise TypeError(f\"position must be a string like 'A1', got {type(position)}\")\n\n    position = position.strip().upper()\n    if len(position) &lt; 2:\n        raise ValueError(f\"Invalid well position {position!r}\")\n\n    row = position[0]\n    col_str = position[1:]\n\n    if row not in \"ABCDEFGH\":\n        raise ValueError(f\"Row {row!r} out of range for 96-well plate (A\u2013H).\")\n\n    try:\n        col = int(col_str)\n    except ValueError as exc:\n        raise ValueError(f\"Column {col_str!r} is not an integer in position {position!r}.\") from exc\n\n    if not (1 &lt;= col &lt;= 12):\n        raise ValueError(f\"Column {col} out of range for 96-well plate (1\u201312).\")\n\n    orig_barcode = str(orig_barcode)\n\n    # ---- Build row mapping (A\u2013H -&gt; pairs like [A,B], [C,D], ..., [O,P]) ----\n    # 384 rows we use: A\u2013P (16 rows), grouped into 8 pairs\n    rows_384 = list(string.ascii_uppercase[:16])  # ['A', ..., 'P']\n    row_pairs = [rows_384[i : i + 2] for i in range(0, 16, 2)]  # [['A','B'], ['C','D'], ..., ['O','P']]\n\n    row_index_96 = \"ABCDEFGH\".index(row)\n\n    # Equivalent to your mapping dict:\n    # mapping = {1:0, 2:0, 3:1, 4:1, 5:0, 6:0, 7:1, 8:1, 9:0, 10:0, 11:1, 12:1}\n    # This can be expressed as:\n    mapping_idx = ((col - 1) // 2) % 2  # gives 0,0,1,1,0,0,1,1,0,0,1,1 for col 1..12\n\n    row_384 = row_pairs[row_index_96][mapping_idx]\n\n    # ---- Column mapping (1\u201312 -&gt; 1 or 2 alternating) ----\n    # Your original colmapping: 1-&gt;1, 2-&gt;2, 3-&gt;1, 4-&gt;2, ...\n    col_384 = 1 if (col % 2) == 1 else 2\n\n    # ---- Determine which 1/3 of the motherplate this is in (0,1,2) ----\n    # 1\u20134 -&gt; 0; 5\u20138 -&gt; 1; 9\u201312 -&gt; 2\n    ast_of_3 = (col - 1) // 4\n\n    # ---- Normalize access to ast_platemapping ----\n    def _get_ast_plates(mapping_obj, barcode: str) -&gt; list[str]:\n        \"\"\"Return a list of AsT barcodes for a given motherplate barcode.\"\"\"\n        try:\n            entry = mapping_obj[barcode]\n        except KeyError as exc:\n            raise KeyError(\n                f\"No entry for motherplate barcode {barcode!r} in ast_platemapping.\"\n            ) from exc\n\n        # For pandas row / Series where first column holds the list\n        # try to access [0]; if that fails, use entry directly.\n        try:\n            candidate = entry[0]\n        except Exception:\n            candidate = entry\n\n        # Normalize to list[str]\n        if isinstance(candidate, (str, bytes)):\n            plates = [candidate]\n        elif isinstance(candidate, Sequence):\n            plates = list(candidate)\n        else:\n            # Last resort: wrap in list\n            plates = [str(candidate)]\n\n        # Filter out obvious empties\n        plates = [str(p) for p in plates if p not in (None, \"\", \"nan\")]\n        if not plates:\n            raise ValueError(f\"ast_platemapping[{barcode!r}] contains no valid AsT barcodes.\")\n        return plates\n\n    ast_plates = _get_ast_plates(ast_platemapping, orig_barcode)\n\n    # ---- Choose the correct AsT plate, safely ----\n    if ast_of_3 &gt;= len(ast_plates):\n        # We are requesting e.g. the 3rd third (index 2) but only 1 or 2 AsT plates exist.\n        if strict:\n            raise ValueError(\n                f\"Motherplate {orig_barcode!r} has only {len(ast_plates)} AsT plate(s), \"\n                f\"cannot select segment index {ast_of_3} for column {col}.\"\n            )\n        # Non-strict mode: fall back to the last available AsT plate\n        # and keep going, but at least make it explicit.\n        ast_index = len(ast_plates) - 1\n    else:\n        ast_index = ast_of_3\n\n    barcode_384_ast = ast_plates[ast_index]\n\n    return str(row_384), str(col_384), str(barcode_384_ast)\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.position_to_rowcol","title":"<code>position_to_rowcol(pos)</code>","text":"<p>Splits a position like \"A1\" into row and col e.g. (\"A\", 1).</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def position_to_rowcol(pos: str) -&gt; tuple[str, int]:\n    \"\"\"\n    Splits a position like \"A1\" into row and col e.g. (\"A\", 1).\n    \"\"\"\n    if not isinstance(pos, str):\n        raise TypeError(\"Position must be a string.\")\n\n    if len(pos) &lt; 2:\n        raise ValueError(f\"Invalid plate position: {pos!r}\")\n\n    row = pos[0]\n    col = pos[1:]\n\n    if not row.isalpha() or not col.isdigit():\n        raise ValueError(f\"Invalid plate position: {pos!r}\")\n\n    return row.upper(), int(col)\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.prepare_visualization","title":"<code>prepare_visualization(df, by_id='Internal ID', whisker_width=1, exclude_negative_zfactors=True, threshold=50.0)</code>","text":"<p>Does formatting for the facet lineplots.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def prepare_visualization(\n    df: pd.DataFrame,\n    by_id: str = \"Internal ID\",\n    whisker_width: int = 1,\n    exclude_negative_zfactors: bool = True,\n    threshold: float = 50.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Does formatting for the facet lineplots.\n    \"\"\"\n    df = df.copy()\n    if exclude_negative_zfactors:\n        df = df[df[\"Z-Factor\"] &gt; 0]\n    df.loc[:, \"Used Replicates\"] = df.groupby([by_id, \"Concentration\", \"Organism\"])[\n        [\"Replicate\"]\n    ].transform(\"count\")\n    df.loc[:, \"Mean Relative Optical Density\"] = (\n        df.groupby([by_id, \"Concentration\", \"Organism\"])[[\"Relative Optical Density\"]]\n        .transform(\"mean\")\n        .round(2)\n    )\n    df.loc[:, \"Std. Relative Optical Density\"] = (\n        df.groupby([by_id, \"Concentration\", \"Organism\"])[[\"Relative Optical Density\"]]\n        .transform(\"std\")\n        .round(2)\n    )\n    df.loc[:, \"uerror\"] = (\n        df[\"Mean Relative Optical Density\"] + df[\"Std. Relative Optical Density\"]\n    )\n    df.loc[:, \"lerror\"] = (\n        df[\"Mean Relative Optical Density\"] - df[\"Std. Relative Optical Density\"]\n    )\n    tmp_list: list[pd.DataFrame] = []\n    for _, grp in df.groupby([by_id, \"Organism\"]):\n        # use replicate == 1 as the meaned OD is the same in all 3 replicates anyways\n        # print(grp)\n        maxconc_below_threshold = (\n            grp[\n                (grp[\"Replicate\"] == 1)\n                &amp; (grp[\"Concentration\"] == grp[\"Concentration\"].max())\n            ][\"Mean Relative Optical Density\"]\n            &lt; threshold\n        )\n        grp[\"max_conc_below_threshold\"] = list(maxconc_below_threshold)[0]\n        tmp_list.append(grp)\n        # .sort_values(by=[\"Concentration\"], ascending=False)\n        # grp_sorted[grp_sorted[\"Concentration\"] == 50][\"Mean Relative Optical Density\"]\n        # print(grp.aggregate())\n    df = pd.concat(tmp_list)\n    # df[\"highest_conc_bigger_50\"] = df.groupby([by_id, \"Organism\"])[\n    #     [\"Mean Relative Optical Density\"]\n    # ].transform(\n    #     lambda meas_per_conc: list(meas_per_conc)[0] &gt; 50\n    # )\n    # print(df)\n\n    df[\"at_all_conc_bigger_50\"] = df.groupby([by_id, \"Organism\"])[\n        [\"Mean Relative Optical Density\"]\n    ].transform(lambda meas_per_conc: all([x &gt; 50 for x in list(meas_per_conc)]))\n    # Bin observations into artificial categories for plotting later:\n    plot_groups = pd.DataFrame()\n    for _, grp in df.groupby([\"AsT Barcode 384\"]):\n        # divide the observations per plate into chunks\n        # number of chunks is defined by using a maximum of 10 colors/observations per plot\n        num_chunks = math.ceil(len(grp[by_id].unique()) / 10)\n        for nr, chunk in enumerate(list(chunks(grp[by_id].unique(), num_chunks))):\n            plot_groups = pd.concat(\n                [\n                    plot_groups,\n                    pd.DataFrame(\n                        {\n                            by_id: chunk,\n                            \"AsT Plate Subgroup\": sum([[nr] * len(chunk)], []),\n                        }\n                    ),\n                ]\n            ).reset_index(drop=True)\n    df = pd.merge(df, plot_groups)\n    return df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.read_sdf_withproperties","title":"<code>read_sdf_withproperties(sdf_filepath)</code>","text":"<p>Reads a SDF file and returns a DataFrame containing the molecules as rdkit molobjects as well as all the encoded properties in the SDF block.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def read_sdf_withproperties(sdf_filepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads a SDF file and returns a DataFrame containing the molecules as rdkit molobjects\n    as well as all the encoded properties in the SDF block.\n    \"\"\"\n    suppl = Chem.SDMolSupplier(sdf_filepath)\n    mols = []\n    nonmol_counter = 0\n    for mol in suppl:\n        if not mol:\n            nonmol_counter += 1\n            continue\n        mol_props = {\"mol\": mol}\n        propnames = mol.GetPropNames()\n        for prop in propnames:\n            mol_props[prop] = mol.GetProp(prop)\n        mols.append(mol_props)\n    if nonmol_counter &gt; 0:\n        print(f\"Ignored molecules: {nonmol_counter}\")\n    return pd.DataFrame(mols)\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.save_plot_per_dataset","title":"<code>save_plot_per_dataset(data, plotfunc, location, plotname=None, saveformats=['svg', 'html'])</code>","text":"<p>This is a convenience function which splits a dataframe into each dataset given in the 'Dataset' column. Then it applies the given plotting function 'plotfunc' to these splits, automatically creates folders if non existent and saves the plots to the corresponding dataset folders. Examples:     save_plot_per_dataset(preprocessed_data, rda.lineplots_facet, \"../figures/\")</p> <pre><code># if an anonymous function (lambda) is used, a plotname has to be provided:\nsave_plot_per_dataset(mic_results_long, lambda x: rda.mic_hitstogram(x, \"MIC50 in \u00b5M\"), \"../figures/\", plotname=\"MIC_Hits_Distribution\")\n</code></pre> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def save_plot_per_dataset(\n    data: pd.DataFrame,\n    plotfunc,\n    location: str,\n    plotname: str | None = None,\n    saveformats: list[str] = [\"svg\", \"html\"],\n) -&gt; None:\n    \"\"\"\n    This is a convenience function which splits a dataframe into each dataset given in the 'Dataset' column.\n    Then it applies the given plotting function 'plotfunc' to these splits,\n    automatically creates folders if non existent and saves the plots to the corresponding dataset folders.\n    Examples:\n        save_plot_per_dataset(preprocessed_data, rda.lineplots_facet, \"../figures/\")\n\n        # if an anonymous function (lambda) is used, a plotname has to be provided:\n        save_plot_per_dataset(mic_results_long, lambda x: rda.mic_hitstogram(x, \"MIC50 in \u00b5M\"), \"../figures/\", plotname=\"MIC_Hits_Distribution\")\n    \"\"\"\n    if plotname is None:\n        plotname = plotfunc.__name__\n        if plotname == \"&lt;lambda&gt;\":\n            raise TypeError(\"Please provide a plotname when using a lambda function.\")\n\n    data = data.loc[\n        (data[\"Dataset\"] != \"Negative Control\") &amp; (data[\"Dataset\"] != \"Blank\")\n    ]\n    reference_df = data.loc[data[\"Dataset\"] == \"Reference\"]\n    for dataset in filter(lambda x: x != \"Reference\", data[\"Dataset\"].unique()):\n        dataset_data = data.loc[data[\"Dataset\"] == dataset]\n        if \"AcD Barcode 384\" in dataset_data:\n            dataset_barcodes = list(dataset_data[\"AcD Barcode 384\"].unique())\n            dataset_references = reference_df.loc[\n                (reference_df[\"AcD Barcode 384\"].isin(dataset_barcodes)),\n                :,\n            ]\n        else:\n            dataset_references = pd.DataFrame()\n        set_plot = plotfunc(pd.concat([dataset_data, dataset_references]))\n        folder_location = os.path.join(location, dataset)\n        pathlib.Path(folder_location).mkdir(parents=True, exist_ok=True)\n        for fformat in saveformats:\n            print(\n                \"Saving: \",\n                os.path.join(\n                    folder_location,\n                    f\"{dataset}_{plotname}.{fformat}\",\n                ),\n            )\n            set_plot.save(\n                os.path.join(\n                    folder_location,\n                    f\"{dataset}_{plotname}.{fformat}\",\n                )\n            )\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.smiles_grid_altair","title":"<code>smiles_grid_altair(df, smiles_col='smiles', n_cols=6, img_size=128, tooltip_cols=None, drop_invalid=True, background='#ffffff', gridtitle='Molecule Grid')</code>","text":"<p>Render a grid of molecule images from a DataFrame using Altair.</p>"},{"location":"reference/utility/#rda_toolbox.utility.smiles_grid_altair--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Must contain a SMILES column (default 'smiles'). Other columns are arbitrary. smiles_col : str     Column name with SMILES strings. n_cols : int     Number of grid columns. img_size : int     Size (pixels) of each PNG image (square). tooltip_cols : list[str] | None     If provided, use only these columns as tooltips. Otherwise include all non-internal columns. drop_invalid : bool     If True, rows with invalid/unparsable SMILES are removed. If False, they'll be kept with empty images. background : str | None     Optional CSS color (e.g., '#ffffff') for chart background.</p>"},{"location":"reference/utility/#rda_toolbox.utility.smiles_grid_altair--returns","title":"Returns","text":"<p>alt.Chart</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def smiles_grid_altair(\n        df: pd.DataFrame,\n        smiles_col: str = \"smiles\",\n        n_cols: int = 6,\n        img_size: int = 128,\n        tooltip_cols: list[str] | None = None,\n        drop_invalid: bool = True,\n        background: str | None = '#ffffff',\n        gridtitle: str | None = \"Molecule Grid\",\n        ):\n    \"\"\"\n    Render a grid of molecule images from a DataFrame using Altair.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain a SMILES column (default 'smiles'). Other columns are arbitrary.\n    smiles_col : str\n        Column name with SMILES strings.\n    n_cols : int\n        Number of grid columns.\n    img_size : int\n        Size (pixels) of each PNG image (square).\n    tooltip_cols : list[str] | None\n        If provided, use only these columns as tooltips. Otherwise include all non-internal columns.\n    drop_invalid : bool\n        If True, rows with invalid/unparsable SMILES are removed. If False, they'll be kept with empty images.\n    background : str | None\n        Optional CSS color (e.g., '#ffffff') for chart background.\n\n    Returns\n    -------\n    alt.Chart\n    \"\"\"\n\n    if smiles_col not in df.columns:\n        raise ValueError(f\"Column '{smiles_col}' not found in DataFrame.\")\n\n    data = df.copy()\n    # --- helper: SMILES -&gt; data URL (PNG in-memory) ---\n    def _smiles_to_data_url(smi: str) -&gt; str | None:\n        try:\n            mol = Chem.MolFromSmiles(str(smi))\n            if mol is None:\n                return None\n            # RDKit renders to PIL Image; save to BytesIO as PNG\n            img = Draw.MolToImage(mol)\n            buf = BytesIO()\n            img.save(buf, format=\"png\")\n            encoded = base64.b64encode(buf.getvalue()).decode()\n            return f\"data:image/png;base64,{encoded}\"\n        except Exception:\n            print(\"Exception: \", smi)\n            return None\n\n    # Create image column\n    data[\"_image_url\"] = data[smiles_col].apply(_smiles_to_data_url)\n\n    # Handle invalids\n    if drop_invalid:\n        data = data.loc[data[\"_image_url\"].notna()].copy()\n\n    if len(data) == 0:\n        raise ValueError(\"No valid molecules to render (all SMILES failed to parse?).\")\n\n    # Grid coordinates\n    data = data.reset_index(drop=True)\n    data[\"_idx\"] = np.arange(len(data))\n    data[\"_col\"] = (data[\"_idx\"] % n_cols).astype(int)\n    data[\"_row\"] = (data[\"_idx\"] // n_cols).astype(int)\n\n    # Tooltips: include all user columns by default (exclude internal helpers)\n    internal_cols = {\"_image_url\", \"_idx\", \"_col\", \"_row\", \"smiles\"}\n    if tooltip_cols is None:\n        tooltip_cols = [c for c in data.columns if c not in internal_cols]\n\n    # Build chart\n    chart = (\n            alt.Chart(data)\n            .mark_image(width=img_size, height=img_size)\n            .encode(\n                x=alt.X(\"_col:O\", axis=None),\n                # reverse rows so row 0 is at the top\n                y=alt.Y(\"_row:O\", axis=None, sort=\"ascending\"),\n                url=\"_image_url:N\",\n                tooltip=tooltip_cols,\n                )\n            .properties(\n                width=n_cols * img_size,\n                height=(int(np.ceil(len(data) / n_cols))) * img_size,\n                background=background,\n                title=gridtitle,\n                )\n            )\n\n    return chart\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.smiles_to_imgstr","title":"<code>smiles_to_imgstr(smiles)</code>","text":"<p>Converts a smiles string to a base64 encoded image string (e.g. for plotting in altair). It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(), use these if you want more fine grained control over the format of the returned string. Example: df[\"image\"] = df[\"smiles\"].apply(lambda smiles: smiles_to_imgstr(smiles))</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def smiles_to_imgstr(smiles):\n    \"\"\"\n    Converts a smiles string to a base64 encoded image string (e.g. for plotting in altair).\n    It's a convenience function consisting of rda.utility.mol_to_bytes() and rda.utility.imgbuffer_to_imgstr(),\n    use these if you want more fine grained control over the format of the returned string.\n    Example: df[\"image\"] = df[\"smiles\"].apply(lambda smiles: smiles_to_imgstr(smiles))\n    \"\"\"\n    return imgbuffer_to_imgstr(mol_to_bytes(Chem.MolFromSmiles(smiles)))\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.split_position","title":"<code>split_position(df, position='Position', row='Row_384', col='Col_384', copy=True)</code>","text":"<p>Split a position like \"A1\" into row and column positions (\"A\", 1) and adds them as columns to the DataFrame. Set <code>copy=True</code> to avoid mutating the provided DataFrame. Hint: Remove NAs before applying this function. E.g. <code>split_position(df.dropna(subset=\"Position\"))</code></p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def split_position(\n    df: pd.DataFrame,\n    position: str = \"Position\",\n    row: str = \"Row_384\",\n    col: str = \"Col_384\",\n    copy: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Split a position like \"A1\" into row and column positions (\"A\", 1) and adds them as columns to the DataFrame. Set `copy=True` to avoid mutating the provided DataFrame.\n    Hint: Remove NAs before applying this function. E.g. `split_position(df.dropna(subset=\"Position\"))`\n    \"\"\"\n    target_df = df.copy() if copy else df\n    parsed_positions = target_df[position].map(position_to_rowcol)\n    target_df[row] = parsed_positions.str.get(0)\n    target_df[col] = parsed_positions.str.get(1)\n    return target_df\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.to_excel_molimages","title":"<code>to_excel_molimages(df, filename, desired_columns, mol_col='mol')</code>","text":"<p>Writes a dataframe containing RDKit molecule objects to an excel file containing the molecular structures as PNG images. Needs a column in df with RDKit mol object (e.g. rdkit.Chem.MolFromInchi, MolFromMolBlock, MolFromSmiles etc.)</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def to_excel_molimages(\n    df: pd.DataFrame, filename: str, desired_columns: list[str], mol_col: str = \"mol\"\n):\n    \"\"\"\n    Writes a dataframe containing RDKit molecule objects to an excel file containing the molecular structures as PNG images.\n    Needs a column in df with RDKit mol object (e.g. rdkit.Chem.MolFromInchi, MolFromMolBlock, MolFromSmiles etc.)\n    \"\"\"\n    writer = pd.ExcelWriter(filename, engine=\"xlsxwriter\")\n    workbook = writer.book\n    # workbook = xlsxwriter.Workbook(\"images_bytesio.xlsx\")\n    worksheet = workbook.add_worksheet()\n    df[\"PIL_img\"] = df[mol_col].map(Draw.MolToImage)\n\n    def get_imgbuffer(image):\n        stream = BytesIO()\n        image.save(stream, format=\"PNG\")\n        return stream\n\n    for i, imgbuf in enumerate(df[\"PIL_img\"].map(get_imgbuffer), start=1):\n        worksheet.set_column(0, 0, 20)\n        worksheet.set_row(i, 120)\n        worksheet.insert_image(\n            f\"A{i+1}\", \"img.png\", {\"image_data\": imgbuf, \"x_scale\": 0.5, \"y_scale\": 0.5}\n        )\n\n    df.loc[:, desired_columns].to_excel(writer, startcol=1, index=False)\n    workbook.close()\n</code></pre>"},{"location":"reference/utility/#rda_toolbox.utility.write_excel_MolImages","title":"<code>write_excel_MolImages(df, filename, molcol_header)</code>","text":"<p>Writes images (.png) of molecules structures into an excel file derived from the given dataframe.</p> Source code in <code>rda_toolbox/utility.py</code> <pre><code>def write_excel_MolImages(df: pd.DataFrame, filename: str, molcol_header: str):\n    \"\"\"\n    Writes images (.png) of molecules structures into an excel file derived from the given dataframe.\n    \"\"\"\n    if molcol_header not in df:\n        raise ValueError(\n            f\"Missing {molcol_header} column in df.\"\n        )\n\n    df[\"img_buf\"] = df[molcol_header].apply(lambda x: mol_to_bytes(x) if x else None)\n    writer = pd.ExcelWriter(filename, engine=\"xlsxwriter\")\n\n    workbook = writer.book\n    worksheet = workbook.add_worksheet()\n    for i, imgbuf in enumerate(list(df[\"img_buf\"]), start=1):\n        worksheet.set_column(0, 0, 20)\n        worksheet.set_row(i, 120)\n        worksheet.insert_image(\n            f\"A{i+1}\", \"img.png\", {\"image_data\": imgbuf, \"x_scale\": 0.5, \"y_scale\": 0.5}\n        )\n\n    df.drop(columns=[\"img_buf\", molcol_header]).to_excel(writer, startcol=1, index=False)\n    workbook.close()\n</code></pre>"}]}